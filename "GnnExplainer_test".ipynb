{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Копия блокнота \"GnnExplainer.ipynb\"",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/io8ex/2021-assignment-numpy/blob/main/%22GnnExplainer_test%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation of the needed packages"
      ],
      "metadata": {
        "id": "I4KwOZ8Evv9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.version.cuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UxOUD0QRHu4x",
        "outputId": "8d462378-7aff-4872-d4d5-c266f04b1452"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'11.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IU9gsYZIc5j",
        "outputId": "62a2902d-2443-46c4-fd07-e587065ed4fc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.10.0+cu111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEn-ZgedHvIr",
        "outputId": "51b85fc0-2b74-41af-a1fb-7bc13fdc220f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 6.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 10.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.13\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_cluster-1.6.0-cp37-cp37m-linux_x86_64.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 8.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.6.0\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-1.10.0%2Bcu113/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (750 kB)\n",
            "\u001b[K     |████████████████████████████████| 750 kB 8.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.1\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.63.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=c83f627bbfc9fa7414c578c9aeab9fb0ced8f9e9cf89ac6f604ed72cf9b5807c\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/a6/a4/ca18c3051fcead866fe7b85700ee2240d883562a1bc70ce421\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading packages and the model"
      ],
      "metadata": {
        "id": "oVUQrzpcv4mI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.datasets import TUDataset, Planetoid\n",
        "from torch_geometric.nn import GCNConv, Set2Set, GNNExplainer\n",
        "import torch_geometric.transforms as T\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from tqdm import tqdm, trange\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "HQTcaK2UHvLz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataset \n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "\n",
        "G = nx.Graph()\n",
        "nb_nodes = 300\n",
        "nb_features = 10\n",
        "nb_dim = 16\n",
        "nodes = [i for i in range(nb_nodes)]\n",
        "nodes_t = [list(np.random.randn(nb_features)) for node in nodes]\n",
        "edges = [[i % nb_nodes, (i + 1) % nb_nodes] for i in range(nb_nodes)]\n",
        "G.add_nodes_from(nodes)\n",
        "G.add_edges_from(edges)\n",
        "nx.draw_networkx(G, node_color='r')\n",
        "\n",
        "# get adjacency\n",
        "\n",
        "# get edges"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "5cq9HS7qGBT5",
        "outputId": "ffd8dce3-65c2-4421-8822-8132b6f8ba07"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from torch_geometric.data import Data\n",
        "\n",
        "# edge_index = torch.tensor([[0, 1],\n",
        "#                            [1, 0],\n",
        "#                            [1, 2],\n",
        "#                            [2, 1]], dtype=torch.long)\n",
        "# x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
        "\n",
        "# data = Data(x=x, edge_index=edge_index.t().contiguous())"
      ],
      "metadata": {
        "id": "KxIvp-SgaPvB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "edge_index = torch.tensor(edges, dtype=torch.long)\n",
        "x = torch.tensor(nodes_t, dtype=torch.float)\n",
        "\n",
        "data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
        "data.x.shape, data.edge_index.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FUfv9B8W-Mp",
        "outputId": "f60dca09-6576-44c7-c5b8-93f83dbea428"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([300, 10]), torch.Size([2, 300]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # #Load the dataset\n",
        "# dataset = 'cora'\n",
        "# path = os.path.join(os.getcwd(), 'data', 'Planetoid')\n",
        "# train_dataset = Planetoid(path, dataset, transform=T.NormalizeFeatures())\n",
        "\n",
        "# # #from torch_geometric.datasets import Twitch\n",
        "# # # Dataset source: https://github.com/benedekrozemberczki/datasets#twitch-social-networks\n",
        "# # #train_dataset = Twitch(root=\".\", name=\"EN\")\n",
        "\n",
        "# # # Since the dataset is comprised of a single huge graph, we extract that graph by indexing 0.\n",
        "# data = train_dataset[0]\n",
        "\n",
        "# Since there is only 1 graph, the train/test split is done by masking regions of the graph. We split the last 500+500 nodes as val and test, and use the rest as the training data.\n",
        "data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "data.train_mask[:data.num_nodes // 2] = 1\n",
        "data.val_mask = None\n",
        "data.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
        "data.test_mask[data.num_nodes // 2:] = 1"
      ],
      "metadata": {
        "id": "wIrZQR3iH1TB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN1DAVuIfrRh",
        "outputId": "7bef8b36-9c7f-45c7-ca4e-07c9c457299c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([300, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Neural Network for explanation"
      ],
      "metadata": {
        "id": "wQNv9Z07wkPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, num_features, dim=16, num_classes=1):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = GCNConv(num_features, dim)\n",
        "        self.conv2 = GCNConv(dim, dim)\n",
        "        self.dense = nn.Linear(dim, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, data=None):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.dense(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "CRcxL9fgJVtg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 500\n",
        "num_features = 10\n",
        "num_classes = 2\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Net(num_features=num_features, dim=nb_dim, num_classes=num_classes).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-3)"
      ],
      "metadata": {
        "id": "jIxHROnbJWQ-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, data):\n",
        "    model.eval()\n",
        "    logits, accs = model(data.x, data.edge_index, data), []\n",
        "    for _, mask in data('train_mask', 'test_mask'):\n",
        "        pred = logits[mask].max(1)[1]\n",
        "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
        "        accs.append(acc)\n",
        "    return accs"
      ],
      "metadata": {
        "id": "yqlvNvacJb3g"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_logits[data.train_mask], data.y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "Z-SEDz0zgaQ_",
        "outputId": "aae702ba-fe5a-4fc5-abbb-5a9238623ac2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ddf93350c97d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlog_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'log_logits' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = 999.0\n",
        "train_acc = 0.0\n",
        "test_acc = 0.0\n",
        "\n",
        "t = trange(epochs, desc=\"Stats: \", position=0)\n",
        "\n",
        "for epoch in t:\n",
        "\n",
        "    model.train()\n",
        "    \n",
        "    loss = 0\n",
        "\n",
        "    data = data.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    log_logits = model(data.x, data.edge_index, data)\n",
        "\n",
        "    # Since the data is a single huge graph, training on the training set is done by masking the nodes that are not in the training set.\n",
        "    loss = F.nll_loss(log_logits[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # validate\n",
        "    train_acc, test_acc = test(model, data)\n",
        "    train_loss = loss\n",
        "    \n",
        "    t.set_description('[Train_loss:{:.6f} Train_acc: {:.4f}, Test_acc: {:.4f}]'.format(loss, train_acc, test_acc))"
      ],
      "metadata": {
        "id": "j9tfQ9BaJd5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use experiments from Github"
      ],
      "metadata": {
        "id": "rKDxT1VcRHzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/RexYing/gnn-model-explainer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gq7hmEnGRHKN",
        "outputId": "29be1f93-7c13-43bc-9d6a-ea831862a495"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gnn-model-explainer'...\n",
            "remote: Enumerating objects: 902, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 902 (delta 8), reused 11 (delta 2), pack-reused 880\u001b[K\n",
            "Receiving objects: 100% (902/902), 308.94 KiB | 4.17 MiB/s, done.\n",
            "Resolving deltas: 100% (561/561), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dir_model = 'gnn-model-explainer'\n",
        "os.chdir(dir_model)"
      ],
      "metadata": {
        "id": "r8CGWIyrRP_i"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUsN5hc4RjRA",
        "outputId": "025cf6ca-e430-4566-e55a-ac9487fac5f2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "os.chdir('/content/gnn-model-explainer')\n",
        "\n",
        "def fxn():\n",
        "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
        "    warnings.warn(\"User\", UserWarning)\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    fxn()\n",
        "    !python train.py --dataset=syn2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsncgvqtR6S6",
        "outputId": "2fb740ef-2264-4f5d-c93e-4263a3583527"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CPU\n",
            "Method: base\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  0 ; loss:  2.089625597000122 ; train_acc:  0.10714285714285714 ; test_acc:  0.10714285714285714 ; train_prec:  0.04020172361724116 ; test_prec:  0.030350978135788263 ; epoch time:  0.06\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  10 ; loss:  2.0179555416107178 ; train_acc:  0.14642857142857144 ; test_acc:  0.1357142857142857 ; train_prec:  0.1537183067255482 ; test_prec:  0.1606869103773585 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  20 ; loss:  1.9506025314331055 ; train_acc:  0.4330357142857143 ; test_acc:  0.42142857142857143 ; train_prec:  0.20729166309369984 ; test_prec:  0.183150721214458 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  30 ; loss:  1.8873385190963745 ; train_acc:  0.44375 ; test_acc:  0.425 ; train_prec:  0.19002296987661052 ; test_prec:  0.1306390977443609 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  40 ; loss:  1.8267396688461304 ; train_acc:  0.4348214285714286 ; test_acc:  0.42142857142857143 ; train_prec:  0.17403446226975638 ; test_prec:  0.1058632553807947 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  50 ; loss:  1.768041729927063 ; train_acc:  0.43125 ; test_acc:  0.42142857142857143 ; train_prec:  0.1394564756300949 ; test_prec:  0.10552014802631579 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  60 ; loss:  1.7114933729171753 ; train_acc:  0.43125 ; test_acc:  0.42142857142857143 ; train_prec:  0.1393719806763285 ; test_prec:  0.10544689152420555 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  70 ; loss:  1.6574504375457764 ; train_acc:  0.43392857142857144 ; test_acc:  0.425 ; train_prec:  0.17098777442682492 ; test_prec:  0.1687109375 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  80 ; loss:  1.6060718297958374 ; train_acc:  0.43660714285714286 ; test_acc:  0.425 ; train_prec:  0.22180968368388138 ; test_prec:  0.1687109375 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  90 ; loss:  1.5576350688934326 ; train_acc:  0.4392857142857143 ; test_acc:  0.42857142857142855 ; train_prec:  0.21794091710758376 ; test_prec:  0.157343317727176 ; epoch time:  0.06\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  100 ; loss:  1.5121742486953735 ; train_acc:  0.44375 ; test_acc:  0.4357142857142857 ; train_prec:  0.16110298895386613 ; test_prec:  0.15925099206349208 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  110 ; loss:  1.469202995300293 ; train_acc:  0.4517857142857143 ; test_acc:  0.4392857142857143 ; train_prec:  0.16548315189702664 ; test_prec:  0.1480543710021322 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  120 ; loss:  1.427621841430664 ; train_acc:  0.4705357142857143 ; test_acc:  0.45357142857142857 ; train_prec:  0.35149560181475037 ; test_prec:  0.16674636732776268 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  130 ; loss:  1.3859615325927734 ; train_acc:  0.4928571428571429 ; test_acc:  0.46785714285714286 ; train_prec:  0.3402344086465704 ; test_prec:  0.17408536585365852 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  140 ; loss:  1.3431750535964966 ; train_acc:  0.5125 ; test_acc:  0.5 ; train_prec:  0.3469785304585752 ; test_prec:  0.37150293182269933 ; epoch time:  0.07\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  150 ; loss:  1.2989872694015503 ; train_acc:  0.5383928571428571 ; test_acc:  0.5178571428571429 ; train_prec:  0.35339560555154825 ; test_prec:  0.33008568164508756 ; epoch time:  0.06\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  160 ; loss:  1.2539918422698975 ; train_acc:  0.5696428571428571 ; test_acc:  0.5607142857142857 ; train_prec:  0.4253046546612499 ; test_prec:  0.39484493324292397 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  170 ; loss:  1.2093026638031006 ; train_acc:  0.6035714285714285 ; test_acc:  0.5785714285714286 ; train_prec:  0.4450882761859803 ; test_prec:  0.40710777894185435 ; epoch time:  0.06\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  180 ; loss:  1.1656367778778076 ; train_acc:  0.6357142857142857 ; test_acc:  0.5928571428571429 ; train_prec:  0.4595690433477114 ; test_prec:  0.4010924669012904 ; epoch time:  0.06\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  190 ; loss:  1.1235209703445435 ; train_acc:  0.6669642857142857 ; test_acc:  0.625 ; train_prec:  0.48509782420705444 ; test_prec:  0.434576079357501 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  200 ; loss:  1.0834909677505493 ; train_acc:  0.7035714285714286 ; test_acc:  0.6535714285714286 ; train_prec:  0.5097954325318377 ; test_prec:  0.45218602791632684 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  210 ; loss:  1.046324372291565 ; train_acc:  0.7232142857142857 ; test_acc:  0.6571428571428571 ; train_prec:  0.5215133975589231 ; test_prec:  0.4548653836534614 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  220 ; loss:  1.012136459350586 ; train_acc:  0.7392857142857143 ; test_acc:  0.6785714285714286 ; train_prec:  0.5323046432913808 ; test_prec:  0.48921949760765554 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  230 ; loss:  0.9805472493171692 ; train_acc:  0.7544642857142857 ; test_acc:  0.6892857142857143 ; train_prec:  0.5437123696537922 ; test_prec:  0.4947432574385559 ; epoch time:  0.06\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  240 ; loss:  0.9511308073997498 ; train_acc:  0.7705357142857143 ; test_acc:  0.7071428571428572 ; train_prec:  0.5566767367238157 ; test_prec:  0.5073239630597572 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  250 ; loss:  0.923477292060852 ; train_acc:  0.7785714285714286 ; test_acc:  0.7178571428571429 ; train_prec:  0.5625441543420941 ; test_prec:  0.5116198740284106 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  260 ; loss:  0.8972800374031067 ; train_acc:  0.7892857142857143 ; test_acc:  0.725 ; train_prec:  0.5720713135242335 ; test_prec:  0.5163308913308913 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  270 ; loss:  0.8721718192100525 ; train_acc:  0.8017857142857143 ; test_acc:  0.7321428571428571 ; train_prec:  0.582874700667088 ; test_prec:  0.5244906068275634 ; epoch time:  0.06\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  280 ; loss:  0.8478960990905762 ; train_acc:  0.8169642857142857 ; test_acc:  0.7464285714285714 ; train_prec:  0.5958700693099587 ; test_prec:  0.5329760795964953 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  290 ; loss:  0.824135959148407 ; train_acc:  0.8232142857142857 ; test_acc:  0.7607142857142857 ; train_prec:  0.6014578278383531 ; test_prec:  0.5452529435410998 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  300 ; loss:  0.8006091713905334 ; train_acc:  0.8357142857142857 ; test_acc:  0.7785714285714286 ; train_prec:  0.6138021779078238 ; test_prec:  0.5622314034581013 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  310 ; loss:  0.7771137356758118 ; train_acc:  0.8473214285714286 ; test_acc:  0.7857142857142857 ; train_prec:  0.6250937949765738 ; test_prec:  0.5677265233569113 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  320 ; loss:  0.753553569316864 ; train_acc:  0.8517857142857143 ; test_acc:  0.7821428571428571 ; train_prec:  0.6288238297552035 ; test_prec:  0.5656209304376136 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  330 ; loss:  0.7300559282302856 ; train_acc:  0.8526785714285714 ; test_acc:  0.7821428571428571 ; train_prec:  0.6296975548054098 ; test_prec:  0.56642949687118 ; epoch time:  0.06\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  340 ; loss:  0.7069457173347473 ; train_acc:  0.8580357142857142 ; test_acc:  0.7857142857142857 ; train_prec:  0.6360056136256309 ; test_prec:  0.5701487820777817 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  350 ; loss:  0.6843938827514648 ; train_acc:  0.8598214285714286 ; test_acc:  0.7821428571428571 ; train_prec:  0.6359245168745943 ; test_prec:  0.5685910843485635 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  360 ; loss:  0.6625370979309082 ; train_acc:  0.8580357142857142 ; test_acc:  0.7821428571428571 ; train_prec:  0.6344095542875501 ; test_prec:  0.5686989446523345 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  370 ; loss:  0.6414652466773987 ; train_acc:  0.8580357142857142 ; test_acc:  0.7821428571428571 ; train_prec:  0.6339301067836453 ; test_prec:  0.5689013606219395 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  380 ; loss:  0.6212733387947083 ; train_acc:  0.8580357142857142 ; test_acc:  0.7892857142857143 ; train_prec:  0.6335032084905524 ; test_prec:  0.5760053978890203 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  390 ; loss:  0.6020657420158386 ; train_acc:  0.8589285714285714 ; test_acc:  0.7892857142857143 ; train_prec:  0.633664129180715 ; test_prec:  0.5760053978890203 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  400 ; loss:  0.5838199853897095 ; train_acc:  0.8589285714285714 ; test_acc:  0.7821428571428571 ; train_prec:  0.6343716564342822 ; test_prec:  0.5686130496445645 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  410 ; loss:  0.566596269607544 ; train_acc:  0.8589285714285714 ; test_acc:  0.7857142857142857 ; train_prec:  0.6345242914979756 ; test_prec:  0.571699694881268 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  420 ; loss:  0.5503836870193481 ; train_acc:  0.8598214285714286 ; test_acc:  0.7821428571428571 ; train_prec:  0.7595521004150468 ; test_prec:  0.5683840156424018 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  430 ; loss:  0.5351751446723938 ; train_acc:  0.8642857142857143 ; test_acc:  0.7785714285714286 ; train_prec:  0.8879657435837274 ; test_prec:  0.564902583281659 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  440 ; loss:  0.520759105682373 ; train_acc:  0.8651785714285715 ; test_acc:  0.7857142857142857 ; train_prec:  0.8644929557273779 ; test_prec:  0.5740926567317535 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "epoch:  450 ; loss:  0.507296621799469 ; train_acc:  0.86875 ; test_acc:  0.7928571428571428 ; train_prec:  0.8741448350500074 ; test_prec:  0.6610219875635233 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  460 ; loss:  0.4942260682582855 ; train_acc:  0.8741071428571429 ; test_acc:  0.8 ; train_prec:  0.8819601534415801 ; test_prec:  0.7991023116753323 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  470 ; loss:  0.48196983337402344 ; train_acc:  0.8821428571428571 ; test_acc:  0.8 ; train_prec:  0.8798040302595539 ; test_prec:  0.770079316465677 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  480 ; loss:  0.4702637791633606 ; train_acc:  0.8928571428571429 ; test_acc:  0.8035714285714286 ; train_prec:  0.8921518238913964 ; test_prec:  0.7488229107273101 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  490 ; loss:  0.4592379033565521 ; train_acc:  0.8991071428571429 ; test_acc:  0.8107142857142857 ; train_prec:  0.8978759278656281 ; test_prec:  0.7474071270225238 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  500 ; loss:  0.4492909610271454 ; train_acc:  0.9035714285714286 ; test_acc:  0.8071428571428572 ; train_prec:  0.903073432786046 ; test_prec:  0.7368268394891113 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  510 ; loss:  0.4390603005886078 ; train_acc:  0.9196428571428571 ; test_acc:  0.8107142857142857 ; train_prec:  0.9054087287573999 ; test_prec:  0.7518223379214759 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  520 ; loss:  0.42948928475379944 ; train_acc:  0.9294642857142857 ; test_acc:  0.8178571428571428 ; train_prec:  0.9215287508521459 ; test_prec:  0.7607100846998476 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  530 ; loss:  0.4211220145225525 ; train_acc:  0.93125 ; test_acc:  0.8142857142857143 ; train_prec:  0.92263274584891 ; test_prec:  0.7528975846998476 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  540 ; loss:  0.4122985005378723 ; train_acc:  0.9446428571428571 ; test_acc:  0.8214285714285714 ; train_prec:  0.9320099023895172 ; test_prec:  0.7637426424199054 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  550 ; loss:  0.4041922688484192 ; train_acc:  0.9491071428571428 ; test_acc:  0.8321428571428572 ; train_prec:  0.9358441894391611 ; test_prec:  0.7804280783497631 ; epoch time:  0.07\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  560 ; loss:  0.39680564403533936 ; train_acc:  0.9508928571428571 ; test_acc:  0.8357142857142857 ; train_prec:  0.9358800533083216 ; test_prec:  0.7822769872488172 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  570 ; loss:  0.38937053084373474 ; train_acc:  0.9517857142857142 ; test_acc:  0.8321428571428572 ; train_prec:  0.9392205684100845 ; test_prec:  0.7779972466787541 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  580 ; loss:  0.3824577033519745 ; train_acc:  0.9526785714285714 ; test_acc:  0.8357142857142857 ; train_prec:  0.940172552545952 ; test_prec:  0.7848145034960109 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  590 ; loss:  0.37618738412857056 ; train_acc:  0.9553571428571429 ; test_acc:  0.8392857142857143 ; train_prec:  0.9396404806368303 ; test_prec:  0.7890179314897614 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  600 ; loss:  0.3693656921386719 ; train_acc:  0.9580357142857143 ; test_acc:  0.8392857142857143 ; train_prec:  0.9467447603056379 ; test_prec:  0.7890179314897614 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  610 ; loss:  0.3635624647140503 ; train_acc:  0.9553571428571429 ; test_acc:  0.8392857142857143 ; train_prec:  0.9403425562642238 ; test_prec:  0.7890179314897614 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  620 ; loss:  0.3586215674877167 ; train_acc:  0.9607142857142857 ; test_acc:  0.8464285714285714 ; train_prec:  0.9465402693231535 ; test_prec:  0.7980723931289664 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  630 ; loss:  0.3519119918346405 ; train_acc:  0.9589285714285715 ; test_acc:  0.8392857142857143 ; train_prec:  0.9441766126286464 ; test_prec:  0.7906762458713457 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  640 ; loss:  0.3466808795928955 ; train_acc:  0.9571428571428572 ; test_acc:  0.8428571428571429 ; train_prec:  0.9423503316121099 ; test_prec:  0.7935176573334468 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  650 ; loss:  0.3441055715084076 ; train_acc:  0.9571428571428572 ; test_acc:  0.8392857142857143 ; train_prec:  0.9443380770906451 ; test_prec:  0.7899336740761422 ; epoch time:  0.06\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  660 ; loss:  0.33812081813812256 ; train_acc:  0.9660714285714286 ; test_acc:  0.8357142857142857 ; train_prec:  0.952433200693397 ; test_prec:  0.785978154399207 ; epoch time:  0.06\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  670 ; loss:  0.33247581124305725 ; train_acc:  0.9616071428571429 ; test_acc:  0.8357142857142857 ; train_prec:  0.9493190804539714 ; test_prec:  0.785978154399207 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  680 ; loss:  0.3278455436229706 ; train_acc:  0.9705357142857143 ; test_acc:  0.85 ; train_prec:  0.9610350362292809 ; test_prec:  0.8033680435960382 ; epoch time:  0.06\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  690 ; loss:  0.3242742121219635 ; train_acc:  0.9660714285714286 ; test_acc:  0.8357142857142857 ; train_prec:  0.9558788156461313 ; test_prec:  0.78392187734293 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  700 ; loss:  0.32045063376426697 ; train_acc:  0.9678571428571429 ; test_acc:  0.8321428571428572 ; train_prec:  0.9578409757810366 ; test_prec:  0.7803184510421353 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  710 ; loss:  0.3161834180355072 ; train_acc:  0.9714285714285714 ; test_acc:  0.8321428571428572 ; train_prec:  0.9613087765165726 ; test_prec:  0.7803184510421353 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  720 ; loss:  0.31496569514274597 ; train_acc:  0.9651785714285714 ; test_acc:  0.8321428571428572 ; train_prec:  0.9547298842458025 ; test_prec:  0.7798074806882107 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  730 ; loss:  0.3090035617351532 ; train_acc:  0.9732142857142857 ; test_acc:  0.8464285714285714 ; train_prec:  0.9634396577148011 ; test_prec:  0.799797743973107 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  740 ; loss:  0.30658653378486633 ; train_acc:  0.9732142857142857 ; test_acc:  0.8357142857142857 ; train_prec:  0.9643133014645715 ; test_prec:  0.7848586965150433 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  750 ; loss:  0.30303725600242615 ; train_acc:  0.975 ; test_acc:  0.8392857142857143 ; train_prec:  0.966302958207412 ; test_prec:  0.7906081309041835 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  760 ; loss:  0.30082783102989197 ; train_acc:  0.9767857142857143 ; test_acc:  0.8321428571428572 ; train_prec:  0.9687297327587328 ; test_prec:  0.7795237291947819 ; epoch time:  0.06\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  770 ; loss:  0.29675620794296265 ; train_acc:  0.975 ; test_acc:  0.8428571428571429 ; train_prec:  0.9668238094467972 ; test_prec:  0.7948374265127895 ; epoch time:  0.07\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  780 ; loss:  0.2939811646938324 ; train_acc:  0.9767857142857143 ; test_acc:  0.8392857142857143 ; train_prec:  0.9683872193928523 ; test_prec:  0.7905190995651522 ; epoch time:  0.06\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  790 ; loss:  0.29092490673065186 ; train_acc:  0.9767857142857143 ; test_acc:  0.8321428571428572 ; train_prec:  0.9685746959067968 ; test_prec:  0.7812552702142486 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  800 ; loss:  0.28970760107040405 ; train_acc:  0.975 ; test_acc:  0.8357142857142857 ; train_prec:  0.9670735207832029 ; test_prec:  0.7864937342494643 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  810 ; loss:  0.28775134682655334 ; train_acc:  0.9767857142857143 ; test_acc:  0.8428571428571429 ; train_prec:  0.9687297327587328 ; test_prec:  0.7910864703040693 ; epoch time:  0.06\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  820 ; loss:  0.2843562066555023 ; train_acc:  0.9758928571428571 ; test_acc:  0.8392857142857143 ; train_prec:  0.9679886538211435 ; test_prec:  0.7905190995651522 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  830 ; loss:  0.28314903378486633 ; train_acc:  0.9741071428571428 ; test_acc:  0.8392857142857143 ; train_prec:  0.9657479060370229 ; test_prec:  0.7905190995651522 ; epoch time:  0.06\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  840 ; loss:  0.28014928102493286 ; train_acc:  0.9767857142857143 ; test_acc:  0.8428571428571429 ; train_prec:  0.9679048035278673 ; test_prec:  0.7942547431034273 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  850 ; loss:  0.278519868850708 ; train_acc:  0.9776785714285714 ; test_acc:  0.8392857142857143 ; train_prec:  0.9695693913159406 ; test_prec:  0.7905190995651522 ; epoch time:  0.06\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  860 ; loss:  0.27618613839149475 ; train_acc:  0.9776785714285714 ; test_acc:  0.8357142857142857 ; train_prec:  0.9702224926798071 ; test_prec:  0.7855977070566853 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  870 ; loss:  0.2740035951137543 ; train_acc:  0.9776785714285714 ; test_acc:  0.8392857142857143 ; train_prec:  0.9702224926798071 ; test_prec:  0.7899160340043228 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  880 ; loss:  0.27368974685668945 ; train_acc:  0.9785714285714285 ; test_acc:  0.8392857142857143 ; train_prec:  0.971113638833349 ; test_prec:  0.7858476568299617 ; epoch time:  0.06\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  890 ; loss:  0.2715210020542145 ; train_acc:  0.9758928571428571 ; test_acc:  0.8357142857142857 ; train_prec:  0.9681796488227655 ; test_prec:  0.78392187734293 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  900 ; loss:  0.2703234851360321 ; train_acc:  0.9767857142857143 ; test_acc:  0.8321428571428572 ; train_prec:  0.9697867663088892 ; test_prec:  0.7803588935825778 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  910 ; loss:  0.26965686678886414 ; train_acc:  0.9767857142857143 ; test_acc:  0.8392857142857143 ; train_prec:  0.9692445955514386 ; test_prec:  0.7876575208812051 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  920 ; loss:  0.26811590790748596 ; train_acc:  0.9776785714285714 ; test_acc:  0.8392857142857143 ; train_prec:  0.9704672558960283 ; test_prec:  0.7876575208812051 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  930 ; loss:  0.26597490906715393 ; train_acc:  0.9776785714285714 ; test_acc:  0.8285714285714286 ; train_prec:  0.9704217537172667 ; test_prec:  0.7734370735725224 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  940 ; loss:  0.26686567068099976 ; train_acc:  0.9758928571428571 ; test_acc:  0.8285714285714286 ; train_prec:  0.9687866526365466 ; test_prec:  0.7734370735725224 ; epoch time:  0.06\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  950 ; loss:  0.2637755572795868 ; train_acc:  0.9776785714285714 ; test_acc:  0.8285714285714286 ; train_prec:  0.9704217537172667 ; test_prec:  0.7734144491381333 ; epoch time:  0.07\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  960 ; loss:  0.28024646639823914 ; train_acc:  0.9714285714285714 ; test_acc:  0.825 ; train_prec:  0.9629829869355135 ; test_prec:  0.7732874503792353 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  970 ; loss:  0.2864550054073334 ; train_acc:  0.9696428571428571 ; test_acc:  0.825 ; train_prec:  0.9563684196479476 ; test_prec:  0.7765286510973369 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  980 ; loss:  0.2811051905155182 ; train_acc:  0.9732142857142857 ; test_acc:  0.8357142857142857 ; train_prec:  0.965657903745716 ; test_prec:  0.7779428199670556 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "epoch:  990 ; loss:  0.2717539072036743 ; train_acc:  0.9767857142857143 ; test_acc:  0.8357142857142857 ; train_prec:  0.9701155267622099 ; test_prec:  0.7817754110928452 ; epoch time:  0.05\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "train.py:297: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  nn.utils.clip_grad_norm(model.parameters(), args.clip)\n",
            "[[245   0   0   0   0   0   0   0]\n",
            " [  0 131   3   0   0   0   0   0]\n",
            " [  0   6 112   6   0   0   1   0]\n",
            " [  0   0   0  65   0   0   0   0]\n",
            " [  0   0   0   0 237   0   0   0]\n",
            " [  0   0   0   0   0 123   2   0]\n",
            " [  0   0   0   0   0   2 124   1]\n",
            " [  1   0   0   0   0   1   1  59]]\n",
            "[[55  0  0  0  0  0  0  0]\n",
            " [ 0 22  3  1  0  0  0  0]\n",
            " [ 0  6 19  7  0  3  0  0]\n",
            " [ 1  0  1 11  0  0  0  2]\n",
            " [ 0  0  0  0 63  0  0  0]\n",
            " [ 0  0  0  0  1 27  6  1]\n",
            " [ 0  1  0  1  1  4 22  4]\n",
            " [ 1  0  0  0  0  0  2 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/gnn-model-explainer')\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    fxn()\n",
        "    !python explainer_main.py --dataset=syn2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqu3I_2_Reag",
        "outputId": "862c85aa-c6f3-43d7-c5d6-08f16bbddd76"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mВыходные данные были обрезаны до нескольких последних строк (5000).\u001b[0m\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  2.3811049461364746 ; mask density:  0.022243231534957886 ; pred:  tensor([0.0290, 0.8184, 0.0846, 0.0057, 0.0050, 0.0416, 0.0141, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  2.3505613803863525 ; mask density:  0.022035494446754456 ; pred:  tensor([0.0290, 0.8186, 0.0847, 0.0057, 0.0050, 0.0415, 0.0140, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  2.3208465576171875 ; mask density:  0.021833639591932297 ; pred:  tensor([0.0290, 0.8187, 0.0847, 0.0057, 0.0050, 0.0414, 0.0140, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  2.2919070720672607 ; mask density:  0.021637560799717903 ; pred:  tensor([0.0290, 0.8189, 0.0848, 0.0057, 0.0050, 0.0412, 0.0139, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  2.2637197971343994 ; mask density:  0.021446984261274338 ; pred:  tensor([0.0289, 0.8190, 0.0848, 0.0057, 0.0050, 0.0411, 0.0139, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  2.236255407333374 ; mask density:  0.02126164920628071 ; pred:  tensor([0.0289, 0.8192, 0.0849, 0.0057, 0.0050, 0.0409, 0.0138, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  2.2094855308532715 ; mask density:  0.021081313490867615 ; pred:  tensor([0.0289, 0.8194, 0.0849, 0.0057, 0.0050, 0.0407, 0.0138, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  2.1833817958831787 ; mask density:  0.020905734971165657 ; pred:  tensor([0.0289, 0.8196, 0.0850, 0.0057, 0.0050, 0.0406, 0.0137, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  9.546817779541016\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_580graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  585 89\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  384.8504943847656 ; mask density:  0.7105876207351685 ; pred:  tensor([0.0265, 0.8154, 0.1059, 0.0076, 0.0075, 0.0301, 0.0058, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  374.2934265136719 ; mask density:  0.6897585391998291 ; pred:  tensor([0.0270, 0.8231, 0.0992, 0.0071, 0.0072, 0.0294, 0.0057, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  363.2743835449219 ; mask density:  0.6680832505226135 ; pred:  tensor([0.0276, 0.8295, 0.0938, 0.0068, 0.0069, 0.0287, 0.0057, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  351.8140869140625 ; mask density:  0.645616352558136 ; pred:  tensor([0.0281, 0.8348, 0.0895, 0.0065, 0.0066, 0.0277, 0.0056, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  339.9413146972656 ; mask density:  0.6224260926246643 ; pred:  tensor([0.0286, 0.8386, 0.0865, 0.0063, 0.0063, 0.0270, 0.0057, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  327.6911315917969 ; mask density:  0.5985972881317139 ; pred:  tensor([0.0289, 0.8411, 0.0843, 0.0061, 0.0061, 0.0267, 0.0058, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  315.1055603027344 ; mask density:  0.5742279887199402 ; pred:  tensor([0.0291, 0.8433, 0.0822, 0.0059, 0.0059, 0.0266, 0.0058, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  302.2356872558594 ; mask density:  0.5494289398193359 ; pred:  tensor([0.0292, 0.8450, 0.0807, 0.0058, 0.0057, 0.0265, 0.0059, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  289.1391906738281 ; mask density:  0.5243233442306519 ; pred:  tensor([0.0293, 0.8462, 0.0797, 0.0058, 0.0056, 0.0264, 0.0060, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  275.8803405761719 ; mask density:  0.4990459382534027 ; pred:  tensor([0.0292, 0.8468, 0.0793, 0.0057, 0.0055, 0.0263, 0.0061, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  262.5286560058594 ; mask density:  0.4737391471862793 ; pred:  tensor([0.0291, 0.8470, 0.0793, 0.0057, 0.0054, 0.0261, 0.0062, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  249.1578369140625 ; mask density:  0.44854122400283813 ; pred:  tensor([0.0290, 0.8472, 0.0792, 0.0057, 0.0054, 0.0261, 0.0063, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  235.84413146972656 ; mask density:  0.42359960079193115 ; pred:  tensor([0.0290, 0.8473, 0.0788, 0.0056, 0.0053, 0.0264, 0.0065, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  222.6651611328125 ; mask density:  0.3990585207939148 ; pred:  tensor([0.0289, 0.8473, 0.0783, 0.0056, 0.0052, 0.0269, 0.0067, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  209.69676208496094 ; mask density:  0.37505561113357544 ; pred:  tensor([0.0290, 0.8472, 0.0777, 0.0055, 0.0051, 0.0274, 0.0069, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  197.01158142089844 ; mask density:  0.35171979665756226 ; pred:  tensor([0.0291, 0.8475, 0.0765, 0.0054, 0.0050, 0.0280, 0.0072, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  184.6783447265625 ; mask density:  0.32916638255119324 ; pred:  tensor([0.0293, 0.8476, 0.0755, 0.0054, 0.0049, 0.0287, 0.0074, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  172.75814819335938 ; mask density:  0.30749621987342834 ; pred:  tensor([0.0294, 0.8474, 0.0747, 0.0053, 0.0049, 0.0294, 0.0077, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  161.3040008544922 ; mask density:  0.2867925763130188 ; pred:  tensor([0.0295, 0.8471, 0.0741, 0.0052, 0.0049, 0.0300, 0.0079, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  150.35968017578125 ; mask density:  0.2671189606189728 ; pred:  tensor([0.0295, 0.8468, 0.0736, 0.0052, 0.0049, 0.0306, 0.0081, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  139.9595184326172 ; mask density:  0.24852265417575836 ; pred:  tensor([0.0296, 0.8462, 0.0734, 0.0052, 0.0049, 0.0312, 0.0083, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  130.12762451171875 ; mask density:  0.23103037476539612 ; pred:  tensor([0.0296, 0.8452, 0.0734, 0.0052, 0.0049, 0.0319, 0.0086, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  120.87785339355469 ; mask density:  0.2146509289741516 ; pred:  tensor([0.0296, 0.8442, 0.0735, 0.0052, 0.0049, 0.0325, 0.0088, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  112.2149429321289 ; mask density:  0.1993774026632309 ; pred:  tensor([0.0296, 0.8432, 0.0737, 0.0052, 0.0049, 0.0332, 0.0090, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  104.13539123535156 ; mask density:  0.18518951535224915 ; pred:  tensor([0.0296, 0.8422, 0.0739, 0.0052, 0.0049, 0.0337, 0.0092, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  96.62821197509766 ; mask density:  0.17205452919006348 ; pred:  tensor([0.0296, 0.8412, 0.0742, 0.0052, 0.0049, 0.0342, 0.0094, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  89.67639923095703 ; mask density:  0.15993021428585052 ; pred:  tensor([0.0295, 0.8401, 0.0746, 0.0052, 0.0049, 0.0347, 0.0096, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  83.25774383544922 ; mask density:  0.14876753091812134 ; pred:  tensor([0.0295, 0.8391, 0.0750, 0.0052, 0.0049, 0.0351, 0.0098, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  77.3465347290039 ; mask density:  0.13851310312747955 ; pred:  tensor([0.0294, 0.8382, 0.0755, 0.0052, 0.0049, 0.0355, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  71.91456604003906 ; mask density:  0.12911081314086914 ; pred:  tensor([0.0295, 0.8372, 0.0757, 0.0053, 0.0049, 0.0359, 0.0102, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  66.9317398071289 ; mask density:  0.12050271779298782 ; pred:  tensor([0.0295, 0.8362, 0.0760, 0.0053, 0.0048, 0.0364, 0.0104, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  62.36749267578125 ; mask density:  0.1126304417848587 ; pred:  tensor([0.0297, 0.8353, 0.0759, 0.0053, 0.0048, 0.0370, 0.0107, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  58.191009521484375 ; mask density:  0.10543717443943024 ; pred:  tensor([0.0299, 0.8346, 0.0755, 0.0052, 0.0048, 0.0377, 0.0109, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  54.372291564941406 ; mask density:  0.09886790812015533 ; pred:  tensor([0.0301, 0.8339, 0.0753, 0.0052, 0.0048, 0.0382, 0.0111, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  50.882240295410156 ; mask density:  0.0928696021437645 ; pred:  tensor([0.0303, 0.8334, 0.0750, 0.0052, 0.0048, 0.0386, 0.0113, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  47.69306945800781 ; mask density:  0.08739247918128967 ; pred:  tensor([0.0305, 0.8331, 0.0747, 0.0052, 0.0047, 0.0389, 0.0114, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  44.77872848510742 ; mask density:  0.08238980174064636 ; pred:  tensor([0.0306, 0.8330, 0.0744, 0.0052, 0.0047, 0.0391, 0.0115, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  42.11479568481445 ; mask density:  0.07781841605901718 ; pred:  tensor([0.0307, 0.8330, 0.0742, 0.0052, 0.0047, 0.0392, 0.0116, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  39.67860412597656 ; mask density:  0.07363870739936829 ; pred:  tensor([0.0308, 0.8332, 0.0740, 0.0052, 0.0047, 0.0392, 0.0116, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  37.44921875 ; mask density:  0.06981392204761505 ; pred:  tensor([0.0309, 0.8334, 0.0738, 0.0052, 0.0047, 0.0391, 0.0116, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  35.407474517822266 ; mask density:  0.06631043553352356 ; pred:  tensor([0.0309, 0.8338, 0.0736, 0.0052, 0.0047, 0.0389, 0.0116, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  33.53585433959961 ; mask density:  0.0630982294678688 ; pred:  tensor([0.0309, 0.8342, 0.0735, 0.0052, 0.0047, 0.0387, 0.0115, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  31.818462371826172 ; mask density:  0.06014980748295784 ; pred:  tensor([0.0309, 0.8346, 0.0735, 0.0052, 0.0047, 0.0385, 0.0114, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  30.240793228149414 ; mask density:  0.057440221309661865 ; pred:  tensor([0.0308, 0.8349, 0.0735, 0.0052, 0.0047, 0.0382, 0.0114, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  28.789690017700195 ; mask density:  0.05494659021496773 ; pred:  tensor([0.0308, 0.8353, 0.0735, 0.0052, 0.0047, 0.0380, 0.0113, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  27.453266143798828 ; mask density:  0.052648529410362244 ; pred:  tensor([0.0307, 0.8356, 0.0736, 0.0052, 0.0047, 0.0377, 0.0112, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  26.22079849243164 ; mask density:  0.05052764341235161 ; pred:  tensor([0.0306, 0.8359, 0.0737, 0.0052, 0.0047, 0.0375, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  25.082612991333008 ; mask density:  0.04856729134917259 ; pred:  tensor([0.0306, 0.8361, 0.0738, 0.0052, 0.0047, 0.0373, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  24.02997398376465 ; mask density:  0.0467524528503418 ; pred:  tensor([0.0305, 0.8362, 0.0740, 0.0052, 0.0047, 0.0371, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  23.055034637451172 ; mask density:  0.04506959393620491 ; pred:  tensor([0.0304, 0.8364, 0.0742, 0.0052, 0.0047, 0.0368, 0.0109, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  22.15070343017578 ; mask density:  0.04350651055574417 ; pred:  tensor([0.0303, 0.8364, 0.0745, 0.0052, 0.0047, 0.0366, 0.0108, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  21.31061363220215 ; mask density:  0.042052220553159714 ; pred:  tensor([0.0302, 0.8365, 0.0748, 0.0052, 0.0048, 0.0365, 0.0107, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  20.529014587402344 ; mask density:  0.04069683328270912 ; pred:  tensor([0.0301, 0.8365, 0.0751, 0.0053, 0.0048, 0.0363, 0.0107, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  19.800729751586914 ; mask density:  0.03943144902586937 ; pred:  tensor([0.0300, 0.8364, 0.0754, 0.0053, 0.0048, 0.0361, 0.0106, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  19.121082305908203 ; mask density:  0.03824808821082115 ; pred:  tensor([0.0299, 0.8364, 0.0757, 0.0053, 0.0048, 0.0360, 0.0105, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  18.485877990722656 ; mask density:  0.03713954612612724 ; pred:  tensor([0.0298, 0.8363, 0.0760, 0.0053, 0.0049, 0.0359, 0.0105, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  17.89130401611328 ; mask density:  0.03609934821724892 ; pred:  tensor([0.0297, 0.8362, 0.0763, 0.0053, 0.0049, 0.0358, 0.0104, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  17.333925247192383 ; mask density:  0.03512171655893326 ; pred:  tensor([0.0297, 0.8361, 0.0765, 0.0054, 0.0049, 0.0357, 0.0104, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  16.81062889099121 ; mask density:  0.03420146554708481 ; pred:  tensor([0.0296, 0.8361, 0.0767, 0.0054, 0.0049, 0.0356, 0.0103, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  16.31861114501953 ; mask density:  0.0333339162170887 ; pred:  tensor([0.0296, 0.8361, 0.0768, 0.0054, 0.0050, 0.0356, 0.0103, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  15.855320930480957 ; mask density:  0.03251491114497185 ; pred:  tensor([0.0295, 0.8361, 0.0769, 0.0054, 0.0050, 0.0355, 0.0102, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  15.418455123901367 ; mask density:  0.03174061328172684 ; pred:  tensor([0.0295, 0.8362, 0.0769, 0.0054, 0.0050, 0.0354, 0.0102, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  15.005928039550781 ; mask density:  0.03100757487118244 ; pred:  tensor([0.0295, 0.8363, 0.0769, 0.0054, 0.0050, 0.0353, 0.0102, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  14.615838050842285 ; mask density:  0.030312664806842804 ; pred:  tensor([0.0295, 0.8365, 0.0768, 0.0054, 0.0050, 0.0353, 0.0101, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  14.246465682983398 ; mask density:  0.02965303137898445 ; pred:  tensor([0.0295, 0.8367, 0.0767, 0.0054, 0.0050, 0.0352, 0.0101, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  13.896244049072266 ; mask density:  0.02902587689459324 ; pred:  tensor([0.0295, 0.8369, 0.0766, 0.0054, 0.0050, 0.0352, 0.0101, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  13.563750267028809 ; mask density:  0.028428882360458374 ; pred:  tensor([0.0296, 0.8371, 0.0764, 0.0054, 0.0050, 0.0351, 0.0101, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  13.247676849365234 ; mask density:  0.02785995788872242 ; pred:  tensor([0.0296, 0.8373, 0.0762, 0.0054, 0.0050, 0.0351, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  12.946843147277832 ; mask density:  0.027317171916365623 ; pred:  tensor([0.0296, 0.8375, 0.0760, 0.0054, 0.0050, 0.0351, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  12.660161018371582 ; mask density:  0.02679877169430256 ; pred:  tensor([0.0297, 0.8377, 0.0759, 0.0054, 0.0050, 0.0351, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  12.386638641357422 ; mask density:  0.026303140446543694 ; pred:  tensor([0.0297, 0.8378, 0.0757, 0.0053, 0.0050, 0.0351, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  12.125377655029297 ; mask density:  0.025828801095485687 ; pred:  tensor([0.0297, 0.8379, 0.0756, 0.0053, 0.0050, 0.0351, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  11.875544548034668 ; mask density:  0.02537437528371811 ; pred:  tensor([0.0298, 0.8380, 0.0755, 0.0053, 0.0050, 0.0351, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  11.636380195617676 ; mask density:  0.024938615038990974 ; pred:  tensor([0.0298, 0.8381, 0.0754, 0.0053, 0.0050, 0.0351, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  11.407181739807129 ; mask density:  0.02452036738395691 ; pred:  tensor([0.0298, 0.8381, 0.0753, 0.0053, 0.0050, 0.0352, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  11.187312126159668 ; mask density:  0.024118568748235703 ; pred:  tensor([0.0298, 0.8381, 0.0753, 0.0053, 0.0050, 0.0352, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  10.976179122924805 ; mask density:  0.02373223938047886 ; pred:  tensor([0.0298, 0.8380, 0.0753, 0.0053, 0.0050, 0.0352, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  10.77324390411377 ; mask density:  0.023360468447208405 ; pred:  tensor([0.0298, 0.8380, 0.0753, 0.0053, 0.0050, 0.0353, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  10.578006744384766 ; mask density:  0.023002417758107185 ; pred:  tensor([0.0298, 0.8379, 0.0753, 0.0053, 0.0051, 0.0354, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  10.390008926391602 ; mask density:  0.022657306864857674 ; pred:  tensor([0.0298, 0.8378, 0.0753, 0.0053, 0.0051, 0.0354, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  10.208820343017578 ; mask density:  0.02232440933585167 ; pred:  tensor([0.0298, 0.8377, 0.0753, 0.0053, 0.0051, 0.0355, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  10.034053802490234 ; mask density:  0.02200305089354515 ; pred:  tensor([0.0298, 0.8376, 0.0753, 0.0053, 0.0051, 0.0355, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  9.865340232849121 ; mask density:  0.021692607551813126 ; pred:  tensor([0.0298, 0.8375, 0.0754, 0.0053, 0.0051, 0.0356, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  9.702346801757812 ; mask density:  0.021392496302723885 ; pred:  tensor([0.0298, 0.8374, 0.0754, 0.0053, 0.0051, 0.0356, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  9.544764518737793 ; mask density:  0.021102165803313255 ; pred:  tensor([0.0298, 0.8374, 0.0754, 0.0053, 0.0051, 0.0357, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  9.392297744750977 ; mask density:  0.02082110568881035 ; pred:  tensor([0.0298, 0.8373, 0.0754, 0.0053, 0.0051, 0.0357, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  9.244680404663086 ; mask density:  0.020548854023218155 ; pred:  tensor([0.0298, 0.8373, 0.0754, 0.0053, 0.0051, 0.0357, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  9.101662635803223 ; mask density:  0.020284967496991158 ; pred:  tensor([0.0298, 0.8373, 0.0753, 0.0053, 0.0051, 0.0358, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  8.963013648986816 ; mask density:  0.020029030740261078 ; pred:  tensor([0.0298, 0.8373, 0.0753, 0.0053, 0.0051, 0.0358, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  8.828516006469727 ; mask density:  0.019780665636062622 ; pred:  tensor([0.0298, 0.8373, 0.0753, 0.0053, 0.0051, 0.0358, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  8.697966575622559 ; mask density:  0.01953951269388199 ; pred:  tensor([0.0298, 0.8373, 0.0752, 0.0053, 0.0051, 0.0358, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  8.571178436279297 ; mask density:  0.019305232912302017 ; pred:  tensor([0.0298, 0.8374, 0.0752, 0.0053, 0.0051, 0.0358, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  8.447977066040039 ; mask density:  0.019077513366937637 ; pred:  tensor([0.0298, 0.8374, 0.0751, 0.0053, 0.0051, 0.0358, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  8.328197479248047 ; mask density:  0.018856052309274673 ; pred:  tensor([0.0299, 0.8375, 0.0751, 0.0053, 0.0051, 0.0358, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  8.211685180664062 ; mask density:  0.018640581518411636 ; pred:  tensor([0.0299, 0.8375, 0.0750, 0.0053, 0.0051, 0.0358, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  8.098298072814941 ; mask density:  0.018430892378091812 ; pred:  tensor([0.0299, 0.8375, 0.0749, 0.0053, 0.0051, 0.0359, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  7.987897872924805 ; mask density:  0.018226733431220055 ; pred:  tensor([0.0299, 0.8375, 0.0749, 0.0053, 0.0051, 0.0359, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  7.8803582191467285 ; mask density:  0.018027866259217262 ; pred:  tensor([0.0299, 0.8375, 0.0749, 0.0053, 0.0051, 0.0359, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  7.775559425354004 ; mask density:  0.017834067344665527 ; pred:  tensor([0.0299, 0.8375, 0.0749, 0.0053, 0.0052, 0.0359, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  7.6733903884887695 ; mask density:  0.017645126208662987 ; pred:  tensor([0.0299, 0.8375, 0.0749, 0.0053, 0.0052, 0.0359, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  12.918869256973267\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_585graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  590 79\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  144.68157958984375 ; mask density:  0.7116658091545105 ; pred:  tensor([0.0402, 0.6748, 0.2321, 0.0279, 0.0029, 0.0096, 0.0105, 0.0020],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  140.6715087890625 ; mask density:  0.692177414894104 ; pred:  tensor([0.0379, 0.7417, 0.1774, 0.0186, 0.0028, 0.0111, 0.0091, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  136.5191192626953 ; mask density:  0.6717550158500671 ; pred:  tensor([0.0372, 0.7852, 0.1394, 0.0135, 0.0027, 0.0126, 0.0082, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  132.22036743164062 ; mask density:  0.6505016684532166 ; pred:  tensor([0.0372, 0.8136, 0.1134, 0.0104, 0.0028, 0.0140, 0.0076, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  127.7798843383789 ; mask density:  0.628502607345581 ; pred:  tensor([0.0378, 0.8302, 0.0975, 0.0087, 0.0028, 0.0149, 0.0071, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  123.20372009277344 ; mask density:  0.6058401465415955 ; pred:  tensor([0.0385, 0.8404, 0.0873, 0.0078, 0.0029, 0.0156, 0.0067, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  118.50618743896484 ; mask density:  0.5826196670532227 ; pred:  tensor([0.0390, 0.8456, 0.0816, 0.0072, 0.0030, 0.0163, 0.0065, 0.0008],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  113.70342254638672 ; mask density:  0.5589582920074463 ; pred:  tensor([3.9294e-02, 8.4830e-01, 7.8545e-02, 6.8619e-03, 2.9958e-03, 1.6759e-02,\n",
            "        6.4065e-03, 8.3317e-04], grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  108.8155288696289 ; mask density:  0.5349817276000977 ; pred:  tensor([3.9331e-02, 8.4964e-01, 7.7183e-02, 6.7004e-03, 3.0230e-03, 1.6986e-02,\n",
            "        6.3140e-03, 8.2603e-04], grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  103.86531066894531 ; mask density:  0.5108186602592468 ; pred:  tensor([3.9329e-02, 8.5068e-01, 7.6216e-02, 6.5812e-03, 3.0411e-03, 1.7112e-02,\n",
            "        6.2184e-03, 8.2017e-04], grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  98.87858581542969 ; mask density:  0.48659616708755493 ; pred:  tensor([3.9406e-02, 8.5160e-01, 7.5302e-02, 6.4788e-03, 3.0465e-03, 1.7210e-02,\n",
            "        6.1397e-03, 8.1474e-04], grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  93.88372039794922 ; mask density:  0.4624527096748352 ; pred:  tensor([3.9327e-02, 8.5167e-01, 7.5401e-02, 6.4655e-03, 3.0405e-03, 1.7175e-02,\n",
            "        6.1095e-03, 8.1487e-04], grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  88.90846252441406 ; mask density:  0.43852952122688293 ; pred:  tensor([3.9177e-02, 8.5156e-01, 7.5728e-02, 6.4700e-03, 3.0257e-03, 1.7127e-02,\n",
            "        6.0990e-03, 8.1631e-04], grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  83.98197174072266 ; mask density:  0.41496798396110535 ; pred:  tensor([3.8931e-02, 8.5100e-01, 7.6609e-02, 6.5231e-03, 3.0023e-03, 1.7010e-02,\n",
            "        6.1055e-03, 8.2007e-04], grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  79.13263702392578 ; mask density:  0.39190810918807983 ; pred:  tensor([3.8639e-02, 8.5003e-01, 7.7925e-02, 6.6184e-03, 2.9672e-03, 1.6863e-02,\n",
            "        6.1355e-03, 8.2602e-04], grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  74.38809204101562 ; mask density:  0.3694721758365631 ; pred:  tensor([3.8349e-02, 8.4843e-01, 7.9788e-02, 6.7758e-03, 2.9175e-03, 1.6701e-02,\n",
            "        6.2075e-03, 8.3498e-04], grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  69.77300262451172 ; mask density:  0.34777626395225525 ; pred:  tensor([3.8026e-02, 8.4684e-01, 8.1656e-02, 6.9359e-03, 2.8791e-03, 1.6545e-02,\n",
            "        6.2705e-03, 8.4495e-04], grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  65.31073760986328 ; mask density:  0.32692113518714905 ; pred:  tensor([0.0377, 0.8450, 0.0838, 0.0071, 0.0028, 0.0164, 0.0063, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  61.021141052246094 ; mask density:  0.306994765996933 ; pred:  tensor([0.0373, 0.8431, 0.0860, 0.0073, 0.0028, 0.0163, 0.0064, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  56.921051025390625 ; mask density:  0.2880663275718689 ; pred:  tensor([0.0369, 0.8410, 0.0883, 0.0075, 0.0028, 0.0162, 0.0065, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  53.022666931152344 ; mask density:  0.2701761722564697 ; pred:  tensor([0.0364, 0.8393, 0.0903, 0.0077, 0.0028, 0.0161, 0.0066, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  49.335872650146484 ; mask density:  0.2533554136753082 ; pred:  tensor([0.0361, 0.8378, 0.0919, 0.0078, 0.0028, 0.0160, 0.0067, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  45.86573028564453 ; mask density:  0.23763613402843475 ; pred:  tensor([0.0361, 0.8368, 0.0930, 0.0079, 0.0028, 0.0158, 0.0066, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  42.61457061767578 ; mask density:  0.22300828993320465 ; pred:  tensor([0.0363, 0.8361, 0.0938, 0.0080, 0.0028, 0.0156, 0.0066, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  39.5803337097168 ; mask density:  0.2094302773475647 ; pred:  tensor([0.0364, 0.8365, 0.0935, 0.0080, 0.0028, 0.0154, 0.0065, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  36.759037017822266 ; mask density:  0.1968654990196228 ; pred:  tensor([0.0363, 0.8378, 0.0925, 0.0078, 0.0028, 0.0154, 0.0064, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  34.145652770996094 ; mask density:  0.18527205288410187 ; pred:  tensor([0.0362, 0.8391, 0.0914, 0.0077, 0.0028, 0.0154, 0.0064, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  31.731712341308594 ; mask density:  0.17457851767539978 ; pred:  tensor([0.0363, 0.8407, 0.0899, 0.0076, 0.0027, 0.0154, 0.0064, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  29.507984161376953 ; mask density:  0.16474013030529022 ; pred:  tensor([0.0363, 0.8419, 0.0888, 0.0075, 0.0027, 0.0155, 0.0064, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  27.463787078857422 ; mask density:  0.155690997838974 ; pred:  tensor([0.0362, 0.8430, 0.0879, 0.0074, 0.0027, 0.0155, 0.0064, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  25.587682723999023 ; mask density:  0.14736977219581604 ; pred:  tensor([0.0361, 0.8440, 0.0871, 0.0073, 0.0027, 0.0156, 0.0064, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  23.868284225463867 ; mask density:  0.13972312211990356 ; pred:  tensor([0.0359, 0.8450, 0.0864, 0.0072, 0.0027, 0.0156, 0.0064, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  22.294282913208008 ; mask density:  0.13270074129104614 ; pred:  tensor([0.0357, 0.8458, 0.0859, 0.0071, 0.0027, 0.0156, 0.0063, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  20.85443878173828 ; mask density:  0.1262512505054474 ; pred:  tensor([0.0356, 0.8465, 0.0854, 0.0071, 0.0027, 0.0156, 0.0063, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  19.537939071655273 ; mask density:  0.12030992656946182 ; pred:  tensor([0.0355, 0.8471, 0.0850, 0.0070, 0.0027, 0.0155, 0.0062, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  18.334360122680664 ; mask density:  0.1148301362991333 ; pred:  tensor([0.0354, 0.8476, 0.0848, 0.0070, 0.0028, 0.0154, 0.0061, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  17.23411750793457 ; mask density:  0.10976945608854294 ; pred:  tensor([0.0353, 0.8479, 0.0847, 0.0070, 0.0028, 0.0153, 0.0060, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  16.228031158447266 ; mask density:  0.10509291291236877 ; pred:  tensor([0.0352, 0.8481, 0.0848, 0.0070, 0.0028, 0.0153, 0.0059, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  15.307852745056152 ; mask density:  0.10077757388353348 ; pred:  tensor([0.0350, 0.8481, 0.0850, 0.0070, 0.0029, 0.0152, 0.0059, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  14.465757369995117 ; mask density:  0.09678617119789124 ; pred:  tensor([0.0350, 0.8480, 0.0852, 0.0070, 0.0029, 0.0151, 0.0058, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  13.694194793701172 ; mask density:  0.09309867769479752 ; pred:  tensor([0.0350, 0.8481, 0.0851, 0.0070, 0.0029, 0.0151, 0.0058, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  12.986747741699219 ; mask density:  0.08969156444072723 ; pred:  tensor([0.0350, 0.8484, 0.0850, 0.0070, 0.0029, 0.0151, 0.0057, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  12.337404251098633 ; mask density:  0.08654534816741943 ; pred:  tensor([0.0349, 0.8488, 0.0846, 0.0070, 0.0029, 0.0151, 0.0057, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  11.740739822387695 ; mask density:  0.08363649994134903 ; pred:  tensor([0.0349, 0.8494, 0.0842, 0.0069, 0.0029, 0.0151, 0.0057, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  11.191874504089355 ; mask density:  0.08094442635774612 ; pred:  tensor([0.0348, 0.8500, 0.0837, 0.0069, 0.0029, 0.0151, 0.0057, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  10.686420440673828 ; mask density:  0.0784507542848587 ; pred:  tensor([0.0347, 0.8503, 0.0835, 0.0069, 0.0029, 0.0151, 0.0056, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  10.220272064208984 ; mask density:  0.07614175975322723 ; pred:  tensor([0.0346, 0.8504, 0.0834, 0.0068, 0.0029, 0.0152, 0.0057, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  9.789790153503418 ; mask density:  0.0740029364824295 ; pred:  tensor([0.0345, 0.8504, 0.0835, 0.0068, 0.0029, 0.0152, 0.0057, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  9.39162826538086 ; mask density:  0.07202067971229553 ; pred:  tensor([0.0344, 0.8504, 0.0837, 0.0068, 0.0029, 0.0152, 0.0057, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  9.022856712341309 ; mask density:  0.07018214464187622 ; pred:  tensor([0.0344, 0.8503, 0.0839, 0.0068, 0.0029, 0.0152, 0.0057, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  8.68072509765625 ; mask density:  0.06847696751356125 ; pred:  tensor([0.0343, 0.8502, 0.0840, 0.0068, 0.0029, 0.0152, 0.0057, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  8.362801551818848 ; mask density:  0.06690932810306549 ; pred:  tensor([0.0342, 0.8501, 0.0842, 0.0068, 0.0029, 0.0151, 0.0057, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  8.066514015197754 ; mask density:  0.0654645711183548 ; pred:  tensor([0.0341, 0.8505, 0.0839, 0.0068, 0.0029, 0.0152, 0.0057, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  7.790159225463867 ; mask density:  0.06413063406944275 ; pred:  tensor([0.0341, 0.8512, 0.0833, 0.0067, 0.0029, 0.0152, 0.0057, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  7.532154560089111 ; mask density:  0.06289614737033844 ; pred:  tensor([0.0341, 0.8519, 0.0826, 0.0066, 0.0029, 0.0153, 0.0057, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  7.290913105010986 ; mask density:  0.06173763424158096 ; pred:  tensor([0.0340, 0.8526, 0.0819, 0.0065, 0.0029, 0.0154, 0.0057, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  7.065112113952637 ; mask density:  0.06064719334244728 ; pred:  tensor([0.0340, 0.8533, 0.0813, 0.0064, 0.0029, 0.0155, 0.0057, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  6.853366374969482 ; mask density:  0.05961723253130913 ; pred:  tensor([0.0340, 0.8539, 0.0808, 0.0064, 0.0029, 0.0155, 0.0056, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  6.654539108276367 ; mask density:  0.0586431585252285 ; pred:  tensor([0.0339, 0.8544, 0.0803, 0.0063, 0.0029, 0.0156, 0.0056, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  6.4677510261535645 ; mask density:  0.05772031843662262 ; pred:  tensor([0.0339, 0.8547, 0.0800, 0.0063, 0.0029, 0.0157, 0.0057, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  6.291810035705566 ; mask density:  0.056842606514692307 ; pred:  tensor([0.0339, 0.8551, 0.0796, 0.0062, 0.0029, 0.0158, 0.0057, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  6.125895977020264 ; mask density:  0.056016672402620316 ; pred:  tensor([0.0338, 0.8556, 0.0790, 0.0061, 0.0029, 0.0159, 0.0057, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  5.9693379402160645 ; mask density:  0.05523796007037163 ; pred:  tensor([0.0338, 0.8561, 0.0785, 0.0060, 0.0029, 0.0160, 0.0057, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  5.821313858032227 ; mask density:  0.054494038224220276 ; pred:  tensor([0.0338, 0.8564, 0.0782, 0.0060, 0.0029, 0.0161, 0.0058, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  5.68113899230957 ; mask density:  0.053782057017087936 ; pred:  tensor([0.0338, 0.8564, 0.0781, 0.0060, 0.0029, 0.0162, 0.0058, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  5.548219203948975 ; mask density:  0.05310599505901337 ; pred:  tensor([0.0337, 0.8565, 0.0780, 0.0060, 0.0029, 0.0163, 0.0058, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  5.422021389007568 ; mask density:  0.052461691200733185 ; pred:  tensor([0.0337, 0.8565, 0.0779, 0.0059, 0.0029, 0.0163, 0.0059, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  5.302135467529297 ; mask density:  0.05184265598654747 ; pred:  tensor([0.0337, 0.8566, 0.0778, 0.0059, 0.0028, 0.0164, 0.0059, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  5.187999725341797 ; mask density:  0.05124655365943909 ; pred:  tensor([0.0337, 0.8565, 0.0778, 0.0059, 0.0028, 0.0165, 0.0059, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  5.0792083740234375 ; mask density:  0.050671517848968506 ; pred:  tensor([0.0336, 0.8564, 0.0779, 0.0059, 0.0028, 0.0165, 0.0060, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  4.975385665893555 ; mask density:  0.05011601001024246 ; pred:  tensor([0.0336, 0.8562, 0.0780, 0.0059, 0.0028, 0.0166, 0.0060, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  4.8762311935424805 ; mask density:  0.049582257866859436 ; pred:  tensor([0.0336, 0.8559, 0.0782, 0.0059, 0.0028, 0.0166, 0.0060, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  4.781407833099365 ; mask density:  0.0490712895989418 ; pred:  tensor([0.0335, 0.8557, 0.0784, 0.0059, 0.0028, 0.0166, 0.0061, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  4.690614223480225 ; mask density:  0.04858127981424332 ; pred:  tensor([0.0335, 0.8556, 0.0785, 0.0059, 0.0028, 0.0167, 0.0061, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  4.6035566329956055 ; mask density:  0.048109784722328186 ; pred:  tensor([0.0335, 0.8556, 0.0784, 0.0059, 0.0028, 0.0168, 0.0061, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  4.520020008087158 ; mask density:  0.047654248774051666 ; pred:  tensor([0.0335, 0.8557, 0.0782, 0.0058, 0.0028, 0.0168, 0.0061, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  4.439790725708008 ; mask density:  0.04721251502633095 ; pred:  tensor([0.0334, 0.8559, 0.0781, 0.0058, 0.0028, 0.0169, 0.0062, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  4.362649917602539 ; mask density:  0.046782657504081726 ; pred:  tensor([0.0334, 0.8560, 0.0779, 0.0058, 0.0028, 0.0170, 0.0062, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  4.288405418395996 ; mask density:  0.046362947672605515 ; pred:  tensor([0.0334, 0.8561, 0.0777, 0.0058, 0.0028, 0.0171, 0.0062, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  4.2168803215026855 ; mask density:  0.04595200717449188 ; pred:  tensor([0.0334, 0.8562, 0.0776, 0.0058, 0.0028, 0.0171, 0.0062, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  4.147923469543457 ; mask density:  0.04554633051156998 ; pred:  tensor([0.0333, 0.8563, 0.0775, 0.0057, 0.0028, 0.0172, 0.0062, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  4.081382751464844 ; mask density:  0.04514814168214798 ; pred:  tensor([0.0333, 0.8562, 0.0776, 0.0057, 0.0028, 0.0172, 0.0063, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  4.017143249511719 ; mask density:  0.04475638270378113 ; pred:  tensor([0.0333, 0.8560, 0.0778, 0.0057, 0.0028, 0.0172, 0.0063, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  3.9550716876983643 ; mask density:  0.04437074810266495 ; pred:  tensor([0.0332, 0.8557, 0.0781, 0.0058, 0.0028, 0.0172, 0.0063, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  3.8950185775756836 ; mask density:  0.043996747583150864 ; pred:  tensor([0.0332, 0.8553, 0.0785, 0.0058, 0.0027, 0.0172, 0.0063, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  3.8368589878082275 ; mask density:  0.043632812798023224 ; pred:  tensor([0.0332, 0.8551, 0.0787, 0.0058, 0.0027, 0.0172, 0.0063, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  3.7804932594299316 ; mask density:  0.04327821359038353 ; pred:  tensor([0.0331, 0.8550, 0.0789, 0.0058, 0.0027, 0.0172, 0.0064, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  3.725837230682373 ; mask density:  0.04293021187186241 ; pred:  tensor([0.0331, 0.8549, 0.0790, 0.0058, 0.0027, 0.0173, 0.0064, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  3.672802686691284 ; mask density:  0.04258854687213898 ; pred:  tensor([0.0330, 0.8548, 0.0791, 0.0058, 0.0027, 0.0173, 0.0064, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  3.621328115463257 ; mask density:  0.04225572198629379 ; pred:  tensor([0.0330, 0.8546, 0.0793, 0.0058, 0.0027, 0.0173, 0.0064, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  3.5713441371917725 ; mask density:  0.04192963242530823 ; pred:  tensor([0.0329, 0.8545, 0.0794, 0.0058, 0.0027, 0.0173, 0.0064, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  3.5227346420288086 ; mask density:  0.04160664603114128 ; pred:  tensor([0.0329, 0.8545, 0.0793, 0.0058, 0.0027, 0.0174, 0.0064, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  3.4754350185394287 ; mask density:  0.041283320635557175 ; pred:  tensor([0.0328, 0.8547, 0.0792, 0.0058, 0.0027, 0.0174, 0.0065, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  3.429382801055908 ; mask density:  0.040958039462566376 ; pred:  tensor([0.0328, 0.8547, 0.0792, 0.0057, 0.0027, 0.0174, 0.0065, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  3.384535551071167 ; mask density:  0.040632955729961395 ; pred:  tensor([0.0327, 0.8545, 0.0794, 0.0057, 0.0027, 0.0175, 0.0065, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  3.3408303260803223 ; mask density:  0.040310006588697433 ; pred:  tensor([0.0327, 0.8543, 0.0796, 0.0058, 0.0027, 0.0175, 0.0065, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  3.298229932785034 ; mask density:  0.03999119624495506 ; pred:  tensor([0.0327, 0.8541, 0.0798, 0.0058, 0.0027, 0.0175, 0.0065, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  3.2566635608673096 ; mask density:  0.03967243432998657 ; pred:  tensor([0.0326, 0.8541, 0.0798, 0.0057, 0.0027, 0.0175, 0.0066, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  3.2160696983337402 ; mask density:  0.0393519252538681 ; pred:  tensor([0.0326, 0.8541, 0.0797, 0.0057, 0.0027, 0.0176, 0.0066, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  3.176370143890381 ; mask density:  0.039028193801641464 ; pred:  tensor([0.0326, 0.8542, 0.0796, 0.0057, 0.0027, 0.0177, 0.0066, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  9.844513177871704\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_590graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  595 109\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  66.63081359863281 ; mask density:  0.7131493091583252 ; pred:  tensor([0.0322, 0.3987, 0.4534, 0.0561, 0.0026, 0.0125, 0.0398, 0.0046],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  64.72357940673828 ; mask density:  0.6945381164550781 ; pred:  tensor([0.0310, 0.4636, 0.4060, 0.0428, 0.0025, 0.0141, 0.0363, 0.0037],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  62.75314712524414 ; mask density:  0.6749414801597595 ; pred:  tensor([0.0288, 0.5305, 0.3559, 0.0317, 0.0024, 0.0157, 0.0319, 0.0030],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  60.731536865234375 ; mask density:  0.6544344425201416 ; pred:  tensor([0.0272, 0.5922, 0.3058, 0.0236, 0.0024, 0.0176, 0.0287, 0.0025],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  58.673095703125 ; mask density:  0.633110761642456 ; pred:  tensor([0.0264, 0.6374, 0.2671, 0.0188, 0.0024, 0.0193, 0.0263, 0.0023],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  56.56983184814453 ; mask density:  0.6110735535621643 ; pred:  tensor([0.0263, 0.6711, 0.2372, 0.0158, 0.0025, 0.0206, 0.0244, 0.0021],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  54.42288589477539 ; mask density:  0.5884332060813904 ; pred:  tensor([0.0265, 0.6959, 0.2147, 0.0138, 0.0025, 0.0216, 0.0230, 0.0019],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  52.235740661621094 ; mask density:  0.5653076171875 ; pred:  tensor([0.0268, 0.7141, 0.1982, 0.0125, 0.0026, 0.0223, 0.0217, 0.0019],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  50.01435852050781 ; mask density:  0.5418214797973633 ; pred:  tensor([0.0271, 0.7280, 0.1853, 0.0116, 0.0026, 0.0228, 0.0207, 0.0018],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  47.76616668701172 ; mask density:  0.518104076385498 ; pred:  tensor([0.0275, 0.7399, 0.1743, 0.0109, 0.0027, 0.0232, 0.0198, 0.0018],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  45.501060485839844 ; mask density:  0.49429041147232056 ; pred:  tensor([0.0280, 0.7509, 0.1641, 0.0103, 0.0027, 0.0235, 0.0189, 0.0017],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  43.23276138305664 ; mask density:  0.4705178439617157 ; pred:  tensor([0.0284, 0.7600, 0.1556, 0.0097, 0.0027, 0.0238, 0.0181, 0.0017],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  40.97328186035156 ; mask density:  0.44692423939704895 ; pred:  tensor([0.0288, 0.7677, 0.1482, 0.0093, 0.0028, 0.0241, 0.0175, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  38.735286712646484 ; mask density:  0.42364487051963806 ; pred:  tensor([0.0291, 0.7743, 0.1419, 0.0089, 0.0028, 0.0244, 0.0169, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  36.53165054321289 ; mask density:  0.40081384778022766 ; pred:  tensor([0.0295, 0.7796, 0.1368, 0.0086, 0.0028, 0.0246, 0.0165, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  34.37438201904297 ; mask density:  0.3785512447357178 ; pred:  tensor([0.0297, 0.7840, 0.1326, 0.0084, 0.0029, 0.0247, 0.0161, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  32.27468490600586 ; mask density:  0.3569653332233429 ; pred:  tensor([0.0300, 0.7877, 0.1292, 0.0082, 0.0029, 0.0248, 0.0157, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  30.242815017700195 ; mask density:  0.3361506164073944 ; pred:  tensor([0.0302, 0.7908, 0.1262, 0.0081, 0.0029, 0.0249, 0.0154, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  28.287874221801758 ; mask density:  0.3161862790584564 ; pred:  tensor([0.0304, 0.7935, 0.1237, 0.0079, 0.0029, 0.0250, 0.0152, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  26.417346954345703 ; mask density:  0.29713159799575806 ; pred:  tensor([0.0305, 0.7959, 0.1215, 0.0078, 0.0029, 0.0250, 0.0149, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  24.637237548828125 ; mask density:  0.2790321409702301 ; pred:  tensor([0.0306, 0.7979, 0.1197, 0.0077, 0.0029, 0.0250, 0.0147, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  22.95189094543457 ; mask density:  0.2619171142578125 ; pred:  tensor([0.0307, 0.7997, 0.1182, 0.0076, 0.0029, 0.0250, 0.0145, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  21.364002227783203 ; mask density:  0.24579955637454987 ; pred:  tensor([0.0307, 0.8012, 0.1169, 0.0076, 0.0029, 0.0249, 0.0143, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  19.874635696411133 ; mask density:  0.2306751161813736 ; pred:  tensor([0.0307, 0.8025, 0.1159, 0.0075, 0.0029, 0.0249, 0.0141, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  18.483642578125 ; mask density:  0.21653439104557037 ; pred:  tensor([0.0307, 0.8035, 0.1152, 0.0075, 0.0029, 0.0248, 0.0139, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  17.189624786376953 ; mask density:  0.20335575938224792 ; pred:  tensor([0.0307, 0.8041, 0.1150, 0.0075, 0.0029, 0.0247, 0.0137, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  15.989858627319336 ; mask density:  0.1911092847585678 ; pred:  tensor([0.0306, 0.8044, 0.1150, 0.0075, 0.0029, 0.0246, 0.0137, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  14.880995750427246 ; mask density:  0.17975224554538727 ; pred:  tensor([0.0305, 0.8043, 0.1152, 0.0075, 0.0029, 0.0245, 0.0136, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  13.8583984375 ; mask density:  0.1692417412996292 ; pred:  tensor([0.0304, 0.8041, 0.1157, 0.0076, 0.0029, 0.0244, 0.0136, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  12.917555809020996 ; mask density:  0.1595224291086197 ; pred:  tensor([0.0303, 0.8038, 0.1163, 0.0076, 0.0029, 0.0243, 0.0135, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  12.052901268005371 ; mask density:  0.15054607391357422 ; pred:  tensor([0.0302, 0.8039, 0.1166, 0.0076, 0.0029, 0.0241, 0.0135, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  11.25979232788086 ; mask density:  0.1422586441040039 ; pred:  tensor([0.0300, 0.8039, 0.1170, 0.0076, 0.0029, 0.0239, 0.0133, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  10.533023834228516 ; mask density:  0.13461118936538696 ; pred:  tensor([0.0299, 0.8039, 0.1175, 0.0077, 0.0028, 0.0236, 0.0132, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  9.867645263671875 ; mask density:  0.12755444645881653 ; pred:  tensor([0.0298, 0.8039, 0.1181, 0.0077, 0.0028, 0.0234, 0.0130, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  9.258753776550293 ; mask density:  0.12104803323745728 ; pred:  tensor([0.0297, 0.8038, 0.1186, 0.0077, 0.0028, 0.0231, 0.0129, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  8.701844215393066 ; mask density:  0.11505010724067688 ; pred:  tensor([0.0295, 0.8036, 0.1193, 0.0078, 0.0028, 0.0229, 0.0127, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  8.192352294921875 ; mask density:  0.10952358692884445 ; pred:  tensor([0.0294, 0.8033, 0.1200, 0.0078, 0.0028, 0.0227, 0.0127, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  7.726230621337891 ; mask density:  0.10443254560232162 ; pred:  tensor([0.0292, 0.8029, 0.1208, 0.0078, 0.0028, 0.0226, 0.0126, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  7.299471378326416 ; mask density:  0.09973884373903275 ; pred:  tensor([0.0290, 0.8024, 0.1216, 0.0079, 0.0028, 0.0225, 0.0125, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  6.9084954261779785 ; mask density:  0.09541307389736176 ; pred:  tensor([0.0288, 0.8020, 0.1223, 0.0079, 0.0028, 0.0224, 0.0125, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  6.550266742706299 ; mask density:  0.09142996370792389 ; pred:  tensor([0.0285, 0.8015, 0.1234, 0.0079, 0.0028, 0.0222, 0.0124, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  6.22181510925293 ; mask density:  0.087766133248806 ; pred:  tensor([0.0283, 0.8008, 0.1244, 0.0080, 0.0028, 0.0221, 0.0123, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  5.920325756072998 ; mask density:  0.08439991623163223 ; pred:  tensor([0.0281, 0.8000, 0.1255, 0.0080, 0.0029, 0.0220, 0.0122, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  5.643521308898926 ; mask density:  0.0813092440366745 ; pred:  tensor([0.0279, 0.7990, 0.1267, 0.0081, 0.0029, 0.0219, 0.0122, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  5.3889312744140625 ; mask density:  0.07847414910793304 ; pred:  tensor([0.0276, 0.7980, 0.1280, 0.0082, 0.0029, 0.0218, 0.0122, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  5.154443264007568 ; mask density:  0.07587550580501556 ; pred:  tensor([0.0274, 0.7971, 0.1291, 0.0082, 0.0029, 0.0218, 0.0122, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  4.937742233276367 ; mask density:  0.0734754428267479 ; pred:  tensor([0.0273, 0.7965, 0.1296, 0.0082, 0.0029, 0.0218, 0.0122, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  4.737362384796143 ; mask density:  0.07125936448574066 ; pred:  tensor([0.0272, 0.7961, 0.1300, 0.0082, 0.0029, 0.0219, 0.0122, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  4.551945686340332 ; mask density:  0.06921074539422989 ; pred:  tensor([0.0271, 0.7958, 0.1302, 0.0082, 0.0029, 0.0220, 0.0123, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  4.380014419555664 ; mask density:  0.06731598824262619 ; pred:  tensor([0.0271, 0.7955, 0.1304, 0.0082, 0.0030, 0.0222, 0.0123, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  4.220030784606934 ; mask density:  0.06556043028831482 ; pred:  tensor([0.0270, 0.7956, 0.1303, 0.0082, 0.0030, 0.0223, 0.0123, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  4.070900917053223 ; mask density:  0.06393163651227951 ; pred:  tensor([0.0269, 0.7960, 0.1299, 0.0082, 0.0030, 0.0224, 0.0122, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  3.932234287261963 ; mask density:  0.06242004781961441 ; pred:  tensor([0.0269, 0.7961, 0.1297, 0.0082, 0.0031, 0.0225, 0.0121, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  3.8034067153930664 ; mask density:  0.061014022678136826 ; pred:  tensor([0.0269, 0.7959, 0.1296, 0.0081, 0.0031, 0.0228, 0.0122, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  3.6832258701324463 ; mask density:  0.0597035251557827 ; pred:  tensor([0.0268, 0.7957, 0.1295, 0.0081, 0.0032, 0.0230, 0.0122, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  3.5713205337524414 ; mask density:  0.05848131328821182 ; pred:  tensor([0.0268, 0.7951, 0.1298, 0.0081, 0.0032, 0.0233, 0.0123, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  3.46649169921875 ; mask density:  0.05733901262283325 ; pred:  tensor([0.0268, 0.7947, 0.1298, 0.0081, 0.0032, 0.0235, 0.0124, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  3.3680672645568848 ; mask density:  0.05626874417066574 ; pred:  tensor([0.0268, 0.7946, 0.1297, 0.0081, 0.0033, 0.0237, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  3.2756471633911133 ; mask density:  0.05526365712285042 ; pred:  tensor([0.0268, 0.7945, 0.1295, 0.0081, 0.0033, 0.0239, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  3.188079357147217 ; mask density:  0.054316841065883636 ; pred:  tensor([0.0268, 0.7949, 0.1288, 0.0080, 0.0033, 0.0241, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  3.1050119400024414 ; mask density:  0.05342275649309158 ; pred:  tensor([0.0269, 0.7958, 0.1277, 0.0080, 0.0034, 0.0243, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  3.026516914367676 ; mask density:  0.05257638171315193 ; pred:  tensor([0.0269, 0.7966, 0.1266, 0.0079, 0.0034, 0.0246, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  2.95236873626709 ; mask density:  0.05177298188209534 ; pred:  tensor([0.0270, 0.7973, 0.1254, 0.0079, 0.0035, 0.0249, 0.0126, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  2.882167100906372 ; mask density:  0.051008228212594986 ; pred:  tensor([0.0270, 0.7981, 0.1241, 0.0078, 0.0035, 0.0254, 0.0127, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  2.8157105445861816 ; mask density:  0.050278499722480774 ; pred:  tensor([0.0271, 0.7987, 0.1227, 0.0077, 0.0036, 0.0259, 0.0128, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  2.7527801990509033 ; mask density:  0.04958102107048035 ; pred:  tensor([0.0271, 0.7992, 0.1215, 0.0076, 0.0036, 0.0265, 0.0130, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  2.6931560039520264 ; mask density:  0.04891274496912956 ; pred:  tensor([0.0272, 0.7996, 0.1204, 0.0076, 0.0037, 0.0270, 0.0131, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  2.6365017890930176 ; mask density:  0.04827112704515457 ; pred:  tensor([0.0272, 0.8000, 0.1192, 0.0075, 0.0037, 0.0276, 0.0132, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  2.5826430320739746 ; mask density:  0.04765387251973152 ; pred:  tensor([0.0273, 0.8004, 0.1181, 0.0075, 0.0038, 0.0281, 0.0134, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  2.531377077102661 ; mask density:  0.047059066593647 ; pred:  tensor([0.0273, 0.8007, 0.1171, 0.0074, 0.0038, 0.0286, 0.0135, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  2.48250150680542 ; mask density:  0.04648473858833313 ; pred:  tensor([0.0274, 0.8011, 0.1161, 0.0074, 0.0039, 0.0291, 0.0136, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  2.4358222484588623 ; mask density:  0.04592929035425186 ; pred:  tensor([0.0274, 0.8014, 0.1151, 0.0073, 0.0039, 0.0296, 0.0137, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  2.391216278076172 ; mask density:  0.04539130628108978 ; pred:  tensor([0.0275, 0.8018, 0.1141, 0.0073, 0.0040, 0.0301, 0.0138, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  2.3485488891601562 ; mask density:  0.044869571924209595 ; pred:  tensor([0.0275, 0.8022, 0.1132, 0.0072, 0.0040, 0.0305, 0.0139, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  2.3076860904693604 ; mask density:  0.044363025575876236 ; pred:  tensor([0.0276, 0.8026, 0.1123, 0.0072, 0.0040, 0.0308, 0.0140, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  2.2685089111328125 ; mask density:  0.04387078434228897 ; pred:  tensor([0.0276, 0.8030, 0.1115, 0.0071, 0.0041, 0.0311, 0.0140, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  2.2309327125549316 ; mask density:  0.043393611907958984 ; pred:  tensor([0.0276, 0.8034, 0.1108, 0.0071, 0.0041, 0.0314, 0.0141, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  2.195012092590332 ; mask density:  0.04293077066540718 ; pred:  tensor([0.0276, 0.8038, 0.1102, 0.0071, 0.0041, 0.0315, 0.0141, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  2.1603925228118896 ; mask density:  0.04248157516121864 ; pred:  tensor([0.0277, 0.8043, 0.1097, 0.0070, 0.0041, 0.0316, 0.0141, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  2.12725567817688 ; mask density:  0.04204529523849487 ; pred:  tensor([0.0277, 0.8048, 0.1091, 0.0070, 0.0041, 0.0317, 0.0141, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  2.095371961593628 ; mask density:  0.04162133112549782 ; pred:  tensor([0.0278, 0.8052, 0.1085, 0.0070, 0.0041, 0.0318, 0.0142, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  2.064629554748535 ; mask density:  0.041209179908037186 ; pred:  tensor([0.0278, 0.8056, 0.1080, 0.0070, 0.0041, 0.0319, 0.0142, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  2.034965753555298 ; mask density:  0.04080845043063164 ; pred:  tensor([0.0278, 0.8059, 0.1076, 0.0069, 0.0041, 0.0319, 0.0142, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  2.0063202381134033 ; mask density:  0.04041885957121849 ; pred:  tensor([0.0279, 0.8062, 0.1072, 0.0069, 0.0040, 0.0319, 0.0142, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  1.978636622428894 ; mask density:  0.04004017263650894 ; pred:  tensor([0.0279, 0.8065, 0.1068, 0.0069, 0.0040, 0.0320, 0.0142, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  1.951863169670105 ; mask density:  0.039672233164310455 ; pred:  tensor([0.0279, 0.8068, 0.1065, 0.0069, 0.0040, 0.0320, 0.0142, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  1.9259498119354248 ; mask density:  0.03931491822004318 ; pred:  tensor([0.0280, 0.8070, 0.1063, 0.0069, 0.0040, 0.0320, 0.0143, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  1.9008514881134033 ; mask density:  0.038968127220869064 ; pred:  tensor([0.0280, 0.8072, 0.1061, 0.0069, 0.0040, 0.0320, 0.0143, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  1.8765244483947754 ; mask density:  0.03863178566098213 ; pred:  tensor([0.0280, 0.8074, 0.1059, 0.0069, 0.0040, 0.0320, 0.0143, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  1.8529291152954102 ; mask density:  0.038305800408124924 ; pred:  tensor([0.0280, 0.8075, 0.1058, 0.0069, 0.0040, 0.0320, 0.0143, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  1.8300279378890991 ; mask density:  0.037990059703588486 ; pred:  tensor([0.0280, 0.8076, 0.1057, 0.0069, 0.0040, 0.0320, 0.0143, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  1.8077853918075562 ; mask density:  0.03768440708518028 ; pred:  tensor([0.0280, 0.8077, 0.1056, 0.0069, 0.0040, 0.0320, 0.0143, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  1.7861695289611816 ; mask density:  0.03738866746425629 ; pred:  tensor([0.0280, 0.8077, 0.1055, 0.0069, 0.0040, 0.0320, 0.0143, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  1.7651491165161133 ; mask density:  0.03710395470261574 ; pred:  tensor([0.0280, 0.8077, 0.1055, 0.0068, 0.0040, 0.0320, 0.0143, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  1.7446842193603516 ; mask density:  0.03682977706193924 ; pred:  tensor([0.0280, 0.8078, 0.1054, 0.0068, 0.0040, 0.0321, 0.0143, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  1.724744200706482 ; mask density:  0.03656977787613869 ; pred:  tensor([0.0281, 0.8078, 0.1053, 0.0068, 0.0040, 0.0321, 0.0144, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  1.7052786350250244 ; mask density:  0.03632251173257828 ; pred:  tensor([0.0281, 0.8079, 0.1051, 0.0068, 0.0039, 0.0322, 0.0144, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  1.6862977743148804 ; mask density:  0.036086391657590866 ; pred:  tensor([0.0281, 0.8080, 0.1049, 0.0068, 0.0039, 0.0323, 0.0144, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  1.6677792072296143 ; mask density:  0.03585900366306305 ; pred:  tensor([0.0281, 0.8081, 0.1047, 0.0068, 0.0039, 0.0323, 0.0145, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  1.6496928930282593 ; mask density:  0.03563876450061798 ; pred:  tensor([0.0282, 0.8082, 0.1044, 0.0068, 0.0039, 0.0324, 0.0145, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  9.609511852264404\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_595graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  600 112\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  53.384803771972656 ; mask density:  0.7102621793746948 ; pred:  tensor([0.0165, 0.5540, 0.3646, 0.0216, 0.0033, 0.0162, 0.0213, 0.0025],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  51.87994384765625 ; mask density:  0.6900096535682678 ; pred:  tensor([0.0180, 0.6068, 0.3150, 0.0189, 0.0034, 0.0167, 0.0191, 0.0022],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  50.33271026611328 ; mask density:  0.6688364148139954 ; pred:  tensor([0.0192, 0.6501, 0.2742, 0.0166, 0.0034, 0.0172, 0.0172, 0.0020],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  48.74508285522461 ; mask density:  0.6468287110328674 ; pred:  tensor([0.0203, 0.6815, 0.2442, 0.0149, 0.0034, 0.0178, 0.0160, 0.0019],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  47.111122131347656 ; mask density:  0.6240756511688232 ; pred:  tensor([0.0211, 0.7061, 0.2206, 0.0136, 0.0034, 0.0184, 0.0150, 0.0018],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  45.429107666015625 ; mask density:  0.6006650328636169 ; pred:  tensor([0.0219, 0.7278, 0.1998, 0.0124, 0.0035, 0.0189, 0.0140, 0.0017],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  43.70392990112305 ; mask density:  0.5766977071762085 ; pred:  tensor([0.0226, 0.7469, 0.1813, 0.0114, 0.0035, 0.0194, 0.0132, 0.0017],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  41.943084716796875 ; mask density:  0.5522881746292114 ; pred:  tensor([0.0233, 0.7630, 0.1657, 0.0105, 0.0035, 0.0199, 0.0125, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  40.1544075012207 ; mask density:  0.5275580883026123 ; pred:  tensor([0.0239, 0.7756, 0.1532, 0.0098, 0.0035, 0.0204, 0.0120, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  38.34321212768555 ; mask density:  0.50263911485672 ; pred:  tensor([0.0244, 0.7873, 0.1419, 0.0091, 0.0035, 0.0208, 0.0116, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  36.519893646240234 ; mask density:  0.47767120599746704 ; pred:  tensor([0.0249, 0.7971, 0.1323, 0.0086, 0.0035, 0.0212, 0.0111, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  34.694313049316406 ; mask density:  0.4528030455112457 ; pred:  tensor([0.0252, 0.8049, 0.1245, 0.0081, 0.0035, 0.0216, 0.0107, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  32.8763542175293 ; mask density:  0.4281803071498871 ; pred:  tensor([0.0255, 0.8111, 0.1183, 0.0077, 0.0035, 0.0221, 0.0104, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  31.07617950439453 ; mask density:  0.40394940972328186 ; pred:  tensor([0.0258, 0.8158, 0.1134, 0.0075, 0.0035, 0.0225, 0.0102, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  29.303808212280273 ; mask density:  0.3802472651004791 ; pred:  tensor([0.0260, 0.8193, 0.1095, 0.0072, 0.0035, 0.0231, 0.0101, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  27.568599700927734 ; mask density:  0.3571793735027313 ; pred:  tensor([0.0262, 0.8220, 0.1061, 0.0070, 0.0035, 0.0237, 0.0101, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  25.879348754882812 ; mask density:  0.3348578214645386 ; pred:  tensor([0.0265, 0.8244, 0.1031, 0.0069, 0.0036, 0.0243, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  24.244701385498047 ; mask density:  0.31338149309158325 ; pred:  tensor([0.0267, 0.8261, 0.1007, 0.0068, 0.0036, 0.0249, 0.0100, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  22.671628952026367 ; mask density:  0.29283788800239563 ; pred:  tensor([0.0269, 0.8274, 0.0986, 0.0067, 0.0037, 0.0255, 0.0100, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  21.166791915893555 ; mask density:  0.2732979357242584 ; pred:  tensor([0.0271, 0.8281, 0.0972, 0.0066, 0.0037, 0.0261, 0.0100, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  19.735332489013672 ; mask density:  0.2548159956932068 ; pred:  tensor([0.0272, 0.8279, 0.0967, 0.0066, 0.0038, 0.0266, 0.0100, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  18.380720138549805 ; mask density:  0.2374110072851181 ; pred:  tensor([0.0272, 0.8270, 0.0970, 0.0066, 0.0038, 0.0270, 0.0101, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  17.104618072509766 ; mask density:  0.22109337151050568 ; pred:  tensor([0.0273, 0.8260, 0.0971, 0.0066, 0.0039, 0.0277, 0.0102, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  15.907611846923828 ; mask density:  0.20585697889328003 ; pred:  tensor([0.0273, 0.8252, 0.0968, 0.0066, 0.0041, 0.0285, 0.0103, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  14.789637565612793 ; mask density:  0.19168610870838165 ; pred:  tensor([0.0273, 0.8243, 0.0964, 0.0066, 0.0042, 0.0295, 0.0104, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  13.749180793762207 ; mask density:  0.17855356633663177 ; pred:  tensor([0.0274, 0.8237, 0.0955, 0.0065, 0.0043, 0.0307, 0.0106, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  12.78465461730957 ; mask density:  0.16642452776432037 ; pred:  tensor([0.0274, 0.8230, 0.0946, 0.0064, 0.0045, 0.0320, 0.0108, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  11.893251419067383 ; mask density:  0.1552535593509674 ; pred:  tensor([0.0275, 0.8222, 0.0934, 0.0063, 0.0046, 0.0337, 0.0109, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  11.07157039642334 ; mask density:  0.14499159157276154 ; pred:  tensor([0.0276, 0.8214, 0.0922, 0.0062, 0.0048, 0.0353, 0.0111, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  10.315884590148926 ; mask density:  0.13558489084243774 ; pred:  tensor([0.0277, 0.8206, 0.0911, 0.0061, 0.0049, 0.0369, 0.0113, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  9.622179985046387 ; mask density:  0.12697918713092804 ; pred:  tensor([0.0278, 0.8198, 0.0901, 0.0060, 0.0050, 0.0383, 0.0115, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  8.986303329467773 ; mask density:  0.11911565810441971 ; pred:  tensor([0.0280, 0.8191, 0.0891, 0.0060, 0.0051, 0.0396, 0.0116, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  8.404108047485352 ; mask density:  0.11194075644016266 ; pred:  tensor([0.0282, 0.8185, 0.0879, 0.0059, 0.0052, 0.0410, 0.0119, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  7.871331691741943 ; mask density:  0.1053943783044815 ; pred:  tensor([0.0284, 0.8181, 0.0870, 0.0058, 0.0052, 0.0419, 0.0120, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  7.384036064147949 ; mask density:  0.09942382574081421 ; pred:  tensor([0.0286, 0.8180, 0.0861, 0.0058, 0.0052, 0.0426, 0.0122, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  6.938512325286865 ; mask density:  0.09398042410612106 ; pred:  tensor([0.0288, 0.8179, 0.0853, 0.0057, 0.0052, 0.0432, 0.0123, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  6.5311431884765625 ; mask density:  0.08901839703321457 ; pred:  tensor([0.0290, 0.8180, 0.0845, 0.0057, 0.0052, 0.0436, 0.0125, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  6.158487796783447 ; mask density:  0.08449412137269974 ; pred:  tensor([0.0292, 0.8184, 0.0838, 0.0057, 0.0052, 0.0438, 0.0125, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  5.817479133605957 ; mask density:  0.0803673267364502 ; pred:  tensor([0.0293, 0.8189, 0.0831, 0.0056, 0.0052, 0.0438, 0.0126, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  5.505270481109619 ; mask density:  0.07660160958766937 ; pred:  tensor([0.0295, 0.8196, 0.0824, 0.0056, 0.0052, 0.0437, 0.0126, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  5.219174861907959 ; mask density:  0.07316334545612335 ; pred:  tensor([0.0296, 0.8204, 0.0818, 0.0056, 0.0051, 0.0434, 0.0126, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  4.956812381744385 ; mask density:  0.07002183794975281 ; pred:  tensor([0.0297, 0.8213, 0.0813, 0.0055, 0.0051, 0.0430, 0.0126, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  4.715998649597168 ; mask density:  0.06714923679828644 ; pred:  tensor([0.0299, 0.8223, 0.0808, 0.0055, 0.0050, 0.0425, 0.0126, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  4.494724750518799 ; mask density:  0.06451962888240814 ; pred:  tensor([0.0300, 0.8233, 0.0803, 0.0055, 0.0049, 0.0420, 0.0126, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  4.291126251220703 ; mask density:  0.06211041659116745 ; pred:  tensor([0.0300, 0.8243, 0.0799, 0.0055, 0.0049, 0.0414, 0.0126, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  4.103589057922363 ; mask density:  0.05990071967244148 ; pred:  tensor([0.0301, 0.8253, 0.0796, 0.0055, 0.0048, 0.0408, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  3.930783748626709 ; mask density:  0.057868897914886475 ; pred:  tensor([0.0302, 0.8261, 0.0793, 0.0054, 0.0048, 0.0403, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  3.771260976791382 ; mask density:  0.05599820241332054 ; pred:  tensor([0.0302, 0.8268, 0.0790, 0.0054, 0.0047, 0.0401, 0.0124, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  3.623664617538452 ; mask density:  0.054275527596473694 ; pred:  tensor([0.0302, 0.8274, 0.0788, 0.0054, 0.0047, 0.0398, 0.0123, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  3.486926555633545 ; mask density:  0.05268660932779312 ; pred:  tensor([0.0301, 0.8280, 0.0786, 0.0054, 0.0047, 0.0395, 0.0122, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  3.360074281692505 ; mask density:  0.05121840536594391 ; pred:  tensor([0.0301, 0.8285, 0.0786, 0.0054, 0.0047, 0.0392, 0.0122, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  3.242211103439331 ; mask density:  0.04985904321074486 ; pred:  tensor([0.0300, 0.8289, 0.0786, 0.0054, 0.0047, 0.0389, 0.0121, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  3.1325511932373047 ; mask density:  0.04859606921672821 ; pred:  tensor([0.0300, 0.8292, 0.0786, 0.0054, 0.0047, 0.0387, 0.0121, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  3.0303690433502197 ; mask density:  0.047419995069503784 ; pred:  tensor([0.0299, 0.8295, 0.0787, 0.0054, 0.0047, 0.0385, 0.0120, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  2.934994697570801 ; mask density:  0.04632236063480377 ; pred:  tensor([0.0298, 0.8296, 0.0788, 0.0054, 0.0046, 0.0383, 0.0120, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  2.8458411693573 ; mask density:  0.04529557377099991 ; pred:  tensor([0.0297, 0.8297, 0.0790, 0.0055, 0.0046, 0.0381, 0.0120, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  2.762378454208374 ; mask density:  0.04433290287852287 ; pred:  tensor([0.0297, 0.8297, 0.0792, 0.0055, 0.0047, 0.0380, 0.0119, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  2.6841320991516113 ; mask density:  0.043428514152765274 ; pred:  tensor([0.0296, 0.8297, 0.0794, 0.0055, 0.0047, 0.0379, 0.0119, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  2.6106724739074707 ; mask density:  0.042577121406793594 ; pred:  tensor([0.0295, 0.8296, 0.0797, 0.0055, 0.0047, 0.0378, 0.0119, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  2.541614294052124 ; mask density:  0.0417720191180706 ; pred:  tensor([0.0294, 0.8294, 0.0799, 0.0055, 0.0047, 0.0378, 0.0119, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  2.4766340255737305 ; mask density:  0.041005127131938934 ; pred:  tensor([0.0293, 0.8292, 0.0802, 0.0055, 0.0047, 0.0378, 0.0119, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  2.4153385162353516 ; mask density:  0.04027389734983444 ; pred:  tensor([0.0293, 0.8289, 0.0804, 0.0055, 0.0047, 0.0378, 0.0120, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  2.3574562072753906 ; mask density:  0.039576105773448944 ; pred:  tensor([0.0292, 0.8285, 0.0807, 0.0056, 0.0047, 0.0379, 0.0120, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  2.3026742935180664 ; mask density:  0.03890937566757202 ; pred:  tensor([0.0291, 0.8282, 0.0809, 0.0056, 0.0047, 0.0380, 0.0120, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  2.2507855892181396 ; mask density:  0.03827188163995743 ; pred:  tensor([0.0291, 0.8278, 0.0811, 0.0056, 0.0047, 0.0381, 0.0121, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  2.201594829559326 ; mask density:  0.03766195848584175 ; pred:  tensor([0.0290, 0.8274, 0.0814, 0.0056, 0.0048, 0.0383, 0.0121, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  2.1549103260040283 ; mask density:  0.03707806393504143 ; pred:  tensor([0.0290, 0.8269, 0.0816, 0.0056, 0.0048, 0.0384, 0.0122, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  2.1106841564178467 ; mask density:  0.036520570516586304 ; pred:  tensor([0.0289, 0.8263, 0.0819, 0.0056, 0.0048, 0.0388, 0.0122, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  2.0686087608337402 ; mask density:  0.035987891256809235 ; pred:  tensor([0.0289, 0.8259, 0.0820, 0.0056, 0.0049, 0.0391, 0.0123, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  2.0284793376922607 ; mask density:  0.03547852113842964 ; pred:  tensor([0.0289, 0.8256, 0.0820, 0.0056, 0.0049, 0.0393, 0.0123, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  1.990146517753601 ; mask density:  0.0349910743534565 ; pred:  tensor([0.0289, 0.8255, 0.0820, 0.0056, 0.0049, 0.0394, 0.0123, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  1.9534837007522583 ; mask density:  0.03452424332499504 ; pred:  tensor([0.0289, 0.8255, 0.0819, 0.0056, 0.0049, 0.0394, 0.0123, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  1.9183827638626099 ; mask density:  0.034076791256666183 ; pred:  tensor([0.0289, 0.8257, 0.0817, 0.0056, 0.0049, 0.0394, 0.0123, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  1.884749412536621 ; mask density:  0.033647555857896805 ; pred:  tensor([0.0290, 0.8260, 0.0815, 0.0056, 0.0049, 0.0393, 0.0123, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  1.8525019884109497 ; mask density:  0.03323543816804886 ; pred:  tensor([0.0290, 0.8264, 0.0812, 0.0056, 0.0048, 0.0392, 0.0123, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  1.8215835094451904 ; mask density:  0.03283937647938728 ; pred:  tensor([0.0291, 0.8268, 0.0809, 0.0056, 0.0048, 0.0390, 0.0123, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  1.7920193672180176 ; mask density:  0.03245843946933746 ; pred:  tensor([0.0292, 0.8271, 0.0806, 0.0056, 0.0048, 0.0390, 0.0123, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  1.7636080980300903 ; mask density:  0.032092928886413574 ; pred:  tensor([0.0292, 0.8272, 0.0804, 0.0055, 0.0048, 0.0391, 0.0123, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  1.7362706661224365 ; mask density:  0.031741853803396225 ; pred:  tensor([0.0292, 0.8273, 0.0803, 0.0055, 0.0048, 0.0391, 0.0123, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  1.7099331617355347 ; mask density:  0.03140431270003319 ; pred:  tensor([0.0293, 0.8274, 0.0801, 0.0055, 0.0048, 0.0391, 0.0123, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  1.6845343112945557 ; mask density:  0.031079484149813652 ; pred:  tensor([0.0293, 0.8273, 0.0801, 0.0055, 0.0048, 0.0392, 0.0124, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  1.660020112991333 ; mask density:  0.03076660819351673 ; pred:  tensor([0.0293, 0.8273, 0.0800, 0.0055, 0.0048, 0.0393, 0.0124, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  1.6363445520401 ; mask density:  0.03046496957540512 ; pred:  tensor([0.0293, 0.8271, 0.0801, 0.0055, 0.0048, 0.0393, 0.0124, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  1.6134637594223022 ; mask density:  0.030173933133482933 ; pred:  tensor([0.0293, 0.8270, 0.0801, 0.0055, 0.0048, 0.0394, 0.0124, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  1.5913392305374146 ; mask density:  0.029892893508076668 ; pred:  tensor([0.0293, 0.8268, 0.0802, 0.0055, 0.0048, 0.0395, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  1.5699907541275024 ; mask density:  0.029621871188282967 ; pred:  tensor([0.0292, 0.8265, 0.0802, 0.0055, 0.0049, 0.0397, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  1.549330711364746 ; mask density:  0.029360255226492882 ; pred:  tensor([0.0292, 0.8263, 0.0803, 0.0055, 0.0049, 0.0398, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  1.529297113418579 ; mask density:  0.029107462614774704 ; pred:  tensor([0.0292, 0.8262, 0.0802, 0.0055, 0.0049, 0.0399, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  1.509856104850769 ; mask density:  0.028862930834293365 ; pred:  tensor([0.0292, 0.8263, 0.0802, 0.0055, 0.0049, 0.0399, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  1.4909796714782715 ; mask density:  0.028626127168536186 ; pred:  tensor([0.0292, 0.8264, 0.0801, 0.0055, 0.0049, 0.0399, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  1.472644567489624 ; mask density:  0.028396546840667725 ; pred:  tensor([0.0293, 0.8266, 0.0800, 0.0055, 0.0049, 0.0398, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  1.4548465013504028 ; mask density:  0.02817276492714882 ; pred:  tensor([0.0293, 0.8268, 0.0799, 0.0055, 0.0048, 0.0397, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  1.4375981092453003 ; mask density:  0.02795454114675522 ; pred:  tensor([0.0293, 0.8269, 0.0798, 0.0055, 0.0048, 0.0397, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  1.4208157062530518 ; mask density:  0.027741659432649612 ; pred:  tensor([0.0293, 0.8270, 0.0797, 0.0055, 0.0048, 0.0397, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  1.4044764041900635 ; mask density:  0.02753392979502678 ; pred:  tensor([0.0293, 0.8269, 0.0797, 0.0055, 0.0048, 0.0397, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  1.3885630369186401 ; mask density:  0.02733117900788784 ; pred:  tensor([0.0293, 0.8269, 0.0797, 0.0055, 0.0048, 0.0397, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  1.3730753660202026 ; mask density:  0.027134066447615623 ; pred:  tensor([0.0293, 0.8267, 0.0798, 0.0055, 0.0048, 0.0398, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  1.3580127954483032 ; mask density:  0.026942312717437744 ; pred:  tensor([0.0293, 0.8267, 0.0798, 0.0055, 0.0049, 0.0399, 0.0126, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  1.3433207273483276 ; mask density:  0.02675562910735607 ; pred:  tensor([0.0293, 0.8267, 0.0798, 0.0055, 0.0049, 0.0399, 0.0126, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  1.3289849758148193 ; mask density:  0.026573743671178818 ; pred:  tensor([0.0293, 0.8268, 0.0797, 0.0055, 0.0048, 0.0399, 0.0126, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  9.349951267242432\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_600graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  605 90\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  140.19505310058594 ; mask density:  0.7111305594444275 ; pred:  tensor([0.0262, 0.8161, 0.1154, 0.0078, 0.0038, 0.0198, 0.0096, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  136.3508758544922 ; mask density:  0.6905070543289185 ; pred:  tensor([0.0272, 0.8334, 0.0987, 0.0068, 0.0038, 0.0203, 0.0085, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  132.34646606445312 ; mask density:  0.6690335869789124 ; pred:  tensor([0.0281, 0.8443, 0.0877, 0.0062, 0.0038, 0.0209, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  128.18638610839844 ; mask density:  0.6467657685279846 ; pred:  tensor([0.0289, 0.8512, 0.0805, 0.0058, 0.0038, 0.0214, 0.0074, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  123.87890625 ; mask density:  0.6237753629684448 ; pred:  tensor([0.0296, 0.8556, 0.0755, 0.0055, 0.0038, 0.0220, 0.0071, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  119.43566131591797 ; mask density:  0.6001443266868591 ; pred:  tensor([0.0301, 0.8585, 0.0717, 0.0052, 0.0038, 0.0228, 0.0069, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  114.8728256225586 ; mask density:  0.5759720206260681 ; pred:  tensor([0.0302, 0.8592, 0.0701, 0.0051, 0.0038, 0.0236, 0.0070, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  110.20873260498047 ; mask density:  0.5513712167739868 ; pred:  tensor([0.0301, 0.8577, 0.0703, 0.0051, 0.0039, 0.0247, 0.0072, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  105.46276092529297 ; mask density:  0.5264658331871033 ; pred:  tensor([0.0299, 0.8554, 0.0705, 0.0051, 0.0039, 0.0266, 0.0075, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  100.65910339355469 ; mask density:  0.5013936758041382 ; pred:  tensor([0.0296, 0.8514, 0.0718, 0.0051, 0.0040, 0.0291, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  95.82186126708984 ; mask density:  0.4762955605983734 ; pred:  tensor([0.0293, 0.8467, 0.0735, 0.0052, 0.0042, 0.0317, 0.0082, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  90.97669219970703 ; mask density:  0.45131930708885193 ; pred:  tensor([0.0292, 0.8423, 0.0750, 0.0053, 0.0043, 0.0341, 0.0086, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  86.15080261230469 ; mask density:  0.42660629749298096 ; pred:  tensor([0.0291, 0.8386, 0.0761, 0.0054, 0.0044, 0.0362, 0.0090, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  81.37139892578125 ; mask density:  0.40229612588882446 ; pred:  tensor([0.0292, 0.8360, 0.0770, 0.0054, 0.0045, 0.0375, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  76.66571044921875 ; mask density:  0.3785187900066376 ; pred:  tensor([0.0294, 0.8348, 0.0769, 0.0054, 0.0044, 0.0384, 0.0094, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  72.06035614013672 ; mask density:  0.3554018437862396 ; pred:  tensor([0.0297, 0.8345, 0.0763, 0.0054, 0.0044, 0.0390, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  67.5795669555664 ; mask density:  0.33304914832115173 ; pred:  tensor([0.0303, 0.8357, 0.0745, 0.0053, 0.0043, 0.0391, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  63.24661636352539 ; mask density:  0.31156134605407715 ; pred:  tensor([0.0309, 0.8372, 0.0728, 0.0052, 0.0043, 0.0389, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  59.08076477050781 ; mask density:  0.29102280735969543 ; pred:  tensor([0.0314, 0.8391, 0.0713, 0.0051, 0.0042, 0.0382, 0.0094, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  55.0985107421875 ; mask density:  0.27149850130081177 ; pred:  tensor([0.0319, 0.8411, 0.0700, 0.0050, 0.0041, 0.0374, 0.0094, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  51.31248092651367 ; mask density:  0.25303250551223755 ; pred:  tensor([0.0323, 0.8429, 0.0689, 0.0049, 0.0040, 0.0365, 0.0093, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  47.73163986206055 ; mask density:  0.23565365374088287 ; pred:  tensor([0.0325, 0.8445, 0.0684, 0.0049, 0.0040, 0.0355, 0.0092, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  44.36100387573242 ; mask density:  0.2193668633699417 ; pred:  tensor([0.0326, 0.8462, 0.0679, 0.0048, 0.0039, 0.0344, 0.0090, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  41.20268249511719 ; mask density:  0.20416560769081116 ; pred:  tensor([0.0326, 0.8477, 0.0675, 0.0048, 0.0039, 0.0335, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  38.255714416503906 ; mask density:  0.19003063440322876 ; pred:  tensor([0.0326, 0.8490, 0.0672, 0.0047, 0.0039, 0.0327, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  35.516258239746094 ; mask density:  0.17693112790584564 ; pred:  tensor([0.0326, 0.8501, 0.0670, 0.0047, 0.0039, 0.0320, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  32.9782600402832 ; mask density:  0.16482731699943542 ; pred:  tensor([0.0326, 0.8511, 0.0669, 0.0047, 0.0038, 0.0313, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  30.63388442993164 ; mask density:  0.15367156267166138 ; pred:  tensor([0.0326, 0.8519, 0.0670, 0.0047, 0.0038, 0.0306, 0.0084, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  28.47403907775879 ; mask density:  0.14341476559638977 ; pred:  tensor([0.0326, 0.8524, 0.0670, 0.0047, 0.0038, 0.0302, 0.0084, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  26.48856544494629 ; mask density:  0.13400109112262726 ; pred:  tensor([0.0326, 0.8527, 0.0671, 0.0046, 0.0037, 0.0298, 0.0084, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  24.66657829284668 ; mask density:  0.1253732442855835 ; pred:  tensor([0.0326, 0.8528, 0.0672, 0.0046, 0.0037, 0.0296, 0.0084, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  22.99700927734375 ; mask density:  0.11747456341981888 ; pred:  tensor([0.0325, 0.8529, 0.0674, 0.0046, 0.0037, 0.0294, 0.0084, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  21.468799591064453 ; mask density:  0.1102486252784729 ; pred:  tensor([0.0325, 0.8528, 0.0677, 0.0046, 0.0036, 0.0293, 0.0084, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  20.071054458618164 ; mask density:  0.10364095121622086 ; pred:  tensor([0.0324, 0.8527, 0.0679, 0.0046, 0.0036, 0.0292, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  18.793554306030273 ; mask density:  0.09759960323572159 ; pred:  tensor([0.0324, 0.8523, 0.0683, 0.0046, 0.0036, 0.0292, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  17.625967025756836 ; mask density:  0.09207525104284286 ; pred:  tensor([0.0323, 0.8519, 0.0686, 0.0047, 0.0036, 0.0292, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  16.55879020690918 ; mask density:  0.08702214062213898 ; pred:  tensor([0.0323, 0.8515, 0.0689, 0.0047, 0.0036, 0.0293, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  15.583157539367676 ; mask density:  0.08239763975143433 ; pred:  tensor([0.0323, 0.8512, 0.0692, 0.0047, 0.0036, 0.0293, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  14.690797805786133 ; mask density:  0.07816258817911148 ; pred:  tensor([0.0322, 0.8508, 0.0695, 0.0047, 0.0036, 0.0294, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  13.874073028564453 ; mask density:  0.07428033649921417 ; pred:  tensor([0.0322, 0.8505, 0.0697, 0.0047, 0.0036, 0.0294, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  13.12598991394043 ; mask density:  0.0707181915640831 ; pred:  tensor([0.0322, 0.8502, 0.0699, 0.0047, 0.0036, 0.0295, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  12.440186500549316 ; mask density:  0.067446269094944 ; pred:  tensor([0.0322, 0.8500, 0.0701, 0.0047, 0.0035, 0.0296, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  11.81080150604248 ; mask density:  0.06443729251623154 ; pred:  tensor([0.0321, 0.8497, 0.0703, 0.0047, 0.0035, 0.0296, 0.0090, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  11.232556343078613 ; mask density:  0.06166672706604004 ; pred:  tensor([0.0321, 0.8495, 0.0705, 0.0047, 0.0035, 0.0296, 0.0090, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  10.700658798217773 ; mask density:  0.0591123066842556 ; pred:  tensor([0.0321, 0.8493, 0.0707, 0.0047, 0.0035, 0.0296, 0.0090, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  10.210756301879883 ; mask density:  0.05675395578145981 ; pred:  tensor([0.0320, 0.8491, 0.0709, 0.0047, 0.0035, 0.0296, 0.0090, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  9.758927345275879 ; mask density:  0.05457352101802826 ; pred:  tensor([0.0320, 0.8489, 0.0711, 0.0047, 0.0035, 0.0295, 0.0091, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  9.34162712097168 ; mask density:  0.052554670721292496 ; pred:  tensor([0.0319, 0.8488, 0.0714, 0.0047, 0.0035, 0.0295, 0.0091, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  8.955667495727539 ; mask density:  0.05068269744515419 ; pred:  tensor([0.0319, 0.8487, 0.0716, 0.0047, 0.0035, 0.0294, 0.0091, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  8.598167419433594 ; mask density:  0.04894435033202171 ; pred:  tensor([0.0318, 0.8486, 0.0718, 0.0047, 0.0035, 0.0293, 0.0091, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  8.266536712646484 ; mask density:  0.04732770845293999 ; pred:  tensor([0.0318, 0.8486, 0.0720, 0.0047, 0.0035, 0.0292, 0.0091, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  7.958440780639648 ; mask density:  0.04582202434539795 ; pred:  tensor([0.0317, 0.8485, 0.0722, 0.0048, 0.0035, 0.0291, 0.0091, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  7.671773433685303 ; mask density:  0.04441763460636139 ; pred:  tensor([0.0317, 0.8486, 0.0724, 0.0048, 0.0034, 0.0289, 0.0091, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  7.404640197753906 ; mask density:  0.0431058332324028 ; pred:  tensor([0.0316, 0.8486, 0.0726, 0.0048, 0.0034, 0.0288, 0.0091, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  7.155332088470459 ; mask density:  0.04187876358628273 ; pred:  tensor([0.0315, 0.8487, 0.0728, 0.0048, 0.0034, 0.0286, 0.0091, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  6.922313690185547 ; mask density:  0.04072931781411171 ; pred:  tensor([0.0315, 0.8488, 0.0730, 0.0048, 0.0034, 0.0284, 0.0091, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  6.7042012214660645 ; mask density:  0.039652131497859955 ; pred:  tensor([0.0314, 0.8489, 0.0732, 0.0048, 0.0034, 0.0282, 0.0091, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  6.499744892120361 ; mask density:  0.038640525192022324 ; pred:  tensor([0.0314, 0.8491, 0.0732, 0.0048, 0.0034, 0.0280, 0.0090, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  6.307814598083496 ; mask density:  0.03768925368785858 ; pred:  tensor([0.0313, 0.8492, 0.0733, 0.0048, 0.0034, 0.0279, 0.0090, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  6.127353668212891 ; mask density:  0.036793533712625504 ; pred:  tensor([0.0312, 0.8493, 0.0734, 0.0048, 0.0034, 0.0278, 0.0090, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  5.957427978515625 ; mask density:  0.035948991775512695 ; pred:  tensor([0.0312, 0.8493, 0.0736, 0.0048, 0.0034, 0.0277, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  5.797194957733154 ; mask density:  0.03515167906880379 ; pred:  tensor([0.0311, 0.8493, 0.0738, 0.0049, 0.0034, 0.0276, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  5.645887851715088 ; mask density:  0.034398019313812256 ; pred:  tensor([0.0310, 0.8492, 0.0740, 0.0049, 0.0034, 0.0275, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  5.502814292907715 ; mask density:  0.03368473798036575 ; pred:  tensor([0.0309, 0.8491, 0.0742, 0.0049, 0.0034, 0.0275, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  5.367343425750732 ; mask density:  0.03300883248448372 ; pred:  tensor([0.0309, 0.8490, 0.0745, 0.0049, 0.0034, 0.0274, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  5.238901138305664 ; mask density:  0.03236759081482887 ; pred:  tensor([0.0308, 0.8488, 0.0748, 0.0049, 0.0034, 0.0274, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  5.117003440856934 ; mask density:  0.03175949305295944 ; pred:  tensor([0.0307, 0.8486, 0.0750, 0.0049, 0.0034, 0.0273, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  5.00106954574585 ; mask density:  0.031182128936052322 ; pred:  tensor([0.0307, 0.8486, 0.0752, 0.0049, 0.0034, 0.0273, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  4.8907599449157715 ; mask density:  0.030632346868515015 ; pred:  tensor([0.0306, 0.8487, 0.0752, 0.0049, 0.0034, 0.0272, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  4.785650253295898 ; mask density:  0.030108245089650154 ; pred:  tensor([0.0306, 0.8488, 0.0753, 0.0050, 0.0034, 0.0271, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  4.68535852432251 ; mask density:  0.029608003795146942 ; pred:  tensor([0.0305, 0.8488, 0.0753, 0.0050, 0.0034, 0.0271, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  4.5895538330078125 ; mask density:  0.029130011796951294 ; pred:  tensor([0.0305, 0.8488, 0.0754, 0.0050, 0.0034, 0.0271, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  4.497937202453613 ; mask density:  0.028672797605395317 ; pred:  tensor([0.0305, 0.8488, 0.0755, 0.0050, 0.0034, 0.0270, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  4.410235404968262 ; mask density:  0.02823501266539097 ; pred:  tensor([0.0304, 0.8487, 0.0756, 0.0050, 0.0034, 0.0270, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  4.326189994812012 ; mask density:  0.027815386652946472 ; pred:  tensor([0.0304, 0.8486, 0.0758, 0.0050, 0.0034, 0.0270, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  4.245569229125977 ; mask density:  0.0274127759039402 ; pred:  tensor([0.0303, 0.8485, 0.0759, 0.0050, 0.0034, 0.0271, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  4.1681599617004395 ; mask density:  0.027026113122701645 ; pred:  tensor([0.0303, 0.8483, 0.0761, 0.0050, 0.0035, 0.0271, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  4.093761444091797 ; mask density:  0.026654420420527458 ; pred:  tensor([0.0302, 0.8482, 0.0762, 0.0050, 0.0035, 0.0271, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  4.022193908691406 ; mask density:  0.02629680000245571 ; pred:  tensor([0.0302, 0.8480, 0.0764, 0.0051, 0.0035, 0.0271, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  3.9532859325408936 ; mask density:  0.02595241740345955 ; pred:  tensor([0.0301, 0.8479, 0.0765, 0.0051, 0.0035, 0.0271, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  3.8868789672851562 ; mask density:  0.025620441883802414 ; pred:  tensor([0.0301, 0.8478, 0.0767, 0.0051, 0.0035, 0.0272, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  3.8228230476379395 ; mask density:  0.025300072506070137 ; pred:  tensor([0.0300, 0.8477, 0.0768, 0.0051, 0.0035, 0.0272, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  3.76100492477417 ; mask density:  0.024990661069750786 ; pred:  tensor([0.0300, 0.8476, 0.0768, 0.0051, 0.0035, 0.0273, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  3.7012674808502197 ; mask density:  0.02469162456691265 ; pred:  tensor([0.0300, 0.8477, 0.0767, 0.0051, 0.0035, 0.0273, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  3.6434996128082275 ; mask density:  0.024402404204010963 ; pred:  tensor([0.0300, 0.8479, 0.0766, 0.0051, 0.0036, 0.0272, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  3.5877695083618164 ; mask density:  0.024122370406985283 ; pred:  tensor([0.0300, 0.8480, 0.0765, 0.0051, 0.0036, 0.0272, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  3.533759832382202 ; mask density:  0.023851051926612854 ; pred:  tensor([0.0300, 0.8480, 0.0765, 0.0051, 0.0036, 0.0272, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  3.4813783168792725 ; mask density:  0.02358793281018734 ; pred:  tensor([0.0299, 0.8478, 0.0766, 0.0051, 0.0036, 0.0273, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  3.430572748184204 ; mask density:  0.02333267405629158 ; pred:  tensor([0.0299, 0.8476, 0.0768, 0.0051, 0.0036, 0.0274, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  3.381268262863159 ; mask density:  0.023084895685315132 ; pred:  tensor([0.0298, 0.8474, 0.0770, 0.0051, 0.0036, 0.0274, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  3.3335344791412354 ; mask density:  0.022844472900032997 ; pred:  tensor([0.0298, 0.8470, 0.0773, 0.0051, 0.0036, 0.0275, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  3.287151575088501 ; mask density:  0.022611064836382866 ; pred:  tensor([0.0298, 0.8469, 0.0774, 0.0051, 0.0036, 0.0275, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  3.2420339584350586 ; mask density:  0.02238435298204422 ; pred:  tensor([0.0298, 0.8469, 0.0774, 0.0051, 0.0036, 0.0275, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  3.198127508163452 ; mask density:  0.022164028137922287 ; pred:  tensor([0.0298, 0.8471, 0.0774, 0.0051, 0.0036, 0.0273, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  3.1553921699523926 ; mask density:  0.02194982022047043 ; pred:  tensor([0.0298, 0.8475, 0.0772, 0.0051, 0.0036, 0.0272, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  3.113821029663086 ; mask density:  0.02174123004078865 ; pred:  tensor([0.0299, 0.8479, 0.0769, 0.0051, 0.0036, 0.0270, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  3.07344913482666 ; mask density:  0.021538039669394493 ; pred:  tensor([0.0299, 0.8481, 0.0768, 0.0051, 0.0035, 0.0270, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  3.0340726375579834 ; mask density:  0.021340232342481613 ; pred:  tensor([0.0299, 0.8481, 0.0769, 0.0051, 0.0035, 0.0269, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  2.995722532272339 ; mask density:  0.021147584542632103 ; pred:  tensor([0.0299, 0.8480, 0.0769, 0.0051, 0.0035, 0.0269, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  2.95832896232605 ; mask density:  0.020959870889782906 ; pred:  tensor([0.0299, 0.8480, 0.0770, 0.0051, 0.0035, 0.0269, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  9.870126485824585\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_605graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  610 83\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  33.27751159667969 ; mask density:  0.7092797160148621 ; pred:  tensor([0.0185, 0.5258, 0.3914, 0.0313, 0.0034, 0.0110, 0.0162, 0.0024],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  32.313941955566406 ; mask density:  0.6898877620697021 ; pred:  tensor([0.0192, 0.5830, 0.3391, 0.0252, 0.0034, 0.0124, 0.0155, 0.0022],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  31.340242385864258 ; mask density:  0.6696157455444336 ; pred:  tensor([0.0200, 0.6276, 0.2974, 0.0211, 0.0034, 0.0136, 0.0149, 0.0020],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  30.34494972229004 ; mask density:  0.6485051512718201 ; pred:  tensor([0.0209, 0.6638, 0.2629, 0.0182, 0.0034, 0.0146, 0.0143, 0.0019],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  29.327112197875977 ; mask density:  0.6266419887542725 ; pred:  tensor([0.0217, 0.6927, 0.2351, 0.0161, 0.0034, 0.0155, 0.0137, 0.0018],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  28.28567123413086 ; mask density:  0.6041152477264404 ; pred:  tensor([0.0225, 0.7161, 0.2123, 0.0144, 0.0034, 0.0163, 0.0132, 0.0017],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  27.221553802490234 ; mask density:  0.5810295343399048 ; pred:  tensor([0.0232, 0.7354, 0.1933, 0.0130, 0.0034, 0.0171, 0.0128, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  26.13759422302246 ; mask density:  0.5574983358383179 ; pred:  tensor([0.0239, 0.7512, 0.1776, 0.0119, 0.0034, 0.0179, 0.0125, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  25.037017822265625 ; mask density:  0.5336476564407349 ; pred:  tensor([0.0245, 0.7641, 0.1647, 0.0111, 0.0035, 0.0185, 0.0122, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  23.924354553222656 ; mask density:  0.5096076726913452 ; pred:  tensor([0.0250, 0.7746, 0.1540, 0.0104, 0.0035, 0.0191, 0.0119, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  22.804006576538086 ; mask density:  0.4855096936225891 ; pred:  tensor([0.0255, 0.7838, 0.1448, 0.0098, 0.0035, 0.0197, 0.0116, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  21.68119239807129 ; mask density:  0.46149909496307373 ; pred:  tensor([0.0259, 0.7920, 0.1366, 0.0093, 0.0035, 0.0201, 0.0113, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  20.5631160736084 ; mask density:  0.43771231174468994 ; pred:  tensor([0.0261, 0.7985, 0.1302, 0.0089, 0.0035, 0.0204, 0.0110, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  19.455121994018555 ; mask density:  0.41428735852241516 ; pred:  tensor([0.0264, 0.8041, 0.1247, 0.0085, 0.0035, 0.0207, 0.0107, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  18.363588333129883 ; mask density:  0.39135390520095825 ; pred:  tensor([0.0265, 0.8088, 0.1202, 0.0082, 0.0035, 0.0210, 0.0105, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  17.29462242126465 ; mask density:  0.3690316677093506 ; pred:  tensor([0.0266, 0.8124, 0.1166, 0.0080, 0.0035, 0.0213, 0.0103, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  16.252944946289062 ; mask density:  0.34743016958236694 ; pred:  tensor([0.0267, 0.8157, 0.1134, 0.0078, 0.0035, 0.0215, 0.0101, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  15.2449951171875 ; mask density:  0.32664039731025696 ; pred:  tensor([0.0267, 0.8179, 0.1113, 0.0077, 0.0035, 0.0217, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  14.273416519165039 ; mask density:  0.3067380487918854 ; pred:  tensor([0.0267, 0.8205, 0.1089, 0.0075, 0.0035, 0.0218, 0.0099, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  13.342254638671875 ; mask density:  0.2877846658229828 ; pred:  tensor([0.0268, 0.8232, 0.1062, 0.0074, 0.0035, 0.0220, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  12.455552101135254 ; mask density:  0.269826740026474 ; pred:  tensor([0.0269, 0.8252, 0.1040, 0.0072, 0.0035, 0.0222, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  11.615612983703613 ; mask density:  0.25289231538772583 ; pred:  tensor([0.0270, 0.8267, 0.1024, 0.0071, 0.0035, 0.0225, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  10.823907852172852 ; mask density:  0.2369929850101471 ; pred:  tensor([0.0271, 0.8275, 0.1011, 0.0070, 0.0035, 0.0229, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  10.079916954040527 ; mask density:  0.22212204337120056 ; pred:  tensor([0.0273, 0.8287, 0.0992, 0.0069, 0.0036, 0.0234, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  9.384443283081055 ; mask density:  0.20825907588005066 ; pred:  tensor([0.0274, 0.8296, 0.0977, 0.0068, 0.0036, 0.0239, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  8.736723899841309 ; mask density:  0.19537585973739624 ; pred:  tensor([0.0275, 0.8303, 0.0965, 0.0067, 0.0036, 0.0244, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  8.136018753051758 ; mask density:  0.18343478441238403 ; pred:  tensor([0.0277, 0.8304, 0.0958, 0.0067, 0.0037, 0.0248, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  7.580167770385742 ; mask density:  0.17239397764205933 ; pred:  tensor([0.0277, 0.8305, 0.0952, 0.0066, 0.0038, 0.0253, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  7.067956447601318 ; mask density:  0.16220813989639282 ; pred:  tensor([0.0276, 0.8297, 0.0953, 0.0066, 0.0038, 0.0257, 0.0100, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  6.5975847244262695 ; mask density:  0.15282687544822693 ; pred:  tensor([0.0274, 0.8280, 0.0964, 0.0066, 0.0039, 0.0262, 0.0102, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  6.165824890136719 ; mask density:  0.1442025750875473 ; pred:  tensor([0.0273, 0.8259, 0.0976, 0.0067, 0.0040, 0.0268, 0.0104, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  5.7698073387146 ; mask density:  0.13628192245960236 ; pred:  tensor([0.0272, 0.8240, 0.0988, 0.0067, 0.0041, 0.0273, 0.0106, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  5.407049655914307 ; mask density:  0.12901444733142853 ; pred:  tensor([0.0271, 0.8221, 0.0999, 0.0068, 0.0042, 0.0278, 0.0108, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  5.0750274658203125 ; mask density:  0.12235020846128464 ; pred:  tensor([0.0271, 0.8204, 0.1009, 0.0068, 0.0042, 0.0283, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  4.7713623046875 ; mask density:  0.11624215543270111 ; pred:  tensor([0.0270, 0.8187, 0.1017, 0.0069, 0.0043, 0.0290, 0.0111, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  4.4934611320495605 ; mask density:  0.11064205318689346 ; pred:  tensor([0.0270, 0.8175, 0.1021, 0.0069, 0.0044, 0.0296, 0.0112, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  4.239105701446533 ; mask density:  0.10550767183303833 ; pred:  tensor([0.0269, 0.8166, 0.1023, 0.0069, 0.0044, 0.0301, 0.0113, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  4.0062456130981445 ; mask density:  0.10079672932624817 ; pred:  tensor([0.0269, 0.8161, 0.1023, 0.0069, 0.0045, 0.0305, 0.0114, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  3.7929277420043945 ; mask density:  0.09647858142852783 ; pred:  tensor([0.0270, 0.8160, 0.1022, 0.0069, 0.0045, 0.0307, 0.0114, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  3.5974512100219727 ; mask density:  0.09253207594156265 ; pred:  tensor([0.0270, 0.8161, 0.1021, 0.0069, 0.0045, 0.0307, 0.0114, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  3.4182260036468506 ; mask density:  0.08892583847045898 ; pred:  tensor([0.0270, 0.8166, 0.1018, 0.0069, 0.0044, 0.0305, 0.0114, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  3.253702402114868 ; mask density:  0.08563058078289032 ; pred:  tensor([0.0270, 0.8173, 0.1013, 0.0068, 0.0044, 0.0303, 0.0114, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  3.1025519371032715 ; mask density:  0.08261838555335999 ; pred:  tensor([0.0271, 0.8183, 0.1008, 0.0068, 0.0043, 0.0300, 0.0114, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  2.9635255336761475 ; mask density:  0.0798666775226593 ; pred:  tensor([0.0272, 0.8195, 0.1000, 0.0067, 0.0042, 0.0296, 0.0113, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  2.8354098796844482 ; mask density:  0.07735098153352737 ; pred:  tensor([0.0273, 0.8210, 0.0991, 0.0067, 0.0041, 0.0293, 0.0113, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  2.7172703742980957 ; mask density:  0.07504728436470032 ; pred:  tensor([0.0274, 0.8226, 0.0980, 0.0066, 0.0040, 0.0288, 0.0112, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  2.6083152294158936 ; mask density:  0.072932668030262 ; pred:  tensor([0.0275, 0.8242, 0.0968, 0.0065, 0.0040, 0.0285, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  2.5077428817749023 ; mask density:  0.07098756730556488 ; pred:  tensor([0.0277, 0.8257, 0.0956, 0.0065, 0.0039, 0.0283, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  2.4147822856903076 ; mask density:  0.06918738037347794 ; pred:  tensor([0.0278, 0.8272, 0.0945, 0.0064, 0.0038, 0.0281, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  2.328824043273926 ; mask density:  0.0675167664885521 ; pred:  tensor([0.0279, 0.8283, 0.0935, 0.0063, 0.0038, 0.0279, 0.0109, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  2.249105930328369 ; mask density:  0.06596291065216064 ; pred:  tensor([0.0281, 0.8294, 0.0927, 0.0063, 0.0037, 0.0277, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  2.1750690937042236 ; mask density:  0.06451365351676941 ; pred:  tensor([0.0282, 0.8302, 0.0921, 0.0063, 0.0037, 0.0276, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  2.1061646938323975 ; mask density:  0.06315827369689941 ; pred:  tensor([0.0283, 0.8310, 0.0915, 0.0062, 0.0036, 0.0275, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  2.0419256687164307 ; mask density:  0.06188744306564331 ; pred:  tensor([0.0283, 0.8316, 0.0911, 0.0062, 0.0036, 0.0273, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  1.9819318056106567 ; mask density:  0.06069289892911911 ; pred:  tensor([0.0284, 0.8320, 0.0908, 0.0062, 0.0036, 0.0272, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  1.9258071184158325 ; mask density:  0.05956749618053436 ; pred:  tensor([0.0284, 0.8324, 0.0905, 0.0062, 0.0035, 0.0270, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  1.873214602470398 ; mask density:  0.05850498378276825 ; pred:  tensor([0.0285, 0.8327, 0.0904, 0.0061, 0.0035, 0.0269, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  1.8238528966903687 ; mask density:  0.05749993398785591 ; pred:  tensor([0.0285, 0.8329, 0.0904, 0.0061, 0.0035, 0.0268, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  1.7774497270584106 ; mask density:  0.05654758960008621 ; pred:  tensor([0.0285, 0.8330, 0.0904, 0.0061, 0.0035, 0.0267, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  1.7337634563446045 ; mask density:  0.055643852800130844 ; pred:  tensor([0.0285, 0.8330, 0.0906, 0.0061, 0.0035, 0.0266, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  1.6925787925720215 ; mask density:  0.054785147309303284 ; pred:  tensor([0.0285, 0.8329, 0.0907, 0.0061, 0.0034, 0.0265, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  1.653699517250061 ; mask density:  0.053968336433172226 ; pred:  tensor([0.0285, 0.8328, 0.0910, 0.0061, 0.0034, 0.0264, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  1.616949200630188 ; mask density:  0.05319061130285263 ; pred:  tensor([0.0285, 0.8326, 0.0913, 0.0061, 0.0034, 0.0263, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  1.5821654796600342 ; mask density:  0.05244928225874901 ; pred:  tensor([0.0285, 0.8324, 0.0916, 0.0061, 0.0034, 0.0262, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  1.5492030382156372 ; mask density:  0.051742035895586014 ; pred:  tensor([0.0284, 0.8321, 0.0919, 0.0062, 0.0034, 0.0261, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  1.5179296731948853 ; mask density:  0.05106671527028084 ; pred:  tensor([0.0284, 0.8319, 0.0923, 0.0062, 0.0034, 0.0261, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  1.4882227182388306 ; mask density:  0.050421297550201416 ; pred:  tensor([0.0284, 0.8316, 0.0926, 0.0062, 0.0034, 0.0260, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  1.4599695205688477 ; mask density:  0.04980383440852165 ; pred:  tensor([0.0284, 0.8313, 0.0930, 0.0062, 0.0034, 0.0259, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  1.4330655336380005 ; mask density:  0.049212418496608734 ; pred:  tensor([0.0283, 0.8310, 0.0933, 0.0062, 0.0034, 0.0259, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  1.4074145555496216 ; mask density:  0.04864514246582985 ; pred:  tensor([0.0283, 0.8307, 0.0936, 0.0062, 0.0033, 0.0258, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  1.3829243183135986 ; mask density:  0.048101916909217834 ; pred:  tensor([0.0283, 0.8305, 0.0939, 0.0062, 0.0033, 0.0258, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  1.359664797782898 ; mask density:  0.0475807823240757 ; pred:  tensor([0.0283, 0.8301, 0.0942, 0.0062, 0.0033, 0.0258, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  1.3375303745269775 ; mask density:  0.047079578042030334 ; pred:  tensor([0.0283, 0.8298, 0.0943, 0.0062, 0.0033, 0.0258, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  1.3162665367126465 ; mask density:  0.04659608379006386 ; pred:  tensor([0.0284, 0.8298, 0.0943, 0.0062, 0.0033, 0.0259, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  1.29581618309021 ; mask density:  0.046128008514642715 ; pred:  tensor([0.0284, 0.8299, 0.0942, 0.0062, 0.0033, 0.0258, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  1.2761305570602417 ; mask density:  0.04567308351397514 ; pred:  tensor([0.0285, 0.8302, 0.0939, 0.0062, 0.0033, 0.0258, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  1.257170557975769 ; mask density:  0.04522918537259102 ; pred:  tensor([0.0286, 0.8307, 0.0935, 0.0062, 0.0033, 0.0257, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  1.2389105558395386 ; mask density:  0.04479234665632248 ; pred:  tensor([0.0286, 0.8312, 0.0931, 0.0061, 0.0032, 0.0257, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  1.2213101387023926 ; mask density:  0.04436114430427551 ; pred:  tensor([0.0287, 0.8318, 0.0926, 0.0061, 0.0032, 0.0256, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  1.204335331916809 ; mask density:  0.04393444582819939 ; pred:  tensor([0.0287, 0.8324, 0.0921, 0.0061, 0.0032, 0.0256, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  1.1879560947418213 ; mask density:  0.04351137951016426 ; pred:  tensor([0.0288, 0.8331, 0.0915, 0.0061, 0.0032, 0.0255, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  1.1721842288970947 ; mask density:  0.04309120774269104 ; pred:  tensor([0.0288, 0.8337, 0.0910, 0.0060, 0.0032, 0.0255, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  1.15707528591156 ; mask density:  0.0426737479865551 ; pred:  tensor([0.0289, 0.8340, 0.0906, 0.0060, 0.0032, 0.0255, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  1.1424171924591064 ; mask density:  0.04225924238562584 ; pred:  tensor([0.0289, 0.8342, 0.0904, 0.0060, 0.0032, 0.0256, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  1.1281821727752686 ; mask density:  0.04184826835989952 ; pred:  tensor([0.0289, 0.8342, 0.0903, 0.0060, 0.0032, 0.0256, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  1.1143487691879272 ; mask density:  0.041441671550273895 ; pred:  tensor([0.0289, 0.8341, 0.0903, 0.0060, 0.0032, 0.0256, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  1.100904107093811 ; mask density:  0.041040435433387756 ; pred:  tensor([0.0289, 0.8340, 0.0904, 0.0060, 0.0032, 0.0257, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  1.087870717048645 ; mask density:  0.04064526781439781 ; pred:  tensor([0.0289, 0.8337, 0.0906, 0.0060, 0.0032, 0.0257, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  1.0752958059310913 ; mask density:  0.04025708884000778 ; pred:  tensor([0.0289, 0.8334, 0.0908, 0.0060, 0.0032, 0.0258, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  1.0630829334259033 ; mask density:  0.039876606315374374 ; pred:  tensor([0.0289, 0.8331, 0.0909, 0.0060, 0.0032, 0.0259, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  1.0512137413024902 ; mask density:  0.03950438275933266 ; pred:  tensor([0.0289, 0.8329, 0.0909, 0.0060, 0.0032, 0.0260, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  1.0396814346313477 ; mask density:  0.03914128243923187 ; pred:  tensor([0.0289, 0.8328, 0.0909, 0.0060, 0.0032, 0.0261, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  1.0284662246704102 ; mask density:  0.038787491619586945 ; pred:  tensor([0.0289, 0.8328, 0.0908, 0.0060, 0.0032, 0.0261, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  1.0175448656082153 ; mask density:  0.03844305872917175 ; pred:  tensor([0.0289, 0.8328, 0.0907, 0.0060, 0.0032, 0.0262, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  1.0069060325622559 ; mask density:  0.03810790926218033 ; pred:  tensor([0.0289, 0.8330, 0.0906, 0.0060, 0.0032, 0.0262, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  0.9965952634811401 ; mask density:  0.037781838327646255 ; pred:  tensor([0.0289, 0.8331, 0.0904, 0.0060, 0.0032, 0.0262, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  0.9865844249725342 ; mask density:  0.03746479004621506 ; pred:  tensor([0.0289, 0.8332, 0.0902, 0.0060, 0.0032, 0.0263, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  0.9768105745315552 ; mask density:  0.03715679049491882 ; pred:  tensor([0.0289, 0.8332, 0.0902, 0.0060, 0.0032, 0.0263, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  0.9672654867172241 ; mask density:  0.03685791790485382 ; pred:  tensor([0.0289, 0.8332, 0.0902, 0.0060, 0.0032, 0.0264, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  0.9579422473907471 ; mask density:  0.03656831383705139 ; pred:  tensor([0.0289, 0.8331, 0.0902, 0.0060, 0.0032, 0.0264, 0.0110, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  9.284208297729492\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_610graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  615 117\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  83.58096313476562 ; mask density:  0.7113166451454163 ; pred:  tensor([0.0289, 0.7444, 0.1612, 0.0133, 0.0053, 0.0291, 0.0151, 0.0028],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  81.27955627441406 ; mask density:  0.6919878721237183 ; pred:  tensor([0.0273, 0.7877, 0.1242, 0.0089, 0.0044, 0.0316, 0.0138, 0.0020],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  78.9056625366211 ; mask density:  0.6716469526290894 ; pred:  tensor([0.0276, 0.8099, 0.1051, 0.0073, 0.0039, 0.0318, 0.0128, 0.0017],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  76.44676971435547 ; mask density:  0.6504181623458862 ; pred:  tensor([0.0282, 0.8229, 0.0947, 0.0065, 0.0036, 0.0307, 0.0119, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  73.9023666381836 ; mask density:  0.6284107565879822 ; pred:  tensor([0.0288, 0.8317, 0.0885, 0.0061, 0.0033, 0.0292, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  71.27782440185547 ; mask density:  0.6057285666465759 ; pred:  tensor([0.0294, 0.8382, 0.0831, 0.0057, 0.0031, 0.0284, 0.0107, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  68.58058166503906 ; mask density:  0.5824751853942871 ; pred:  tensor([0.0301, 0.8440, 0.0779, 0.0054, 0.0030, 0.0280, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  65.82243347167969 ; mask density:  0.5587624311447144 ; pred:  tensor([0.0307, 0.8480, 0.0744, 0.0052, 0.0029, 0.0276, 0.0101, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  63.01526641845703 ; mask density:  0.5347115397453308 ; pred:  tensor([0.0310, 0.8505, 0.0724, 0.0051, 0.0029, 0.0271, 0.0098, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  60.17161178588867 ; mask density:  0.5104494094848633 ; pred:  tensor([0.0312, 0.8525, 0.0712, 0.0050, 0.0029, 0.0267, 0.0095, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  57.30607604980469 ; mask density:  0.48611050844192505 ; pred:  tensor([0.0313, 0.8540, 0.0705, 0.0050, 0.0029, 0.0261, 0.0092, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  54.434268951416016 ; mask density:  0.4618338942527771 ; pred:  tensor([0.0311, 0.8552, 0.0704, 0.0049, 0.0029, 0.0255, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  51.572566986083984 ; mask density:  0.4377593696117401 ; pred:  tensor([0.0309, 0.8561, 0.0705, 0.0049, 0.0029, 0.0249, 0.0086, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  48.737335205078125 ; mask density:  0.414022296667099 ; pred:  tensor([0.0307, 0.8567, 0.0709, 0.0050, 0.0029, 0.0244, 0.0083, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  45.94466018676758 ; mask density:  0.39075517654418945 ; pred:  tensor([0.0307, 0.8573, 0.0710, 0.0050, 0.0029, 0.0240, 0.0081, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  43.210391998291016 ; mask density:  0.3680822253227234 ; pred:  tensor([0.0306, 0.8577, 0.0712, 0.0050, 0.0030, 0.0236, 0.0080, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  40.54927062988281 ; mask density:  0.3461166024208069 ; pred:  tensor([0.0305, 0.8578, 0.0716, 0.0050, 0.0030, 0.0233, 0.0079, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  37.97459411621094 ; mask density:  0.3249562978744507 ; pred:  tensor([0.0304, 0.8576, 0.0722, 0.0051, 0.0030, 0.0230, 0.0078, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  35.49771499633789 ; mask density:  0.3046794831752777 ; pred:  tensor([0.0303, 0.8574, 0.0728, 0.0051, 0.0030, 0.0227, 0.0077, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  33.128238677978516 ; mask density:  0.285347044467926 ; pred:  tensor([0.0302, 0.8573, 0.0732, 0.0051, 0.0030, 0.0226, 0.0077, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  30.873889923095703 ; mask density:  0.2670072615146637 ; pred:  tensor([0.0301, 0.8570, 0.0737, 0.0051, 0.0030, 0.0225, 0.0076, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  28.74001121520996 ; mask density:  0.24968378245830536 ; pred:  tensor([0.0299, 0.8567, 0.0742, 0.0052, 0.0030, 0.0223, 0.0076, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  26.729604721069336 ; mask density:  0.23338915407657623 ; pred:  tensor([0.0300, 0.8567, 0.0742, 0.0051, 0.0030, 0.0224, 0.0076, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  24.844402313232422 ; mask density:  0.21812327206134796 ; pred:  tensor([0.0300, 0.8567, 0.0740, 0.0051, 0.0030, 0.0225, 0.0076, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  23.084178924560547 ; mask density:  0.2038724571466446 ; pred:  tensor([0.0301, 0.8564, 0.0740, 0.0051, 0.0030, 0.0227, 0.0077, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  21.447216033935547 ; mask density:  0.1906176656484604 ; pred:  tensor([0.0301, 0.8557, 0.0744, 0.0051, 0.0031, 0.0229, 0.0078, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  19.930330276489258 ; mask density:  0.17832359671592712 ; pred:  tensor([0.0300, 0.8545, 0.0753, 0.0051, 0.0030, 0.0232, 0.0079, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  18.52839469909668 ; mask density:  0.16694553196430206 ; pred:  tensor([0.0300, 0.8533, 0.0759, 0.0051, 0.0030, 0.0236, 0.0081, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  17.236284255981445 ; mask density:  0.1564360409975052 ; pred:  tensor([0.0300, 0.8519, 0.0765, 0.0051, 0.0031, 0.0242, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  16.047880172729492 ; mask density:  0.14674170315265656 ; pred:  tensor([0.0300, 0.8505, 0.0770, 0.0051, 0.0031, 0.0249, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  14.956801414489746 ; mask density:  0.13781075179576874 ; pred:  tensor([0.0300, 0.8491, 0.0775, 0.0051, 0.0031, 0.0255, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  13.956676483154297 ; mask density:  0.12958791851997375 ; pred:  tensor([0.0300, 0.8477, 0.0781, 0.0051, 0.0031, 0.0261, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  13.040804862976074 ; mask density:  0.12202081084251404 ; pred:  tensor([0.0300, 0.8463, 0.0786, 0.0051, 0.0032, 0.0267, 0.0090, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  12.202726364135742 ; mask density:  0.11505983769893646 ; pred:  tensor([0.0301, 0.8451, 0.0789, 0.0051, 0.0032, 0.0273, 0.0092, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  11.436375617980957 ; mask density:  0.10865665227174759 ; pred:  tensor([0.0303, 0.8439, 0.0791, 0.0051, 0.0032, 0.0279, 0.0094, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  10.735746383666992 ; mask density:  0.10276678949594498 ; pred:  tensor([0.0304, 0.8426, 0.0794, 0.0051, 0.0032, 0.0285, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  10.095120429992676 ; mask density:  0.09734545648097992 ; pred:  tensor([0.0306, 0.8415, 0.0795, 0.0051, 0.0032, 0.0292, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  9.509174346923828 ; mask density:  0.09235316514968872 ; pred:  tensor([0.0307, 0.8405, 0.0795, 0.0051, 0.0032, 0.0298, 0.0100, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  8.973075866699219 ; mask density:  0.08775381743907928 ; pred:  tensor([0.0309, 0.8396, 0.0796, 0.0051, 0.0032, 0.0303, 0.0102, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  8.482216835021973 ; mask density:  0.08351235091686249 ; pred:  tensor([0.0310, 0.8389, 0.0796, 0.0051, 0.0032, 0.0307, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  8.032358169555664 ; mask density:  0.07959704846143723 ; pred:  tensor([0.0311, 0.8383, 0.0796, 0.0050, 0.0032, 0.0311, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  7.619755268096924 ; mask density:  0.07597967237234116 ; pred:  tensor([0.0312, 0.8378, 0.0795, 0.0050, 0.0032, 0.0314, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  7.240927696228027 ; mask density:  0.07263527810573578 ; pred:  tensor([0.0313, 0.8375, 0.0794, 0.0050, 0.0032, 0.0316, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  6.892541885375977 ; mask density:  0.06953860074281693 ; pred:  tensor([0.0314, 0.8374, 0.0792, 0.0050, 0.0032, 0.0318, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  6.571999549865723 ; mask density:  0.0666700080037117 ; pred:  tensor([0.0314, 0.8373, 0.0791, 0.0050, 0.0032, 0.0319, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  6.276690483093262 ; mask density:  0.06401176750659943 ; pred:  tensor([0.0315, 0.8373, 0.0789, 0.0050, 0.0032, 0.0320, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  6.00427770614624 ; mask density:  0.06154751405119896 ; pred:  tensor([0.0315, 0.8373, 0.0789, 0.0050, 0.0032, 0.0320, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  5.752650260925293 ; mask density:  0.05926197022199631 ; pred:  tensor([0.0315, 0.8374, 0.0788, 0.0049, 0.0032, 0.0320, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  5.519892692565918 ; mask density:  0.05714115872979164 ; pred:  tensor([0.0315, 0.8375, 0.0787, 0.0049, 0.0032, 0.0320, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  5.304293155670166 ; mask density:  0.055172089487314224 ; pred:  tensor([0.0315, 0.8376, 0.0787, 0.0049, 0.0032, 0.0319, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  5.1043009757995605 ; mask density:  0.05334267020225525 ; pred:  tensor([0.0315, 0.8377, 0.0787, 0.0049, 0.0032, 0.0318, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  4.918513774871826 ; mask density:  0.05164184421300888 ; pred:  tensor([0.0315, 0.8379, 0.0787, 0.0049, 0.0032, 0.0316, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  4.745670318603516 ; mask density:  0.05005915090441704 ; pred:  tensor([0.0315, 0.8381, 0.0787, 0.0049, 0.0032, 0.0315, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  4.5846266746521 ; mask density:  0.04858485981822014 ; pred:  tensor([0.0315, 0.8382, 0.0787, 0.0049, 0.0032, 0.0313, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  4.434354782104492 ; mask density:  0.047210030257701874 ; pred:  tensor([0.0315, 0.8384, 0.0787, 0.0049, 0.0032, 0.0312, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  4.293924808502197 ; mask density:  0.04592636972665787 ; pred:  tensor([0.0315, 0.8386, 0.0787, 0.0049, 0.0032, 0.0310, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  4.162496566772461 ; mask density:  0.044726308435201645 ; pred:  tensor([0.0315, 0.8388, 0.0787, 0.0049, 0.0032, 0.0308, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  4.03931188583374 ; mask density:  0.04360285773873329 ; pred:  tensor([0.0315, 0.8390, 0.0787, 0.0049, 0.0032, 0.0307, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  3.9236807823181152 ; mask density:  0.042549628764390945 ; pred:  tensor([0.0315, 0.8391, 0.0788, 0.0049, 0.0032, 0.0305, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  3.8149826526641846 ; mask density:  0.041559409350156784 ; pred:  tensor([0.0314, 0.8393, 0.0788, 0.0049, 0.0031, 0.0304, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  3.712675094604492 ; mask density:  0.040627025067806244 ; pred:  tensor([0.0314, 0.8393, 0.0788, 0.0049, 0.0031, 0.0303, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  3.616208791732788 ; mask density:  0.03974781185388565 ; pred:  tensor([0.0314, 0.8393, 0.0789, 0.0049, 0.0031, 0.0303, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  3.5251212120056152 ; mask density:  0.03891754895448685 ; pred:  tensor([0.0314, 0.8392, 0.0790, 0.0049, 0.0031, 0.0303, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  3.4389936923980713 ; mask density:  0.03813241422176361 ; pred:  tensor([0.0314, 0.8391, 0.0790, 0.0049, 0.0031, 0.0303, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  3.3574492931365967 ; mask density:  0.03739034757018089 ; pred:  tensor([0.0314, 0.8390, 0.0791, 0.0049, 0.0032, 0.0304, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  3.2801671028137207 ; mask density:  0.03668805956840515 ; pred:  tensor([0.0313, 0.8389, 0.0792, 0.0049, 0.0032, 0.0305, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  3.20680832862854 ; mask density:  0.03602256253361702 ; pred:  tensor([0.0313, 0.8387, 0.0793, 0.0049, 0.0032, 0.0305, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  3.1370849609375 ; mask density:  0.035391103476285934 ; pred:  tensor([0.0313, 0.8387, 0.0794, 0.0049, 0.0032, 0.0305, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  3.070732593536377 ; mask density:  0.03479119762778282 ; pred:  tensor([0.0313, 0.8386, 0.0794, 0.0049, 0.0032, 0.0305, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  3.0075111389160156 ; mask density:  0.034220535308122635 ; pred:  tensor([0.0313, 0.8386, 0.0795, 0.0049, 0.0032, 0.0305, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  2.947200298309326 ; mask density:  0.033677008002996445 ; pred:  tensor([0.0312, 0.8386, 0.0795, 0.0049, 0.0032, 0.0305, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  2.8895981311798096 ; mask density:  0.03315868228673935 ; pred:  tensor([0.0312, 0.8386, 0.0796, 0.0049, 0.0032, 0.0305, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  2.834521770477295 ; mask density:  0.032663773745298386 ; pred:  tensor([0.0312, 0.8386, 0.0796, 0.0049, 0.0032, 0.0304, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  2.7818026542663574 ; mask density:  0.03219064325094223 ; pred:  tensor([0.0312, 0.8387, 0.0796, 0.0049, 0.0032, 0.0304, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  2.7312862873077393 ; mask density:  0.031737811863422394 ; pred:  tensor([0.0312, 0.8388, 0.0796, 0.0049, 0.0032, 0.0303, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  2.6828317642211914 ; mask density:  0.03130389750003815 ; pred:  tensor([0.0312, 0.8389, 0.0796, 0.0049, 0.0032, 0.0302, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  2.6363091468811035 ; mask density:  0.030887635424733162 ; pred:  tensor([0.0312, 0.8390, 0.0796, 0.0049, 0.0032, 0.0302, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  2.591604709625244 ; mask density:  0.030486486852169037 ; pred:  tensor([0.0312, 0.8392, 0.0796, 0.0049, 0.0032, 0.0301, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  2.548600673675537 ; mask density:  0.030099572613835335 ; pred:  tensor([0.0311, 0.8392, 0.0796, 0.0049, 0.0032, 0.0301, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  2.507185459136963 ; mask density:  0.029726095497608185 ; pred:  tensor([0.0311, 0.8392, 0.0796, 0.0049, 0.0032, 0.0300, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  2.4672799110412598 ; mask density:  0.029366720467805862 ; pred:  tensor([0.0311, 0.8392, 0.0797, 0.0049, 0.0032, 0.0301, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  2.4287948608398438 ; mask density:  0.0290206428617239 ; pred:  tensor([0.0311, 0.8392, 0.0797, 0.0049, 0.0032, 0.0301, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  2.3916468620300293 ; mask density:  0.02868710458278656 ; pred:  tensor([0.0311, 0.8392, 0.0797, 0.0049, 0.0032, 0.0301, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  2.3557605743408203 ; mask density:  0.028365394100546837 ; pred:  tensor([0.0311, 0.8392, 0.0797, 0.0050, 0.0032, 0.0301, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  2.3210597038269043 ; mask density:  0.028055384755134583 ; pred:  tensor([0.0311, 0.8392, 0.0797, 0.0050, 0.0032, 0.0301, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  2.287484645843506 ; mask density:  0.027756400406360626 ; pred:  tensor([0.0311, 0.8393, 0.0797, 0.0050, 0.0032, 0.0300, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  2.254977226257324 ; mask density:  0.027467822656035423 ; pred:  tensor([0.0310, 0.8394, 0.0796, 0.0050, 0.0032, 0.0300, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  2.2234835624694824 ; mask density:  0.02718905173242092 ; pred:  tensor([0.0310, 0.8396, 0.0796, 0.0049, 0.0032, 0.0299, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  2.1929523944854736 ; mask density:  0.026919526979327202 ; pred:  tensor([0.0310, 0.8397, 0.0796, 0.0049, 0.0032, 0.0298, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  2.1633448600769043 ; mask density:  0.02665741741657257 ; pred:  tensor([0.0310, 0.8399, 0.0795, 0.0049, 0.0032, 0.0298, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  2.1346046924591064 ; mask density:  0.02640240080654621 ; pred:  tensor([0.0310, 0.8400, 0.0795, 0.0049, 0.0032, 0.0297, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  2.106684446334839 ; mask density:  0.02615421451628208 ; pred:  tensor([0.0310, 0.8400, 0.0795, 0.0049, 0.0032, 0.0297, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  2.0795624256134033 ; mask density:  0.025913912802934647 ; pred:  tensor([0.0310, 0.8400, 0.0795, 0.0050, 0.0032, 0.0297, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  2.053196668624878 ; mask density:  0.02568112313747406 ; pred:  tensor([0.0310, 0.8401, 0.0795, 0.0050, 0.0032, 0.0297, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  2.027549982070923 ; mask density:  0.025455474853515625 ; pred:  tensor([0.0309, 0.8401, 0.0795, 0.0050, 0.0032, 0.0297, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  2.0025908946990967 ; mask density:  0.025236615911126137 ; pred:  tensor([0.0309, 0.8402, 0.0795, 0.0050, 0.0032, 0.0297, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  1.9782898426055908 ; mask density:  0.025024212896823883 ; pred:  tensor([0.0309, 0.8403, 0.0795, 0.0050, 0.0032, 0.0296, 0.0103, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  1.9546184539794922 ; mask density:  0.02481793612241745 ; pred:  tensor([0.0309, 0.8404, 0.0795, 0.0050, 0.0032, 0.0296, 0.0103, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  1.9315505027770996 ; mask density:  0.024617480114102364 ; pred:  tensor([0.0309, 0.8406, 0.0794, 0.0050, 0.0032, 0.0295, 0.0103, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  1.9090626239776611 ; mask density:  0.024422548711299896 ; pred:  tensor([0.0309, 0.8407, 0.0794, 0.0050, 0.0032, 0.0294, 0.0102, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  9.635708570480347\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_615graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  620 102\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  59.35395431518555 ; mask density:  0.70999675989151 ; pred:  tensor([0.0256, 0.7301, 0.2013, 0.0153, 0.0023, 0.0127, 0.0110, 0.0018],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  57.720367431640625 ; mask density:  0.6903702020645142 ; pred:  tensor([0.0253, 0.7657, 0.1695, 0.0123, 0.0024, 0.0135, 0.0098, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  56.030704498291016 ; mask density:  0.6699689030647278 ; pred:  tensor([0.0252, 0.7911, 0.1460, 0.0103, 0.0025, 0.0144, 0.0090, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  54.28242111206055 ; mask density:  0.6487985849380493 ; pred:  tensor([0.0254, 0.8098, 0.1283, 0.0090, 0.0026, 0.0153, 0.0083, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  52.47700881958008 ; mask density:  0.6268953084945679 ; pred:  tensor([0.0258, 0.8231, 0.1152, 0.0080, 0.0027, 0.0161, 0.0079, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  50.61711502075195 ; mask density:  0.6043335199356079 ; pred:  tensor([0.0263, 0.8325, 0.1055, 0.0073, 0.0027, 0.0168, 0.0076, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  48.70713806152344 ; mask density:  0.5812056064605713 ; pred:  tensor([0.0269, 0.8399, 0.0977, 0.0068, 0.0028, 0.0174, 0.0073, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  46.754173278808594 ; mask density:  0.5576382279396057 ; pred:  tensor([0.0275, 0.8455, 0.0916, 0.0064, 0.0029, 0.0179, 0.0071, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  44.76654815673828 ; mask density:  0.5337413549423218 ; pred:  tensor([0.0280, 0.8499, 0.0868, 0.0061, 0.0030, 0.0183, 0.0068, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  42.753665924072266 ; mask density:  0.5096425414085388 ; pred:  tensor([0.0285, 0.8531, 0.0831, 0.0058, 0.0031, 0.0187, 0.0066, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  40.725341796875 ; mask density:  0.4854770004749298 ; pred:  tensor([0.0290, 0.8556, 0.0801, 0.0056, 0.0032, 0.0190, 0.0064, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  38.69328689575195 ; mask density:  0.461381733417511 ; pred:  tensor([0.0293, 0.8569, 0.0784, 0.0055, 0.0032, 0.0193, 0.0064, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  36.668731689453125 ; mask density:  0.43749916553497314 ; pred:  tensor([0.0294, 0.8572, 0.0777, 0.0055, 0.0033, 0.0195, 0.0063, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  34.66265106201172 ; mask density:  0.4139765799045563 ; pred:  tensor([0.0295, 0.8573, 0.0774, 0.0055, 0.0033, 0.0196, 0.0063, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  32.68684768676758 ; mask density:  0.39094606041908264 ; pred:  tensor([0.0295, 0.8567, 0.0776, 0.0055, 0.0034, 0.0198, 0.0063, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  30.75236701965332 ; mask density:  0.36853334307670593 ; pred:  tensor([0.0295, 0.8557, 0.0782, 0.0055, 0.0035, 0.0202, 0.0063, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  28.869579315185547 ; mask density:  0.34685227274894714 ; pred:  tensor([0.0294, 0.8543, 0.0791, 0.0056, 0.0036, 0.0207, 0.0064, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  27.048208236694336 ; mask density:  0.3259972929954529 ; pred:  tensor([0.0293, 0.8524, 0.0805, 0.0056, 0.0037, 0.0211, 0.0063, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  25.295869827270508 ; mask density:  0.30604565143585205 ; pred:  tensor([0.0292, 0.8503, 0.0820, 0.0057, 0.0038, 0.0215, 0.0063, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  23.619356155395508 ; mask density:  0.2870572805404663 ; pred:  tensor([0.0291, 0.8483, 0.0835, 0.0058, 0.0038, 0.0219, 0.0063, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  22.024343490600586 ; mask density:  0.2690870761871338 ; pred:  tensor([0.0290, 0.8461, 0.0853, 0.0060, 0.0039, 0.0222, 0.0064, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  20.514902114868164 ; mask density:  0.2521592974662781 ; pred:  tensor([0.0289, 0.8436, 0.0873, 0.0061, 0.0040, 0.0226, 0.0064, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  19.093093872070312 ; mask density:  0.2362854778766632 ; pred:  tensor([0.0288, 0.8413, 0.0890, 0.0062, 0.0041, 0.0231, 0.0064, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  17.759687423706055 ; mask density:  0.22145694494247437 ; pred:  tensor([0.0287, 0.8393, 0.0904, 0.0063, 0.0041, 0.0235, 0.0065, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  16.514463424682617 ; mask density:  0.20765681564807892 ; pred:  tensor([0.0286, 0.8376, 0.0916, 0.0063, 0.0042, 0.0239, 0.0066, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  15.35593318939209 ; mask density:  0.19485507905483246 ; pred:  tensor([0.0285, 0.8365, 0.0923, 0.0064, 0.0042, 0.0243, 0.0067, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  14.281482696533203 ; mask density:  0.18301324546337128 ; pred:  tensor([0.0285, 0.8361, 0.0923, 0.0064, 0.0042, 0.0246, 0.0067, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  13.287986755371094 ; mask density:  0.17208434641361237 ; pred:  tensor([0.0285, 0.8364, 0.0919, 0.0063, 0.0042, 0.0247, 0.0067, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  12.371673583984375 ; mask density:  0.16201776266098022 ; pred:  tensor([0.0286, 0.8374, 0.0909, 0.0062, 0.0042, 0.0247, 0.0068, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  11.528605461120605 ; mask density:  0.1527586579322815 ; pred:  tensor([0.0287, 0.8387, 0.0898, 0.0062, 0.0041, 0.0247, 0.0068, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  10.754581451416016 ; mask density:  0.14425033330917358 ; pred:  tensor([0.0287, 0.8400, 0.0885, 0.0061, 0.0041, 0.0247, 0.0068, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  10.044903755187988 ; mask density:  0.13643747568130493 ; pred:  tensor([0.0288, 0.8412, 0.0874, 0.0060, 0.0040, 0.0246, 0.0068, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  9.394885063171387 ; mask density:  0.1292627453804016 ; pred:  tensor([0.0289, 0.8425, 0.0863, 0.0059, 0.0040, 0.0245, 0.0068, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  8.799985885620117 ; mask density:  0.1226779967546463 ; pred:  tensor([0.0289, 0.8437, 0.0852, 0.0058, 0.0039, 0.0244, 0.0069, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  8.255932807922363 ; mask density:  0.11663395166397095 ; pred:  tensor([0.0290, 0.8446, 0.0843, 0.0057, 0.0039, 0.0244, 0.0069, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  7.7583909034729 ; mask density:  0.11108347028493881 ; pred:  tensor([0.0291, 0.8455, 0.0836, 0.0057, 0.0038, 0.0243, 0.0070, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  7.303335666656494 ; mask density:  0.10598213970661163 ; pred:  tensor([0.0291, 0.8463, 0.0829, 0.0056, 0.0037, 0.0242, 0.0070, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  6.887018203735352 ; mask density:  0.10128870606422424 ; pred:  tensor([0.0292, 0.8469, 0.0823, 0.0056, 0.0037, 0.0241, 0.0071, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  6.505965709686279 ; mask density:  0.09696611762046814 ; pred:  tensor([0.0292, 0.8474, 0.0819, 0.0055, 0.0037, 0.0241, 0.0071, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  6.157017230987549 ; mask density:  0.09298056364059448 ; pred:  tensor([0.0292, 0.8479, 0.0815, 0.0055, 0.0036, 0.0240, 0.0072, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  5.837128639221191 ; mask density:  0.08929034322500229 ; pred:  tensor([0.0293, 0.8483, 0.0812, 0.0055, 0.0036, 0.0239, 0.0072, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  5.543471336364746 ; mask density:  0.08587102591991425 ; pred:  tensor([0.0293, 0.8488, 0.0807, 0.0054, 0.0035, 0.0239, 0.0073, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  5.273709297180176 ; mask density:  0.08269929140806198 ; pred:  tensor([0.0294, 0.8492, 0.0804, 0.0054, 0.0035, 0.0238, 0.0073, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  5.025623321533203 ; mask density:  0.07975409179925919 ; pred:  tensor([0.0295, 0.8495, 0.0800, 0.0054, 0.0034, 0.0237, 0.0074, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  4.797177314758301 ; mask density:  0.0770159587264061 ; pred:  tensor([0.0296, 0.8498, 0.0798, 0.0053, 0.0034, 0.0236, 0.0074, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  4.586552143096924 ; mask density:  0.07446783781051636 ; pred:  tensor([0.0296, 0.8500, 0.0796, 0.0053, 0.0033, 0.0236, 0.0075, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  4.392137050628662 ; mask density:  0.0720936730504036 ; pred:  tensor([0.0296, 0.8501, 0.0795, 0.0053, 0.0033, 0.0235, 0.0075, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  4.212396144866943 ; mask density:  0.06987854838371277 ; pred:  tensor([0.0297, 0.8502, 0.0795, 0.0053, 0.0033, 0.0235, 0.0076, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  4.0460124015808105 ; mask density:  0.06780917942523956 ; pred:  tensor([0.0297, 0.8501, 0.0795, 0.0053, 0.0033, 0.0234, 0.0076, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  3.8917760848999023 ; mask density:  0.06587313860654831 ; pred:  tensor([0.0297, 0.8500, 0.0797, 0.0053, 0.0033, 0.0234, 0.0077, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  3.7485694885253906 ; mask density:  0.06405855715274811 ; pred:  tensor([0.0297, 0.8498, 0.0798, 0.0053, 0.0032, 0.0234, 0.0077, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  3.615347385406494 ; mask density:  0.06235062703490257 ; pred:  tensor([0.0296, 0.8496, 0.0800, 0.0053, 0.0032, 0.0235, 0.0078, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  3.491309404373169 ; mask density:  0.060741208493709564 ; pred:  tensor([0.0297, 0.8493, 0.0799, 0.0053, 0.0032, 0.0237, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  3.3757126331329346 ; mask density:  0.05922231823205948 ; pred:  tensor([0.0297, 0.8490, 0.0799, 0.0052, 0.0032, 0.0239, 0.0080, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  3.2678122520446777 ; mask density:  0.05778694525361061 ; pred:  tensor([0.0297, 0.8487, 0.0800, 0.0052, 0.0032, 0.0241, 0.0081, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  3.166994333267212 ; mask density:  0.05642802640795708 ; pred:  tensor([0.0297, 0.8482, 0.0801, 0.0052, 0.0032, 0.0243, 0.0082, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  3.072604179382324 ; mask density:  0.055138926953077316 ; pred:  tensor([0.0297, 0.8477, 0.0804, 0.0052, 0.0032, 0.0245, 0.0082, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  2.98410964012146 ; mask density:  0.05391370505094528 ; pred:  tensor([0.0296, 0.8471, 0.0807, 0.0052, 0.0032, 0.0247, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  2.9010136127471924 ; mask density:  0.052747007459402084 ; pred:  tensor([0.0296, 0.8466, 0.0810, 0.0053, 0.0032, 0.0249, 0.0084, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  2.822869062423706 ; mask density:  0.051634132862091064 ; pred:  tensor([0.0296, 0.8460, 0.0813, 0.0053, 0.0033, 0.0250, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  2.749237537384033 ; mask density:  0.05057016387581825 ; pred:  tensor([0.0296, 0.8455, 0.0816, 0.0053, 0.0033, 0.0252, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  2.6797776222229004 ; mask density:  0.049551308155059814 ; pred:  tensor([0.0295, 0.8450, 0.0819, 0.0053, 0.0033, 0.0254, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  2.6140971183776855 ; mask density:  0.0485745407640934 ; pred:  tensor([0.0295, 0.8446, 0.0821, 0.0053, 0.0033, 0.0255, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  2.5519659519195557 ; mask density:  0.047638051211833954 ; pred:  tensor([0.0294, 0.8442, 0.0823, 0.0053, 0.0033, 0.0256, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  2.4930734634399414 ; mask density:  0.046739690005779266 ; pred:  tensor([0.0294, 0.8439, 0.0826, 0.0053, 0.0033, 0.0257, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  2.437232255935669 ; mask density:  0.04588000848889351 ; pred:  tensor([0.0293, 0.8436, 0.0827, 0.0053, 0.0033, 0.0258, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  2.3842227458953857 ; mask density:  0.04505792260169983 ; pred:  tensor([0.0293, 0.8435, 0.0828, 0.0053, 0.0033, 0.0259, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  2.333812952041626 ; mask density:  0.04427071660757065 ; pred:  tensor([0.0293, 0.8435, 0.0828, 0.0053, 0.0033, 0.0259, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  2.2858357429504395 ; mask density:  0.04351712763309479 ; pred:  tensor([0.0293, 0.8436, 0.0828, 0.0053, 0.0034, 0.0259, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  2.240114450454712 ; mask density:  0.042795564979314804 ; pred:  tensor([0.0293, 0.8436, 0.0827, 0.0053, 0.0034, 0.0259, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  2.1964948177337646 ; mask density:  0.04210415855050087 ; pred:  tensor([0.0293, 0.8438, 0.0826, 0.0053, 0.0034, 0.0259, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  2.1548335552215576 ; mask density:  0.04144081845879555 ; pred:  tensor([0.0293, 0.8439, 0.0825, 0.0053, 0.0034, 0.0259, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  2.114997148513794 ; mask density:  0.040803227573633194 ; pred:  tensor([0.0293, 0.8441, 0.0823, 0.0053, 0.0034, 0.0259, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  2.07686185836792 ; mask density:  0.0401889868080616 ; pred:  tensor([0.0293, 0.8443, 0.0821, 0.0053, 0.0034, 0.0259, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  2.0403146743774414 ; mask density:  0.03959629312157631 ; pred:  tensor([0.0293, 0.8445, 0.0820, 0.0053, 0.0034, 0.0259, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  2.0052475929260254 ; mask density:  0.03902266547083855 ; pred:  tensor([0.0293, 0.8447, 0.0818, 0.0052, 0.0034, 0.0259, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  1.9715625047683716 ; mask density:  0.038465797901153564 ; pred:  tensor([0.0293, 0.8449, 0.0816, 0.0052, 0.0034, 0.0259, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  1.939168930053711 ; mask density:  0.03792355954647064 ; pred:  tensor([0.0293, 0.8450, 0.0814, 0.0052, 0.0034, 0.0259, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  1.907983422279358 ; mask density:  0.03739401698112488 ; pred:  tensor([0.0293, 0.8452, 0.0813, 0.0052, 0.0034, 0.0259, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  1.877928614616394 ; mask density:  0.036875467747449875 ; pred:  tensor([0.0294, 0.8453, 0.0811, 0.0052, 0.0034, 0.0259, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  1.8489338159561157 ; mask density:  0.03636658191680908 ; pred:  tensor([0.0294, 0.8454, 0.0810, 0.0052, 0.0034, 0.0259, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  1.820935606956482 ; mask density:  0.03586641326546669 ; pred:  tensor([0.0294, 0.8455, 0.0809, 0.0052, 0.0034, 0.0260, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  1.7938741445541382 ; mask density:  0.03537442907691002 ; pred:  tensor([0.0294, 0.8456, 0.0808, 0.0052, 0.0034, 0.0260, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  1.7676979303359985 ; mask density:  0.03489058464765549 ; pred:  tensor([0.0294, 0.8456, 0.0807, 0.0052, 0.0034, 0.0260, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  1.7423603534698486 ; mask density:  0.034415338188409805 ; pred:  tensor([0.0294, 0.8456, 0.0806, 0.0052, 0.0034, 0.0261, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  1.717818260192871 ; mask density:  0.033949583768844604 ; pred:  tensor([0.0294, 0.8456, 0.0806, 0.0052, 0.0035, 0.0261, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  1.6940360069274902 ; mask density:  0.033494677394628525 ; pred:  tensor([0.0294, 0.8456, 0.0805, 0.0052, 0.0035, 0.0262, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  1.6709823608398438 ; mask density:  0.033052217215299606 ; pred:  tensor([0.0294, 0.8456, 0.0805, 0.0052, 0.0035, 0.0262, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  1.6486270427703857 ; mask density:  0.03262389451265335 ; pred:  tensor([0.0294, 0.8456, 0.0804, 0.0052, 0.0035, 0.0262, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  1.6269434690475464 ; mask density:  0.03221135213971138 ; pred:  tensor([0.0294, 0.8455, 0.0804, 0.0052, 0.0035, 0.0263, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  1.6059094667434692 ; mask density:  0.03181541711091995 ; pred:  tensor([0.0294, 0.8455, 0.0804, 0.0052, 0.0035, 0.0263, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  1.5855001211166382 ; mask density:  0.031437184661626816 ; pred:  tensor([0.0294, 0.8455, 0.0803, 0.0052, 0.0035, 0.0264, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  1.5656936168670654 ; mask density:  0.03107728250324726 ; pred:  tensor([0.0294, 0.8455, 0.0803, 0.0052, 0.0035, 0.0264, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  1.5464752912521362 ; mask density:  0.03073749504983425 ; pred:  tensor([0.0294, 0.8455, 0.0803, 0.0052, 0.0035, 0.0264, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  1.5278065204620361 ; mask density:  0.030413366854190826 ; pred:  tensor([0.0295, 0.8456, 0.0802, 0.0052, 0.0035, 0.0264, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  1.509674072265625 ; mask density:  0.03010447695851326 ; pred:  tensor([0.0295, 0.8456, 0.0801, 0.0052, 0.0035, 0.0265, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  1.4920730590820312 ; mask density:  0.029814137145876884 ; pred:  tensor([0.0295, 0.8454, 0.0802, 0.0052, 0.0035, 0.0265, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  1.4749412536621094 ; mask density:  0.02954104170203209 ; pred:  tensor([0.0295, 0.8455, 0.0801, 0.0052, 0.0035, 0.0266, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  1.4582586288452148 ; mask density:  0.029283791780471802 ; pred:  tensor([0.0295, 0.8456, 0.0800, 0.0052, 0.0035, 0.0265, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  1.4420475959777832 ; mask density:  0.02903531678020954 ; pred:  tensor([0.0295, 0.8458, 0.0798, 0.0052, 0.0035, 0.0265, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  9.349602460861206\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_620graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  625 61\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  38.5806770324707 ; mask density:  0.7130981683731079 ; pred:  tensor([0.0250, 0.7854, 0.1415, 0.0096, 0.0036, 0.0213, 0.0118, 0.0018],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  37.53873825073242 ; mask density:  0.6940515637397766 ; pred:  tensor([0.0252, 0.8067, 0.1230, 0.0082, 0.0037, 0.0214, 0.0103, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  36.45575714111328 ; mask density:  0.6739437580108643 ; pred:  tensor([0.0259, 0.8209, 0.1094, 0.0073, 0.0037, 0.0220, 0.0094, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  35.3350944519043 ; mask density:  0.6529098749160767 ; pred:  tensor([0.0263, 0.8304, 0.1003, 0.0066, 0.0037, 0.0223, 0.0089, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  34.17597961425781 ; mask density:  0.6310643553733826 ; pred:  tensor([0.0267, 0.8371, 0.0938, 0.0062, 0.0037, 0.0226, 0.0086, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  32.98128890991211 ; mask density:  0.6085129976272583 ; pred:  tensor([0.0271, 0.8411, 0.0898, 0.0059, 0.0037, 0.0228, 0.0083, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  31.753372192382812 ; mask density:  0.5853597521781921 ; pred:  tensor([0.0275, 0.8441, 0.0866, 0.0057, 0.0037, 0.0230, 0.0081, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  30.496671676635742 ; mask density:  0.5617209076881409 ; pred:  tensor([0.0278, 0.8464, 0.0839, 0.0056, 0.0037, 0.0233, 0.0080, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  29.216848373413086 ; mask density:  0.537723183631897 ; pred:  tensor([0.0282, 0.8480, 0.0818, 0.0055, 0.0038, 0.0236, 0.0080, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  27.919857025146484 ; mask density:  0.5134984850883484 ; pred:  tensor([0.0284, 0.8490, 0.0803, 0.0054, 0.0038, 0.0240, 0.0079, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  26.61215591430664 ; mask density:  0.4891830086708069 ; pred:  tensor([0.0286, 0.8496, 0.0792, 0.0053, 0.0038, 0.0243, 0.0080, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  25.300790786743164 ; mask density:  0.46492019295692444 ; pred:  tensor([0.0287, 0.8499, 0.0785, 0.0053, 0.0038, 0.0246, 0.0080, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  23.993148803710938 ; mask density:  0.4408506751060486 ; pred:  tensor([0.0288, 0.8498, 0.0781, 0.0053, 0.0038, 0.0250, 0.0081, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  22.69634246826172 ; mask density:  0.4171082079410553 ; pred:  tensor([0.0289, 0.8496, 0.0776, 0.0053, 0.0038, 0.0254, 0.0083, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  21.417945861816406 ; mask density:  0.393833190202713 ; pred:  tensor([0.0288, 0.8492, 0.0775, 0.0053, 0.0038, 0.0258, 0.0084, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  20.16518783569336 ; mask density:  0.37114807963371277 ; pred:  tensor([0.0287, 0.8487, 0.0776, 0.0053, 0.0038, 0.0262, 0.0086, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  18.945283889770508 ; mask density:  0.3491864800453186 ; pred:  tensor([0.0286, 0.8476, 0.0782, 0.0053, 0.0038, 0.0266, 0.0087, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  17.76403045654297 ; mask density:  0.3280324935913086 ; pred:  tensor([0.0284, 0.8462, 0.0793, 0.0054, 0.0039, 0.0267, 0.0089, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  16.626455307006836 ; mask density:  0.3077634572982788 ; pred:  tensor([0.0282, 0.8450, 0.0802, 0.0054, 0.0039, 0.0270, 0.0090, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  15.536839485168457 ; mask density:  0.28842824697494507 ; pred:  tensor([0.0281, 0.8440, 0.0809, 0.0055, 0.0039, 0.0272, 0.0091, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  14.499076843261719 ; mask density:  0.2700822353363037 ; pred:  tensor([0.0280, 0.8430, 0.0818, 0.0055, 0.0039, 0.0273, 0.0092, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  13.515816688537598 ; mask density:  0.25276726484298706 ; pred:  tensor([0.0280, 0.8420, 0.0822, 0.0056, 0.0039, 0.0277, 0.0094, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  12.588834762573242 ; mask density:  0.2364889532327652 ; pred:  tensor([0.0280, 0.8409, 0.0827, 0.0056, 0.0039, 0.0281, 0.0096, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  11.718953132629395 ; mask density:  0.22125694155693054 ; pred:  tensor([0.0280, 0.8397, 0.0832, 0.0056, 0.0039, 0.0285, 0.0098, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  10.905856132507324 ; mask density:  0.20705531537532806 ; pred:  tensor([0.0281, 0.8385, 0.0834, 0.0056, 0.0039, 0.0291, 0.0101, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  10.148933410644531 ; mask density:  0.19387254118919373 ; pred:  tensor([0.0281, 0.8373, 0.0836, 0.0056, 0.0039, 0.0298, 0.0103, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  9.44674015045166 ; mask density:  0.18167191743850708 ; pred:  tensor([0.0282, 0.8361, 0.0838, 0.0057, 0.0039, 0.0304, 0.0106, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  8.797192573547363 ; mask density:  0.1703929603099823 ; pred:  tensor([0.0283, 0.8350, 0.0840, 0.0057, 0.0039, 0.0310, 0.0108, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  8.19763469696045 ; mask density:  0.1599861979484558 ; pred:  tensor([0.0284, 0.8342, 0.0840, 0.0057, 0.0039, 0.0315, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  7.645687103271484 ; mask density:  0.15039923787117004 ; pred:  tensor([0.0285, 0.8334, 0.0840, 0.0057, 0.0039, 0.0320, 0.0112, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  7.138394832611084 ; mask density:  0.14156611263751984 ; pred:  tensor([0.0286, 0.8327, 0.0839, 0.0057, 0.0039, 0.0324, 0.0114, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  6.672675609588623 ; mask density:  0.13343891501426697 ; pred:  tensor([0.0288, 0.8322, 0.0838, 0.0057, 0.0039, 0.0328, 0.0116, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  6.245654582977295 ; mask density:  0.12595675885677338 ; pred:  tensor([0.0288, 0.8318, 0.0836, 0.0057, 0.0039, 0.0331, 0.0117, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  5.854282379150391 ; mask density:  0.11906512081623077 ; pred:  tensor([0.0289, 0.8316, 0.0835, 0.0057, 0.0039, 0.0333, 0.0118, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  5.495828628540039 ; mask density:  0.11272478848695755 ; pred:  tensor([0.0289, 0.8315, 0.0835, 0.0057, 0.0039, 0.0334, 0.0119, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  5.167768478393555 ; mask density:  0.10689157992601395 ; pred:  tensor([0.0288, 0.8313, 0.0837, 0.0057, 0.0039, 0.0334, 0.0120, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  4.867541313171387 ; mask density:  0.10152895003557205 ; pred:  tensor([0.0287, 0.8310, 0.0841, 0.0057, 0.0039, 0.0334, 0.0120, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  4.5928521156311035 ; mask density:  0.09660128504037857 ; pred:  tensor([0.0286, 0.8306, 0.0846, 0.0057, 0.0039, 0.0333, 0.0120, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  4.3414106369018555 ; mask density:  0.09207107871770859 ; pred:  tensor([0.0284, 0.8302, 0.0853, 0.0057, 0.0039, 0.0332, 0.0121, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  4.111236095428467 ; mask density:  0.08790488541126251 ; pred:  tensor([0.0282, 0.8296, 0.0860, 0.0057, 0.0039, 0.0331, 0.0121, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  3.900385618209839 ; mask density:  0.08407207578420639 ; pred:  tensor([0.0281, 0.8289, 0.0869, 0.0058, 0.0039, 0.0330, 0.0122, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  3.7071902751922607 ; mask density:  0.08057455718517303 ; pred:  tensor([0.0279, 0.8280, 0.0879, 0.0058, 0.0039, 0.0329, 0.0123, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  3.529608726501465 ; mask density:  0.0773468092083931 ; pred:  tensor([0.0278, 0.8272, 0.0884, 0.0058, 0.0039, 0.0331, 0.0123, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  3.366769552230835 ; mask density:  0.07437790185213089 ; pred:  tensor([0.0277, 0.8262, 0.0892, 0.0059, 0.0039, 0.0333, 0.0124, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  3.217442512512207 ; mask density:  0.071644127368927 ; pred:  tensor([0.0275, 0.8251, 0.0899, 0.0059, 0.0040, 0.0336, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  3.079537868499756 ; mask density:  0.06912370771169662 ; pred:  tensor([0.0275, 0.8247, 0.0905, 0.0059, 0.0039, 0.0335, 0.0126, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  2.9522504806518555 ; mask density:  0.06682009249925613 ; pred:  tensor([0.0274, 0.8249, 0.0910, 0.0059, 0.0039, 0.0330, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  2.8344738483428955 ; mask density:  0.06470456719398499 ; pred:  tensor([0.0274, 0.8252, 0.0908, 0.0059, 0.0039, 0.0328, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  2.725142240524292 ; mask density:  0.06275657564401627 ; pred:  tensor([0.0275, 0.8260, 0.0900, 0.0059, 0.0039, 0.0329, 0.0124, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  2.6240391731262207 ; mask density:  0.06094926968216896 ; pred:  tensor([0.0277, 0.8266, 0.0891, 0.0058, 0.0039, 0.0331, 0.0124, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  2.5301012992858887 ; mask density:  0.05926860123872757 ; pred:  tensor([0.0278, 0.8276, 0.0882, 0.0058, 0.0039, 0.0330, 0.0123, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  2.443098306655884 ; mask density:  0.05769253894686699 ; pred:  tensor([0.0280, 0.8286, 0.0874, 0.0057, 0.0039, 0.0328, 0.0122, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  2.362190008163452 ; mask density:  0.056212056428194046 ; pred:  tensor([0.0281, 0.8295, 0.0867, 0.0057, 0.0039, 0.0327, 0.0121, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  2.286818504333496 ; mask density:  0.05481911450624466 ; pred:  tensor([0.0282, 0.8304, 0.0861, 0.0057, 0.0038, 0.0324, 0.0120, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  2.216590404510498 ; mask density:  0.05352208390831947 ; pred:  tensor([0.0283, 0.8310, 0.0858, 0.0057, 0.0038, 0.0322, 0.0120, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  2.150813579559326 ; mask density:  0.05229775607585907 ; pred:  tensor([0.0284, 0.8314, 0.0852, 0.0056, 0.0038, 0.0323, 0.0119, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  2.0893146991729736 ; mask density:  0.05114050209522247 ; pred:  tensor([0.0285, 0.8316, 0.0849, 0.0056, 0.0038, 0.0323, 0.0119, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  2.0316433906555176 ; mask density:  0.05004526302218437 ; pred:  tensor([0.0285, 0.8318, 0.0847, 0.0056, 0.0038, 0.0323, 0.0119, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  1.977476954460144 ; mask density:  0.04900742322206497 ; pred:  tensor([0.0286, 0.8319, 0.0846, 0.0056, 0.0038, 0.0323, 0.0119, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  1.9265272617340088 ; mask density:  0.0480227917432785 ; pred:  tensor([0.0286, 0.8320, 0.0845, 0.0056, 0.0038, 0.0323, 0.0119, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  1.8786323070526123 ; mask density:  0.04710312932729721 ; pred:  tensor([0.0286, 0.8319, 0.0847, 0.0056, 0.0038, 0.0322, 0.0119, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  1.8333158493041992 ; mask density:  0.046227987855672836 ; pred:  tensor([0.0286, 0.8316, 0.0846, 0.0056, 0.0038, 0.0324, 0.0120, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  1.7906193733215332 ; mask density:  0.045394398272037506 ; pred:  tensor([0.0286, 0.8313, 0.0847, 0.0056, 0.0038, 0.0326, 0.0120, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  1.7502180337905884 ; mask density:  0.0445919893682003 ; pred:  tensor([0.0286, 0.8309, 0.0849, 0.0056, 0.0038, 0.0327, 0.0121, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  1.7118809223175049 ; mask density:  0.04381966218352318 ; pred:  tensor([0.0286, 0.8307, 0.0850, 0.0056, 0.0038, 0.0328, 0.0121, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  1.6755104064941406 ; mask density:  0.043076422065496445 ; pred:  tensor([0.0286, 0.8304, 0.0851, 0.0056, 0.0038, 0.0329, 0.0122, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  1.6409595012664795 ; mask density:  0.042361218482255936 ; pred:  tensor([0.0286, 0.8303, 0.0853, 0.0056, 0.0038, 0.0329, 0.0122, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  1.6082730293273926 ; mask density:  0.04171048104763031 ; pred:  tensor([0.0286, 0.8300, 0.0856, 0.0056, 0.0038, 0.0329, 0.0122, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  1.5773193836212158 ; mask density:  0.04110150784254074 ; pred:  tensor([0.0286, 0.8296, 0.0857, 0.0056, 0.0038, 0.0330, 0.0123, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  1.5478005409240723 ; mask density:  0.04052634909749031 ; pred:  tensor([0.0286, 0.8293, 0.0857, 0.0056, 0.0038, 0.0333, 0.0124, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  1.5194880962371826 ; mask density:  0.03998379409313202 ; pred:  tensor([0.0287, 0.8294, 0.0855, 0.0056, 0.0038, 0.0334, 0.0124, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  1.492289423942566 ; mask density:  0.039471231400966644 ; pred:  tensor([0.0287, 0.8299, 0.0851, 0.0055, 0.0038, 0.0332, 0.0124, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  1.4664692878723145 ; mask density:  0.03899318724870682 ; pred:  tensor([0.0288, 0.8305, 0.0850, 0.0055, 0.0037, 0.0328, 0.0123, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  1.4416857957839966 ; mask density:  0.03854703530669212 ; pred:  tensor([0.0289, 0.8310, 0.0847, 0.0055, 0.0037, 0.0327, 0.0123, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  1.4177268743515015 ; mask density:  0.03813238441944122 ; pred:  tensor([0.0290, 0.8315, 0.0840, 0.0055, 0.0037, 0.0328, 0.0122, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  1.394894003868103 ; mask density:  0.03774137422442436 ; pred:  tensor([0.0291, 0.8318, 0.0833, 0.0054, 0.0037, 0.0331, 0.0122, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  1.3730199337005615 ; mask density:  0.037372130900621414 ; pred:  tensor([0.0292, 0.8322, 0.0827, 0.0054, 0.0037, 0.0331, 0.0122, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  1.3519179821014404 ; mask density:  0.03700436279177666 ; pred:  tensor([0.0293, 0.8329, 0.0822, 0.0054, 0.0037, 0.0330, 0.0121, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  1.3316911458969116 ; mask density:  0.03663930296897888 ; pred:  tensor([0.0294, 0.8335, 0.0818, 0.0054, 0.0037, 0.0328, 0.0121, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  1.31216299533844 ; mask density:  0.03627801313996315 ; pred:  tensor([0.0294, 0.8340, 0.0816, 0.0054, 0.0037, 0.0326, 0.0120, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  1.293375015258789 ; mask density:  0.035930488258600235 ; pred:  tensor([0.0295, 0.8343, 0.0815, 0.0054, 0.0037, 0.0324, 0.0120, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  1.2750989198684692 ; mask density:  0.03558812662959099 ; pred:  tensor([0.0295, 0.8344, 0.0813, 0.0053, 0.0037, 0.0325, 0.0120, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  1.2575722932815552 ; mask density:  0.03526995703577995 ; pred:  tensor([0.0295, 0.8342, 0.0813, 0.0053, 0.0037, 0.0326, 0.0120, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  1.240665078163147 ; mask density:  0.034973740577697754 ; pred:  tensor([0.0295, 0.8343, 0.0814, 0.0053, 0.0037, 0.0325, 0.0120, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  1.224403738975525 ; mask density:  0.03470628336071968 ; pred:  tensor([0.0295, 0.8346, 0.0814, 0.0053, 0.0036, 0.0322, 0.0120, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  1.208583116531372 ; mask density:  0.03446420654654503 ; pred:  tensor([0.0295, 0.8347, 0.0813, 0.0053, 0.0036, 0.0322, 0.0120, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  1.19327712059021 ; mask density:  0.0342358723282814 ; pred:  tensor([0.0296, 0.8347, 0.0811, 0.0053, 0.0037, 0.0323, 0.0120, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  1.1785002946853638 ; mask density:  0.03401998430490494 ; pred:  tensor([0.0296, 0.8349, 0.0810, 0.0053, 0.0037, 0.0323, 0.0119, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  1.1641017198562622 ; mask density:  0.03381519019603729 ; pred:  tensor([0.0296, 0.8354, 0.0808, 0.0053, 0.0036, 0.0321, 0.0119, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  1.1503098011016846 ; mask density:  0.0336117222905159 ; pred:  tensor([0.0296, 0.8358, 0.0808, 0.0053, 0.0036, 0.0318, 0.0118, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  1.1367812156677246 ; mask density:  0.03340291604399681 ; pred:  tensor([0.0296, 0.8359, 0.0806, 0.0053, 0.0036, 0.0319, 0.0118, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  1.1236881017684937 ; mask density:  0.033189933747053146 ; pred:  tensor([0.0296, 0.8358, 0.0807, 0.0053, 0.0037, 0.0319, 0.0118, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  1.1109853982925415 ; mask density:  0.032988663762807846 ; pred:  tensor([0.0296, 0.8356, 0.0809, 0.0053, 0.0037, 0.0319, 0.0118, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  1.0986183881759644 ; mask density:  0.03279796987771988 ; pred:  tensor([0.0295, 0.8357, 0.0810, 0.0053, 0.0037, 0.0318, 0.0118, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  1.0865635871887207 ; mask density:  0.03262283280491829 ; pred:  tensor([0.0295, 0.8360, 0.0810, 0.0053, 0.0036, 0.0316, 0.0117, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  1.0748671293258667 ; mask density:  0.032455578446388245 ; pred:  tensor([0.0295, 0.8360, 0.0810, 0.0053, 0.0036, 0.0315, 0.0117, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  1.0635658502578735 ; mask density:  0.0322868712246418 ; pred:  tensor([0.0295, 0.8362, 0.0810, 0.0053, 0.0036, 0.0314, 0.0117, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  1.0525081157684326 ; mask density:  0.0321258082985878 ; pred:  tensor([0.0295, 0.8359, 0.0811, 0.0053, 0.0037, 0.0316, 0.0117, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  1.041771650314331 ; mask density:  0.03197188675403595 ; pred:  tensor([0.0294, 0.8358, 0.0811, 0.0053, 0.0037, 0.0317, 0.0117, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  1.0312689542770386 ; mask density:  0.031810685992240906 ; pred:  tensor([0.0294, 0.8360, 0.0811, 0.0053, 0.0037, 0.0316, 0.0117, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  9.432250261306763\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_625graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  630 87\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  45.7765007019043 ; mask density:  0.7098031640052795 ; pred:  tensor([0.0187, 0.5896, 0.3338, 0.0218, 0.0027, 0.0139, 0.0174, 0.0021],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  44.48241424560547 ; mask density:  0.6898449659347534 ; pred:  tensor([0.0198, 0.6364, 0.2905, 0.0191, 0.0027, 0.0143, 0.0153, 0.0019],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  43.15184020996094 ; mask density:  0.6690300703048706 ; pred:  tensor([0.0208, 0.6754, 0.2540, 0.0168, 0.0027, 0.0148, 0.0137, 0.0018],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  41.78300476074219 ; mask density:  0.6474318504333496 ; pred:  tensor([0.0217, 0.7073, 0.2239, 0.0149, 0.0027, 0.0154, 0.0124, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  40.38496780395508 ; mask density:  0.6251271367073059 ; pred:  tensor([0.0223, 0.7272, 0.2052, 0.0136, 0.0027, 0.0158, 0.0116, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  38.94852828979492 ; mask density:  0.6021914482116699 ; pred:  tensor([0.0227, 0.7437, 0.1895, 0.0125, 0.0028, 0.0163, 0.0110, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  37.47453689575195 ; mask density:  0.5787103176116943 ; pred:  tensor([0.0232, 0.7591, 0.1747, 0.0115, 0.0028, 0.0168, 0.0105, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  35.970157623291016 ; mask density:  0.5547927021980286 ; pred:  tensor([0.0237, 0.7718, 0.1623, 0.0106, 0.0028, 0.0174, 0.0101, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  34.441017150878906 ; mask density:  0.5305571556091309 ; pred:  tensor([0.0241, 0.7824, 0.1519, 0.0099, 0.0028, 0.0179, 0.0097, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  32.89518737792969 ; mask density:  0.5061379671096802 ; pred:  tensor([0.0246, 0.7905, 0.1433, 0.0093, 0.0028, 0.0186, 0.0096, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  31.337846755981445 ; mask density:  0.481658011674881 ; pred:  tensor([0.0251, 0.7982, 0.1350, 0.0088, 0.0029, 0.0193, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  29.778236389160156 ; mask density:  0.4572599530220032 ; pred:  tensor([0.0257, 0.8046, 0.1276, 0.0083, 0.0029, 0.0202, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  28.225210189819336 ; mask density:  0.4330849051475525 ; pred:  tensor([0.0263, 0.8098, 0.1211, 0.0079, 0.0029, 0.0213, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  26.686983108520508 ; mask density:  0.40927326679229736 ; pred:  tensor([0.0268, 0.8143, 0.1152, 0.0075, 0.0030, 0.0225, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  25.17237091064453 ; mask density:  0.38595736026763916 ; pred:  tensor([0.0273, 0.8180, 0.1101, 0.0071, 0.0031, 0.0237, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  23.69068717956543 ; mask density:  0.3632698655128479 ; pred:  tensor([0.0277, 0.8201, 0.1067, 0.0069, 0.0031, 0.0248, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  22.248994827270508 ; mask density:  0.34132805466651917 ; pred:  tensor([0.0279, 0.8215, 0.1042, 0.0067, 0.0032, 0.0256, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  20.853511810302734 ; mask density:  0.32022786140441895 ; pred:  tensor([0.0281, 0.8230, 0.1019, 0.0065, 0.0032, 0.0264, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  19.51030158996582 ; mask density:  0.3000491261482239 ; pred:  tensor([0.0283, 0.8247, 0.0994, 0.0063, 0.0033, 0.0270, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  18.224689483642578 ; mask density:  0.2808522582054138 ; pred:  tensor([0.0286, 0.8265, 0.0971, 0.0062, 0.0033, 0.0274, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  17.000959396362305 ; mask density:  0.26268091797828674 ; pred:  tensor([0.0288, 0.8281, 0.0949, 0.0060, 0.0033, 0.0278, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  15.84223461151123 ; mask density:  0.2455630600452423 ; pred:  tensor([0.0290, 0.8294, 0.0932, 0.0059, 0.0033, 0.0281, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  14.750216484069824 ; mask density:  0.22950918972492218 ; pred:  tensor([0.0292, 0.8305, 0.0918, 0.0058, 0.0033, 0.0283, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  13.72571086883545 ; mask density:  0.21451251208782196 ; pred:  tensor([0.0293, 0.8313, 0.0906, 0.0058, 0.0034, 0.0286, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  12.768383979797363 ; mask density:  0.20055462419986725 ; pred:  tensor([0.0295, 0.8318, 0.0897, 0.0057, 0.0034, 0.0288, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  11.87727165222168 ; mask density:  0.18760915100574493 ; pred:  tensor([0.0296, 0.8321, 0.0890, 0.0057, 0.0034, 0.0292, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  11.050742149353027 ; mask density:  0.1756390631198883 ; pred:  tensor([0.0296, 0.8322, 0.0885, 0.0056, 0.0035, 0.0295, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  10.286449432373047 ; mask density:  0.1645994931459427 ; pred:  tensor([0.0296, 0.8320, 0.0882, 0.0056, 0.0035, 0.0300, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  9.581583976745605 ; mask density:  0.15444116294384003 ; pred:  tensor([0.0297, 0.8317, 0.0879, 0.0056, 0.0036, 0.0304, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  8.933058738708496 ; mask density:  0.1451101154088974 ; pred:  tensor([0.0296, 0.8313, 0.0879, 0.0056, 0.0036, 0.0308, 0.0100, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  8.337430000305176 ; mask density:  0.13655005395412445 ; pred:  tensor([0.0296, 0.8308, 0.0877, 0.0056, 0.0037, 0.0314, 0.0100, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  7.791215419769287 ; mask density:  0.1287042647600174 ; pred:  tensor([0.0297, 0.8302, 0.0876, 0.0056, 0.0037, 0.0320, 0.0101, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  7.2909088134765625 ; mask density:  0.12151893228292465 ; pred:  tensor([0.0297, 0.8296, 0.0874, 0.0055, 0.0037, 0.0326, 0.0102, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  6.833071708679199 ; mask density:  0.11494002491235733 ; pred:  tensor([0.0297, 0.8290, 0.0873, 0.0055, 0.0038, 0.0331, 0.0103, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  6.414272785186768 ; mask density:  0.10891445726156235 ; pred:  tensor([0.0297, 0.8285, 0.0872, 0.0055, 0.0038, 0.0336, 0.0103, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  6.03125 ; mask density:  0.10339350253343582 ; pred:  tensor([0.0298, 0.8280, 0.0871, 0.0055, 0.0039, 0.0341, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  5.680992603302002 ; mask density:  0.09833161532878876 ; pred:  tensor([0.0298, 0.8276, 0.0870, 0.0055, 0.0039, 0.0345, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  5.36065149307251 ; mask density:  0.09368666261434555 ; pred:  tensor([0.0298, 0.8272, 0.0868, 0.0055, 0.0039, 0.0349, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  5.067523956298828 ; mask density:  0.08941897004842758 ; pred:  tensor([0.0299, 0.8269, 0.0867, 0.0055, 0.0039, 0.0352, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  4.799113750457764 ; mask density:  0.0854920819401741 ; pred:  tensor([0.0299, 0.8267, 0.0865, 0.0055, 0.0039, 0.0355, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  4.553141117095947 ; mask density:  0.08187271654605865 ; pred:  tensor([0.0300, 0.8266, 0.0863, 0.0055, 0.0039, 0.0357, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  4.327524185180664 ; mask density:  0.07853049784898758 ; pred:  tensor([0.0300, 0.8266, 0.0862, 0.0055, 0.0040, 0.0358, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  4.120358467102051 ; mask density:  0.07543683797121048 ; pred:  tensor([0.0300, 0.8267, 0.0860, 0.0055, 0.0040, 0.0359, 0.0108, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  3.9298977851867676 ; mask density:  0.07256710529327393 ; pred:  tensor([0.0301, 0.8268, 0.0857, 0.0055, 0.0040, 0.0359, 0.0108, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  3.7546043395996094 ; mask density:  0.0698990449309349 ; pred:  tensor([0.0302, 0.8270, 0.0854, 0.0055, 0.0039, 0.0359, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  3.5930488109588623 ; mask density:  0.0674128383398056 ; pred:  tensor([0.0302, 0.8273, 0.0852, 0.0054, 0.0039, 0.0358, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  3.443958044052124 ; mask density:  0.06509079039096832 ; pred:  tensor([0.0303, 0.8275, 0.0849, 0.0054, 0.0039, 0.0358, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  3.3061752319335938 ; mask density:  0.06291720271110535 ; pred:  tensor([0.0303, 0.8277, 0.0847, 0.0054, 0.0039, 0.0358, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  3.1786539554595947 ; mask density:  0.06087817624211311 ; pred:  tensor([0.0304, 0.8279, 0.0845, 0.0054, 0.0039, 0.0357, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  3.0604538917541504 ; mask density:  0.058961544185876846 ; pred:  tensor([0.0304, 0.8281, 0.0843, 0.0054, 0.0039, 0.0357, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  2.950714588165283 ; mask density:  0.05715596675872803 ; pred:  tensor([0.0305, 0.8282, 0.0842, 0.0054, 0.0039, 0.0357, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  2.848670244216919 ; mask density:  0.05545229837298393 ; pred:  tensor([0.0305, 0.8283, 0.0840, 0.0054, 0.0039, 0.0357, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  2.7536442279815674 ; mask density:  0.05384260043501854 ; pred:  tensor([0.0305, 0.8284, 0.0839, 0.0054, 0.0039, 0.0357, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  2.665017604827881 ; mask density:  0.052320029586553574 ; pred:  tensor([0.0306, 0.8284, 0.0838, 0.0054, 0.0039, 0.0358, 0.0110, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  2.5822105407714844 ; mask density:  0.050877686589956284 ; pred:  tensor([0.0306, 0.8283, 0.0837, 0.0054, 0.0039, 0.0359, 0.0110, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  2.504728078842163 ; mask density:  0.04951062425971031 ; pred:  tensor([0.0306, 0.8283, 0.0836, 0.0054, 0.0039, 0.0360, 0.0110, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  2.4321348667144775 ; mask density:  0.04821460321545601 ; pred:  tensor([0.0306, 0.8282, 0.0836, 0.0054, 0.0039, 0.0361, 0.0110, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  2.3640265464782715 ; mask density:  0.046985924243927 ; pred:  tensor([0.0307, 0.8281, 0.0835, 0.0054, 0.0039, 0.0362, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  2.300039768218994 ; mask density:  0.04582137241959572 ; pred:  tensor([0.0307, 0.8279, 0.0834, 0.0054, 0.0040, 0.0364, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  2.2398478984832764 ; mask density:  0.04471832513809204 ; pred:  tensor([0.0307, 0.8278, 0.0834, 0.0054, 0.0040, 0.0365, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  2.1831581592559814 ; mask density:  0.04367396980524063 ; pred:  tensor([0.0307, 0.8277, 0.0834, 0.0054, 0.0040, 0.0366, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  2.1296870708465576 ; mask density:  0.04268563166260719 ; pred:  tensor([0.0307, 0.8275, 0.0834, 0.0054, 0.0040, 0.0368, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  2.079186201095581 ; mask density:  0.041750743985176086 ; pred:  tensor([0.0307, 0.8274, 0.0834, 0.0054, 0.0040, 0.0368, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  2.0314297676086426 ; mask density:  0.04086677357554436 ; pred:  tensor([0.0307, 0.8274, 0.0833, 0.0054, 0.0040, 0.0369, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  1.9862128496170044 ; mask density:  0.04003122076392174 ; pred:  tensor([0.0307, 0.8273, 0.0833, 0.0054, 0.0040, 0.0370, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  1.943347692489624 ; mask density:  0.03924160450696945 ; pred:  tensor([0.0307, 0.8273, 0.0833, 0.0054, 0.0040, 0.0370, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  1.9026623964309692 ; mask density:  0.03849542886018753 ; pred:  tensor([0.0307, 0.8274, 0.0832, 0.0054, 0.0040, 0.0370, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  1.8640007972717285 ; mask density:  0.03779023140668869 ; pred:  tensor([0.0307, 0.8275, 0.0831, 0.0054, 0.0040, 0.0370, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  1.8272181749343872 ; mask density:  0.03712352737784386 ; pred:  tensor([0.0307, 0.8276, 0.0831, 0.0054, 0.0040, 0.0370, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  1.7921844720840454 ; mask density:  0.03649294003844261 ; pred:  tensor([0.0307, 0.8277, 0.0830, 0.0053, 0.0040, 0.0369, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  1.758780837059021 ; mask density:  0.035896185785532 ; pred:  tensor([0.0308, 0.8278, 0.0829, 0.0053, 0.0040, 0.0369, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  1.7268949747085571 ; mask density:  0.035331033170223236 ; pred:  tensor([0.0308, 0.8279, 0.0828, 0.0053, 0.0040, 0.0369, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  1.6964248418807983 ; mask density:  0.03479532524943352 ; pred:  tensor([0.0308, 0.8281, 0.0827, 0.0053, 0.0040, 0.0368, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  1.6672680377960205 ; mask density:  0.034287139773368835 ; pred:  tensor([0.0308, 0.8282, 0.0826, 0.0053, 0.0040, 0.0368, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  1.6393404006958008 ; mask density:  0.03380454331636429 ; pred:  tensor([0.0308, 0.8283, 0.0825, 0.0053, 0.0040, 0.0368, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  1.6125662326812744 ; mask density:  0.033345747739076614 ; pred:  tensor([0.0309, 0.8285, 0.0824, 0.0053, 0.0040, 0.0367, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  1.5868724584579468 ; mask density:  0.032909054309129715 ; pred:  tensor([0.0309, 0.8286, 0.0823, 0.0053, 0.0040, 0.0366, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  1.5621916055679321 ; mask density:  0.032492902129888535 ; pred:  tensor([0.0309, 0.8288, 0.0822, 0.0053, 0.0040, 0.0366, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  1.538461446762085 ; mask density:  0.032095856964588165 ; pred:  tensor([0.0310, 0.8290, 0.0821, 0.0053, 0.0040, 0.0365, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  1.5156257152557373 ; mask density:  0.03171653300523758 ; pred:  tensor([0.0310, 0.8291, 0.0819, 0.0053, 0.0040, 0.0365, 0.0110, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  1.4936317205429077 ; mask density:  0.03135374188423157 ; pred:  tensor([0.0310, 0.8292, 0.0819, 0.0053, 0.0039, 0.0365, 0.0110, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  1.4724260568618774 ; mask density:  0.03100637160241604 ; pred:  tensor([0.0310, 0.8292, 0.0818, 0.0053, 0.0039, 0.0365, 0.0110, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  1.4519637823104858 ; mask density:  0.030673446133732796 ; pred:  tensor([0.0310, 0.8292, 0.0817, 0.0053, 0.0039, 0.0366, 0.0110, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  1.4322028160095215 ; mask density:  0.030354058369994164 ; pred:  tensor([0.0311, 0.8292, 0.0817, 0.0053, 0.0039, 0.0366, 0.0110, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  1.4131050109863281 ; mask density:  0.03004738688468933 ; pred:  tensor([0.0311, 0.8292, 0.0816, 0.0053, 0.0040, 0.0367, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  1.3946354389190674 ; mask density:  0.029752681031823158 ; pred:  tensor([0.0311, 0.8291, 0.0816, 0.0053, 0.0040, 0.0368, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  1.376760482788086 ; mask density:  0.029469255357980728 ; pred:  tensor([0.0311, 0.8290, 0.0816, 0.0053, 0.0040, 0.0368, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  1.3594491481781006 ; mask density:  0.029196474701166153 ; pred:  tensor([0.0311, 0.8289, 0.0815, 0.0053, 0.0040, 0.0369, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  1.3426727056503296 ; mask density:  0.028933754190802574 ; pred:  tensor([0.0311, 0.8289, 0.0815, 0.0053, 0.0040, 0.0370, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  1.3264033794403076 ; mask density:  0.02868053875863552 ; pred:  tensor([0.0311, 0.8288, 0.0815, 0.0053, 0.0040, 0.0370, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  1.3106160163879395 ; mask density:  0.028436319902539253 ; pred:  tensor([0.0311, 0.8288, 0.0815, 0.0053, 0.0040, 0.0371, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  1.2952868938446045 ; mask density:  0.02820061892271042 ; pred:  tensor([0.0311, 0.8288, 0.0814, 0.0053, 0.0040, 0.0371, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  1.2803938388824463 ; mask density:  0.027972979471087456 ; pred:  tensor([0.0312, 0.8288, 0.0814, 0.0052, 0.0040, 0.0371, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  1.2659167051315308 ; mask density:  0.027752967551350594 ; pred:  tensor([0.0312, 0.8289, 0.0814, 0.0052, 0.0040, 0.0370, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  1.2518360614776611 ; mask density:  0.027540167793631554 ; pred:  tensor([0.0312, 0.8290, 0.0814, 0.0052, 0.0040, 0.0370, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  1.238128662109375 ; mask density:  0.027334323152899742 ; pred:  tensor([0.0312, 0.8291, 0.0813, 0.0052, 0.0040, 0.0370, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  1.2247745990753174 ; mask density:  0.027135051786899567 ; pred:  tensor([0.0312, 0.8292, 0.0813, 0.0052, 0.0039, 0.0369, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  1.2117657661437988 ; mask density:  0.02694198489189148 ; pred:  tensor([0.0312, 0.8294, 0.0812, 0.0052, 0.0039, 0.0368, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  1.1990886926651 ; mask density:  0.026754779741168022 ; pred:  tensor([0.0312, 0.8296, 0.0811, 0.0052, 0.0039, 0.0366, 0.0110, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  1.1867315769195557 ; mask density:  0.02657311223447323 ; pred:  tensor([0.0312, 0.8299, 0.0810, 0.0052, 0.0039, 0.0365, 0.0110, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  9.64981722831726\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_630graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  635 146\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  211.96658325195312 ; mask density:  0.7115522623062134 ; pred:  tensor([0.0374, 0.6467, 0.2600, 0.0317, 0.0058, 0.0085, 0.0073, 0.0026],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  206.09901428222656 ; mask density:  0.6918359994888306 ; pred:  tensor([0.0368, 0.7122, 0.2050, 0.0230, 0.0056, 0.0093, 0.0061, 0.0020],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  200.01112365722656 ; mask density:  0.6712214946746826 ; pred:  tensor([0.0367, 0.7570, 0.1664, 0.0176, 0.0054, 0.0100, 0.0053, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  193.7040557861328 ; mask density:  0.6497323513031006 ; pred:  tensor([0.0373, 0.7854, 0.1404, 0.0144, 0.0055, 0.0107, 0.0047, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  187.1811065673828 ; mask density:  0.6274540424346924 ; pred:  tensor([0.0381, 0.8045, 0.1224, 0.0124, 0.0056, 0.0114, 0.0044, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  180.4569091796875 ; mask density:  0.6044866442680359 ; pred:  tensor([0.0387, 0.8173, 0.1101, 0.0110, 0.0056, 0.0120, 0.0042, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  173.5531463623047 ; mask density:  0.5809375643730164 ; pred:  tensor([0.0388, 0.8251, 0.1026, 0.0102, 0.0056, 0.0125, 0.0041, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  166.4944610595703 ; mask density:  0.5569232702255249 ; pred:  tensor([0.0387, 0.8307, 0.0974, 0.0095, 0.0055, 0.0130, 0.0041, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  159.31112670898438 ; mask density:  0.5325666069984436 ; pred:  tensor([0.0385, 0.8351, 0.0933, 0.0090, 0.0054, 0.0135, 0.0041, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  152.0380401611328 ; mask density:  0.507997453212738 ; pred:  tensor([0.0383, 0.8386, 0.0901, 0.0086, 0.0053, 0.0139, 0.0042, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  144.71298217773438 ; mask density:  0.4833509624004364 ; pred:  tensor([0.0380, 0.8413, 0.0877, 0.0083, 0.0052, 0.0144, 0.0042, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  137.37623596191406 ; mask density:  0.4587695896625519 ; pred:  tensor([0.0376, 0.8434, 0.0857, 0.0080, 0.0052, 0.0149, 0.0042, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  130.0695343017578 ; mask density:  0.4343966543674469 ; pred:  tensor([0.0371, 0.8451, 0.0842, 0.0078, 0.0051, 0.0153, 0.0043, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  122.83524322509766 ; mask density:  0.4103764593601227 ; pred:  tensor([0.0367, 0.8464, 0.0831, 0.0077, 0.0051, 0.0158, 0.0043, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  115.7147445678711 ; mask density:  0.38684266805648804 ; pred:  tensor([0.0362, 0.8474, 0.0822, 0.0075, 0.0050, 0.0162, 0.0044, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  108.74876403808594 ; mask density:  0.363932728767395 ; pred:  tensor([0.0357, 0.8478, 0.0819, 0.0074, 0.0049, 0.0166, 0.0045, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  101.97479248046875 ; mask density:  0.3417623043060303 ; pred:  tensor([0.0353, 0.8476, 0.0822, 0.0075, 0.0049, 0.0169, 0.0046, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  95.42586517333984 ; mask density:  0.3204306960105896 ; pred:  tensor([0.0349, 0.8474, 0.0826, 0.0075, 0.0048, 0.0172, 0.0047, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  89.13126373291016 ; mask density:  0.30002328753471375 ; pred:  tensor([0.0345, 0.8471, 0.0830, 0.0075, 0.0047, 0.0174, 0.0048, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  83.11521911621094 ; mask density:  0.28060778975486755 ; pred:  tensor([0.0340, 0.8468, 0.0835, 0.0075, 0.0046, 0.0177, 0.0049, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  77.39659118652344 ; mask density:  0.262231707572937 ; pred:  tensor([0.0336, 0.8465, 0.0841, 0.0075, 0.0045, 0.0179, 0.0050, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  71.98860931396484 ; mask density:  0.24492226541042328 ; pred:  tensor([0.0332, 0.8463, 0.0845, 0.0075, 0.0044, 0.0180, 0.0051, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  66.89916229248047 ; mask density:  0.22869053483009338 ; pred:  tensor([0.0329, 0.8464, 0.0847, 0.0075, 0.0043, 0.0181, 0.0051, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  62.13117980957031 ; mask density:  0.21353095769882202 ; pred:  tensor([0.0326, 0.8465, 0.0848, 0.0075, 0.0043, 0.0181, 0.0052, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  57.68336868286133 ; mask density:  0.19942958652973175 ; pred:  tensor([0.0325, 0.8465, 0.0851, 0.0075, 0.0042, 0.0180, 0.0052, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  53.54916763305664 ; mask density:  0.18635395169258118 ; pred:  tensor([0.0323, 0.8468, 0.0850, 0.0075, 0.0042, 0.0179, 0.0052, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  49.71980667114258 ; mask density:  0.17426469922065735 ; pred:  tensor([0.0322, 0.8471, 0.0850, 0.0074, 0.0042, 0.0178, 0.0052, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  46.183624267578125 ; mask density:  0.16311341524124146 ; pred:  tensor([0.0320, 0.8472, 0.0852, 0.0074, 0.0041, 0.0177, 0.0052, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  42.92633819580078 ; mask density:  0.15284933149814606 ; pred:  tensor([0.0319, 0.8470, 0.0856, 0.0075, 0.0041, 0.0176, 0.0052, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  39.93250274658203 ; mask density:  0.14341798424720764 ; pred:  tensor([0.0318, 0.8467, 0.0862, 0.0075, 0.0041, 0.0174, 0.0052, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  37.185665130615234 ; mask density:  0.1347680538892746 ; pred:  tensor([0.0317, 0.8462, 0.0869, 0.0076, 0.0041, 0.0172, 0.0052, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  34.668853759765625 ; mask density:  0.12680891156196594 ; pred:  tensor([0.0316, 0.8458, 0.0875, 0.0076, 0.0041, 0.0171, 0.0052, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  32.364620208740234 ; mask density:  0.11948801577091217 ; pred:  tensor([0.0315, 0.8457, 0.0876, 0.0076, 0.0041, 0.0171, 0.0053, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  30.257061004638672 ; mask density:  0.11275654286146164 ; pred:  tensor([0.0315, 0.8454, 0.0879, 0.0077, 0.0041, 0.0171, 0.0053, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  28.330265045166016 ; mask density:  0.10656847804784775 ; pred:  tensor([0.0314, 0.8451, 0.0882, 0.0077, 0.0041, 0.0171, 0.0053, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  26.569278717041016 ; mask density:  0.10088050365447998 ; pred:  tensor([0.0313, 0.8444, 0.0888, 0.0077, 0.0041, 0.0171, 0.0053, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  24.96005630493164 ; mask density:  0.09565065801143646 ; pred:  tensor([0.0311, 0.8435, 0.0898, 0.0078, 0.0041, 0.0172, 0.0054, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  23.489116668701172 ; mask density:  0.09084880352020264 ; pred:  tensor([0.0309, 0.8423, 0.0909, 0.0079, 0.0041, 0.0172, 0.0055, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  22.144163131713867 ; mask density:  0.08644084632396698 ; pred:  tensor([0.0308, 0.8410, 0.0922, 0.0080, 0.0041, 0.0172, 0.0055, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  20.91335105895996 ; mask density:  0.08239677548408508 ; pred:  tensor([0.0307, 0.8399, 0.0932, 0.0081, 0.0041, 0.0173, 0.0056, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  19.786149978637695 ; mask density:  0.07868485152721405 ; pred:  tensor([0.0306, 0.8390, 0.0941, 0.0082, 0.0041, 0.0173, 0.0056, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  18.753419876098633 ; mask density:  0.07530327141284943 ; pred:  tensor([0.0305, 0.8378, 0.0953, 0.0083, 0.0041, 0.0172, 0.0057, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  17.805763244628906 ; mask density:  0.07222224026918411 ; pred:  tensor([0.0304, 0.8371, 0.0962, 0.0084, 0.0040, 0.0170, 0.0057, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  16.934946060180664 ; mask density:  0.06941334158182144 ; pred:  tensor([0.0304, 0.8370, 0.0965, 0.0084, 0.0040, 0.0169, 0.0057, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  16.133474349975586 ; mask density:  0.0668499767780304 ; pred:  tensor([0.0304, 0.8377, 0.0960, 0.0083, 0.0040, 0.0168, 0.0057, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  15.395008087158203 ; mask density:  0.0645064115524292 ; pred:  tensor([0.0304, 0.8390, 0.0949, 0.0082, 0.0039, 0.0167, 0.0057, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  14.713491439819336 ; mask density:  0.06235693022608757 ; pred:  tensor([0.0304, 0.8406, 0.0935, 0.0080, 0.0039, 0.0167, 0.0057, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  14.083369255065918 ; mask density:  0.06037690490484238 ; pred:  tensor([0.0303, 0.8429, 0.0915, 0.0077, 0.0038, 0.0169, 0.0057, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  13.500463485717773 ; mask density:  0.05854598432779312 ; pred:  tensor([0.0303, 0.8452, 0.0893, 0.0075, 0.0038, 0.0171, 0.0057, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  12.960440635681152 ; mask density:  0.05684491991996765 ; pred:  tensor([0.0303, 0.8473, 0.0872, 0.0072, 0.0038, 0.0174, 0.0057, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  12.459432601928711 ; mask density:  0.055257830768823624 ; pred:  tensor([0.0303, 0.8492, 0.0853, 0.0070, 0.0038, 0.0176, 0.0057, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  11.993926048278809 ; mask density:  0.053770773112773895 ; pred:  tensor([0.0304, 0.8508, 0.0837, 0.0068, 0.0037, 0.0178, 0.0058, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  11.560799598693848 ; mask density:  0.05236406996846199 ; pred:  tensor([0.0303, 0.8520, 0.0824, 0.0066, 0.0037, 0.0181, 0.0058, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  11.157142639160156 ; mask density:  0.05102979764342308 ; pred:  tensor([0.0303, 0.8529, 0.0816, 0.0065, 0.0037, 0.0182, 0.0058, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  10.780275344848633 ; mask density:  0.04975375160574913 ; pred:  tensor([0.0303, 0.8535, 0.0810, 0.0064, 0.0037, 0.0183, 0.0058, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  10.427872657775879 ; mask density:  0.0485399030148983 ; pred:  tensor([0.0302, 0.8537, 0.0807, 0.0064, 0.0037, 0.0185, 0.0058, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  10.098114013671875 ; mask density:  0.04738382250070572 ; pred:  tensor([0.0301, 0.8535, 0.0808, 0.0063, 0.0037, 0.0186, 0.0059, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  9.788884162902832 ; mask density:  0.046282026916742325 ; pred:  tensor([0.0300, 0.8533, 0.0810, 0.0063, 0.0037, 0.0187, 0.0059, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  9.498493194580078 ; mask density:  0.0452314168214798 ; pred:  tensor([0.0298, 0.8530, 0.0813, 0.0063, 0.0037, 0.0189, 0.0060, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  9.225464820861816 ; mask density:  0.04423017427325249 ; pred:  tensor([0.0297, 0.8525, 0.0816, 0.0063, 0.0037, 0.0191, 0.0061, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  8.96834945678711 ; mask density:  0.04327663034200668 ; pred:  tensor([0.0295, 0.8520, 0.0820, 0.0063, 0.0037, 0.0192, 0.0062, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  8.725997924804688 ; mask density:  0.042372070252895355 ; pred:  tensor([0.0294, 0.8514, 0.0825, 0.0063, 0.0037, 0.0194, 0.0062, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  8.497270584106445 ; mask density:  0.0415155403316021 ; pred:  tensor([0.0292, 0.8507, 0.0831, 0.0063, 0.0037, 0.0195, 0.0063, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  8.281060218811035 ; mask density:  0.04070346802473068 ; pred:  tensor([0.0291, 0.8501, 0.0837, 0.0063, 0.0037, 0.0197, 0.0064, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  8.07628059387207 ; mask density:  0.039935071021318436 ; pred:  tensor([0.0290, 0.8495, 0.0841, 0.0063, 0.0037, 0.0198, 0.0065, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  7.882246017456055 ; mask density:  0.03921519219875336 ; pred:  tensor([0.0289, 0.8490, 0.0845, 0.0063, 0.0037, 0.0200, 0.0066, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  7.698144912719727 ; mask density:  0.038541264832019806 ; pred:  tensor([0.0288, 0.8484, 0.0850, 0.0064, 0.0037, 0.0201, 0.0066, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  7.523148536682129 ; mask density:  0.037888918071985245 ; pred:  tensor([0.0287, 0.8480, 0.0854, 0.0064, 0.0037, 0.0202, 0.0067, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  7.356316089630127 ; mask density:  0.03725762665271759 ; pred:  tensor([0.0286, 0.8477, 0.0854, 0.0064, 0.0037, 0.0204, 0.0067, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  7.197307586669922 ; mask density:  0.03665586933493614 ; pred:  tensor([0.0286, 0.8475, 0.0856, 0.0064, 0.0037, 0.0205, 0.0068, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  7.045620441436768 ; mask density:  0.03608130291104317 ; pred:  tensor([0.0285, 0.8473, 0.0857, 0.0064, 0.0037, 0.0206, 0.0068, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  6.900728702545166 ; mask density:  0.03553157299757004 ; pred:  tensor([0.0285, 0.8472, 0.0858, 0.0064, 0.0037, 0.0206, 0.0068, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  6.762157440185547 ; mask density:  0.03500280901789665 ; pred:  tensor([0.0284, 0.8470, 0.0858, 0.0064, 0.0037, 0.0207, 0.0069, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  6.629484176635742 ; mask density:  0.034492917358875275 ; pred:  tensor([0.0284, 0.8469, 0.0859, 0.0064, 0.0037, 0.0208, 0.0069, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  6.502341270446777 ; mask density:  0.0339999757707119 ; pred:  tensor([0.0283, 0.8467, 0.0859, 0.0064, 0.0037, 0.0209, 0.0069, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  6.380276203155518 ; mask density:  0.03350970149040222 ; pred:  tensor([0.0283, 0.8467, 0.0859, 0.0064, 0.0038, 0.0210, 0.0069, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  6.262637615203857 ; mask density:  0.03301997855305672 ; pred:  tensor([0.0283, 0.8468, 0.0856, 0.0063, 0.0038, 0.0211, 0.0069, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  6.148892402648926 ; mask density:  0.032520540058612823 ; pred:  tensor([0.0284, 0.8474, 0.0848, 0.0063, 0.0038, 0.0213, 0.0069, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  6.0388407707214355 ; mask density:  0.03201470151543617 ; pred:  tensor([0.0285, 0.8482, 0.0835, 0.0062, 0.0038, 0.0217, 0.0070, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  5.932733535766602 ; mask density:  0.03150710090994835 ; pred:  tensor([0.0286, 0.8489, 0.0824, 0.0060, 0.0038, 0.0221, 0.0070, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  5.830379962921143 ; mask density:  0.03099958784878254 ; pred:  tensor([0.0286, 0.8495, 0.0814, 0.0059, 0.0039, 0.0226, 0.0071, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  5.731560230255127 ; mask density:  0.03049357980489731 ; pred:  tensor([0.0287, 0.8499, 0.0804, 0.0058, 0.0039, 0.0231, 0.0072, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  5.63609504699707 ; mask density:  0.029991140589118004 ; pred:  tensor([0.0287, 0.8502, 0.0795, 0.0057, 0.0039, 0.0236, 0.0073, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  5.543720722198486 ; mask density:  0.029494253918528557 ; pred:  tensor([0.0288, 0.8503, 0.0788, 0.0056, 0.0039, 0.0241, 0.0074, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  5.454512119293213 ; mask density:  0.02900747023522854 ; pred:  tensor([0.0288, 0.8501, 0.0783, 0.0055, 0.0039, 0.0246, 0.0076, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  5.36849308013916 ; mask density:  0.028528235852718353 ; pred:  tensor([0.0289, 0.8495, 0.0781, 0.0055, 0.0039, 0.0253, 0.0077, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  5.285287380218506 ; mask density:  0.02805873192846775 ; pred:  tensor([0.0289, 0.8488, 0.0778, 0.0054, 0.0040, 0.0261, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  5.204799175262451 ; mask density:  0.027601206675171852 ; pred:  tensor([0.0290, 0.8478, 0.0775, 0.0054, 0.0040, 0.0270, 0.0081, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  5.12693977355957 ; mask density:  0.027156654745340347 ; pred:  tensor([0.0292, 0.8466, 0.0774, 0.0054, 0.0040, 0.0279, 0.0083, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  5.051447868347168 ; mask density:  0.02672468312084675 ; pred:  tensor([0.0293, 0.8454, 0.0773, 0.0054, 0.0040, 0.0288, 0.0086, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  4.978219985961914 ; mask density:  0.02630510739982128 ; pred:  tensor([0.0294, 0.8441, 0.0773, 0.0054, 0.0041, 0.0297, 0.0088, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  4.907103061676025 ; mask density:  0.025896655395627022 ; pred:  tensor([0.0295, 0.8429, 0.0774, 0.0054, 0.0041, 0.0305, 0.0091, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  4.838020324707031 ; mask density:  0.02549896389245987 ; pred:  tensor([0.0297, 0.8416, 0.0776, 0.0054, 0.0041, 0.0312, 0.0093, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  4.770900249481201 ; mask density:  0.0251116044819355 ; pred:  tensor([0.0298, 0.8403, 0.0777, 0.0054, 0.0041, 0.0320, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  4.705494403839111 ; mask density:  0.024731427431106567 ; pred:  tensor([0.0299, 0.8391, 0.0778, 0.0054, 0.0041, 0.0327, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  4.641798973083496 ; mask density:  0.024359598755836487 ; pred:  tensor([0.0301, 0.8380, 0.0778, 0.0054, 0.0042, 0.0333, 0.0100, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  4.579718589782715 ; mask density:  0.023995308205485344 ; pred:  tensor([0.0302, 0.8370, 0.0778, 0.0054, 0.0042, 0.0340, 0.0102, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  4.5191779136657715 ; mask density:  0.023634199053049088 ; pred:  tensor([0.0303, 0.8361, 0.0778, 0.0054, 0.0042, 0.0346, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  4.459862232208252 ; mask density:  0.023276610299944878 ; pred:  tensor([0.0304, 0.8354, 0.0777, 0.0054, 0.0042, 0.0350, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  4.4017839431762695 ; mask density:  0.022923678159713745 ; pred:  tensor([0.0305, 0.8350, 0.0775, 0.0054, 0.0043, 0.0354, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  9.931684970855713\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_635graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  640 226\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  466.94854736328125 ; mask density:  0.7104141116142273 ; pred:  tensor([3.9857e-02, 8.6951e-01, 5.7848e-02, 4.7560e-03, 3.7855e-03, 1.8407e-02,\n",
            "        5.0170e-03, 8.1814e-04], grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  454.1392822265625 ; mask density:  0.689463198184967 ; pred:  tensor([3.8156e-02, 8.6761e-01, 6.0983e-02, 4.8914e-03, 3.6842e-03, 1.8662e-02,\n",
            "        5.1765e-03, 8.3227e-04], grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  440.7680358886719 ; mask density:  0.6676892638206482 ; pred:  tensor([3.6780e-02, 8.6500e-01, 6.4410e-02, 5.0526e-03, 3.5903e-03, 1.8961e-02,\n",
            "        5.3584e-03, 8.5044e-04], grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  426.86016845703125 ; mask density:  0.6451470255851746 ; pred:  tensor([0.0356, 0.8621, 0.0678, 0.0052, 0.0035, 0.0193, 0.0055, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  412.4498596191406 ; mask density:  0.6219053864479065 ; pred:  tensor([0.0347, 0.8591, 0.0710, 0.0054, 0.0035, 0.0197, 0.0057, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  397.5804443359375 ; mask density:  0.5980474352836609 ; pred:  tensor([0.0339, 0.8562, 0.0741, 0.0056, 0.0035, 0.0200, 0.0058, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  382.3040771484375 ; mask density:  0.573663592338562 ; pred:  tensor([0.0332, 0.8534, 0.0769, 0.0058, 0.0035, 0.0204, 0.0059, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  366.6820068359375 ; mask density:  0.5488653779029846 ; pred:  tensor([0.0326, 0.8512, 0.0787, 0.0058, 0.0036, 0.0212, 0.0060, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  350.7850036621094 ; mask density:  0.5237736105918884 ; pred:  tensor([0.0320, 0.8489, 0.0805, 0.0059, 0.0036, 0.0219, 0.0061, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  334.69049072265625 ; mask density:  0.4985159933567047 ; pred:  tensor([0.0315, 0.8472, 0.0819, 0.0060, 0.0037, 0.0226, 0.0062, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  318.48309326171875 ; mask density:  0.47323182225227356 ; pred:  tensor([0.0309, 0.8459, 0.0828, 0.0060, 0.0038, 0.0233, 0.0063, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  302.2527160644531 ; mask density:  0.4480634033679962 ; pred:  tensor([0.0303, 0.8449, 0.0835, 0.0061, 0.0039, 0.0240, 0.0064, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  286.0921325683594 ; mask density:  0.4231594502925873 ; pred:  tensor([0.0298, 0.8441, 0.0837, 0.0061, 0.0040, 0.0248, 0.0065, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  270.0950622558594 ; mask density:  0.39865556359291077 ; pred:  tensor([0.0295, 0.8437, 0.0835, 0.0061, 0.0040, 0.0256, 0.0066, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  254.3538360595703 ; mask density:  0.3746916949748993 ; pred:  tensor([0.0292, 0.8437, 0.0828, 0.0060, 0.0041, 0.0265, 0.0067, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  238.9569549560547 ; mask density:  0.35138726234436035 ; pred:  tensor([0.0295, 0.8445, 0.0803, 0.0058, 0.0041, 0.0278, 0.0069, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  223.98805236816406 ; mask density:  0.32885614037513733 ; pred:  tensor([0.0299, 0.8451, 0.0779, 0.0057, 0.0041, 0.0290, 0.0071, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  209.52096557617188 ; mask density:  0.3071976900100708 ; pred:  tensor([0.0304, 0.8456, 0.0760, 0.0056, 0.0041, 0.0300, 0.0073, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  195.61936950683594 ; mask density:  0.2864914536476135 ; pred:  tensor([0.0309, 0.8466, 0.0733, 0.0054, 0.0041, 0.0311, 0.0075, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  182.3371124267578 ; mask density:  0.26680487394332886 ; pred:  tensor([0.0312, 0.8472, 0.0715, 0.0053, 0.0041, 0.0320, 0.0077, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  169.71527099609375 ; mask density:  0.2481895238161087 ; pred:  tensor([0.0315, 0.8475, 0.0703, 0.0052, 0.0042, 0.0325, 0.0078, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  157.78334045410156 ; mask density:  0.2306760549545288 ; pred:  tensor([0.0317, 0.8476, 0.0696, 0.0051, 0.0042, 0.0328, 0.0078, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  146.55853271484375 ; mask density:  0.21427710354328156 ; pred:  tensor([0.0318, 0.8477, 0.0693, 0.0051, 0.0042, 0.0330, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  136.04635620117188 ; mask density:  0.19898603856563568 ; pred:  tensor([0.0319, 0.8479, 0.0691, 0.0050, 0.0042, 0.0329, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  126.2422103881836 ; mask density:  0.1847805380821228 ; pred:  tensor([0.0318, 0.8484, 0.0693, 0.0050, 0.0042, 0.0324, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  117.13298797607422 ; mask density:  0.17163005471229553 ; pred:  tensor([0.0316, 0.8489, 0.0697, 0.0050, 0.0041, 0.0318, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  108.69779968261719 ; mask density:  0.1594924032688141 ; pred:  tensor([0.0313, 0.8493, 0.0704, 0.0050, 0.0040, 0.0310, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  100.90982818603516 ; mask density:  0.1483183354139328 ; pred:  tensor([0.0310, 0.8497, 0.0712, 0.0050, 0.0039, 0.0302, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  93.73777770996094 ; mask density:  0.13805420696735382 ; pred:  tensor([0.0307, 0.8499, 0.0721, 0.0050, 0.0039, 0.0294, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  87.147216796875 ; mask density:  0.12864314019680023 ; pred:  tensor([0.0305, 0.8499, 0.0732, 0.0051, 0.0038, 0.0286, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  81.101806640625 ; mask density:  0.12002665549516678 ; pred:  tensor([0.0302, 0.8498, 0.0745, 0.0051, 0.0037, 0.0278, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  75.56429290771484 ; mask density:  0.11214643716812134 ; pred:  tensor([0.0299, 0.8495, 0.0758, 0.0052, 0.0036, 0.0270, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  70.49752044677734 ; mask density:  0.10494592785835266 ; pred:  tensor([0.0296, 0.8490, 0.0773, 0.0052, 0.0035, 0.0264, 0.0080, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  65.86522674560547 ; mask density:  0.09837105870246887 ; pred:  tensor([0.0293, 0.8481, 0.0790, 0.0053, 0.0034, 0.0258, 0.0081, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  61.631553649902344 ; mask density:  0.09236624836921692 ; pred:  tensor([0.0291, 0.8469, 0.0801, 0.0053, 0.0034, 0.0259, 0.0082, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  57.7625617980957 ; mask density:  0.08688413351774216 ; pred:  tensor([0.0290, 0.8459, 0.0805, 0.0053, 0.0034, 0.0265, 0.0082, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  54.227622985839844 ; mask density:  0.08187920600175858 ; pred:  tensor([0.0289, 0.8446, 0.0812, 0.0054, 0.0035, 0.0271, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  50.99664306640625 ; mask density:  0.07730820775032043 ; pred:  tensor([0.0288, 0.8436, 0.0818, 0.0054, 0.0035, 0.0275, 0.0084, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  48.04201889038086 ; mask density:  0.07313131541013718 ; pred:  tensor([0.0288, 0.8429, 0.0821, 0.0054, 0.0035, 0.0278, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  45.33823776245117 ; mask density:  0.06931096315383911 ; pred:  tensor([0.0288, 0.8427, 0.0822, 0.0054, 0.0035, 0.0279, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  42.86211013793945 ; mask density:  0.06581448018550873 ; pred:  tensor([0.0288, 0.8428, 0.0820, 0.0054, 0.0035, 0.0279, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  40.59239196777344 ; mask density:  0.06261112540960312 ; pred:  tensor([0.0289, 0.8432, 0.0818, 0.0053, 0.0035, 0.0277, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  38.50971603393555 ; mask density:  0.05967290699481964 ; pred:  tensor([0.0289, 0.8438, 0.0814, 0.0053, 0.0034, 0.0275, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  36.59651184082031 ; mask density:  0.056974146515131 ; pred:  tensor([0.0290, 0.8446, 0.0810, 0.0053, 0.0034, 0.0271, 0.0084, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  34.83684539794922 ; mask density:  0.05449174344539642 ; pred:  tensor([0.0291, 0.8454, 0.0806, 0.0053, 0.0034, 0.0268, 0.0084, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  33.21632766723633 ; mask density:  0.052205707877874374 ; pred:  tensor([0.0292, 0.8462, 0.0801, 0.0053, 0.0034, 0.0264, 0.0084, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  31.721891403198242 ; mask density:  0.05009746178984642 ; pred:  tensor([0.0293, 0.8469, 0.0796, 0.0052, 0.0033, 0.0262, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  30.341829299926758 ; mask density:  0.04815005511045456 ; pred:  tensor([0.0294, 0.8476, 0.0792, 0.0052, 0.0033, 0.0259, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  29.065549850463867 ; mask density:  0.04634823277592659 ; pred:  tensor([0.0295, 0.8482, 0.0788, 0.0052, 0.0033, 0.0258, 0.0082, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  27.88343620300293 ; mask density:  0.044680263847112656 ; pred:  tensor([0.0295, 0.8487, 0.0784, 0.0052, 0.0033, 0.0256, 0.0082, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  26.786956787109375 ; mask density:  0.043132483959198 ; pred:  tensor([0.0296, 0.8491, 0.0781, 0.0052, 0.0033, 0.0255, 0.0081, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  25.768367767333984 ; mask density:  0.04169485345482826 ; pred:  tensor([0.0296, 0.8494, 0.0778, 0.0052, 0.0033, 0.0255, 0.0081, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  24.820709228515625 ; mask density:  0.04035728797316551 ; pred:  tensor([0.0297, 0.8496, 0.0776, 0.0052, 0.0033, 0.0254, 0.0081, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  23.937700271606445 ; mask density:  0.03911098837852478 ; pred:  tensor([0.0297, 0.8498, 0.0775, 0.0052, 0.0034, 0.0254, 0.0080, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  23.11368179321289 ; mask density:  0.03794771805405617 ; pred:  tensor([0.0297, 0.8499, 0.0774, 0.0052, 0.0034, 0.0254, 0.0080, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  22.34354019165039 ; mask density:  0.03686005622148514 ; pred:  tensor([0.0297, 0.8500, 0.0773, 0.0052, 0.0034, 0.0255, 0.0080, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  21.62268829345703 ; mask density:  0.03583955392241478 ; pred:  tensor([0.0297, 0.8501, 0.0772, 0.0052, 0.0034, 0.0255, 0.0079, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  20.94693374633789 ; mask density:  0.034880392253398895 ; pred:  tensor([0.0297, 0.8501, 0.0771, 0.0052, 0.0034, 0.0255, 0.0079, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  20.3125057220459 ; mask density:  0.03397739306092262 ; pred:  tensor([0.0297, 0.8501, 0.0771, 0.0052, 0.0034, 0.0255, 0.0079, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  19.71599006652832 ; mask density:  0.033125925809144974 ; pred:  tensor([0.0297, 0.8501, 0.0770, 0.0052, 0.0034, 0.0256, 0.0079, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  19.154312133789062 ; mask density:  0.032321806997060776 ; pred:  tensor([0.0297, 0.8501, 0.0770, 0.0052, 0.0035, 0.0256, 0.0079, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  18.624664306640625 ; mask density:  0.03155924379825592 ; pred:  tensor([0.0297, 0.8501, 0.0769, 0.0052, 0.0035, 0.0257, 0.0079, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  18.12452507019043 ; mask density:  0.030834166333079338 ; pred:  tensor([0.0297, 0.8500, 0.0770, 0.0052, 0.0035, 0.0258, 0.0078, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  17.651582717895508 ; mask density:  0.0301445834338665 ; pred:  tensor([0.0296, 0.8497, 0.0771, 0.0052, 0.0035, 0.0259, 0.0078, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  17.203765869140625 ; mask density:  0.02948792465031147 ; pred:  tensor([0.0296, 0.8494, 0.0772, 0.0052, 0.0035, 0.0261, 0.0079, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  16.779155731201172 ; mask density:  0.028864014893770218 ; pred:  tensor([0.0296, 0.8491, 0.0774, 0.0052, 0.0036, 0.0262, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  16.376041412353516 ; mask density:  0.02827066369354725 ; pred:  tensor([0.0296, 0.8489, 0.0774, 0.0052, 0.0036, 0.0264, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  15.992817878723145 ; mask density:  0.027705851942300797 ; pred:  tensor([0.0296, 0.8489, 0.0773, 0.0052, 0.0036, 0.0265, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  15.628072738647461 ; mask density:  0.027165472507476807 ; pred:  tensor([0.0296, 0.8490, 0.0771, 0.0052, 0.0036, 0.0266, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  15.280488967895508 ; mask density:  0.026648983359336853 ; pred:  tensor([0.0296, 0.8490, 0.0770, 0.0052, 0.0036, 0.0266, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  14.948854446411133 ; mask density:  0.026154866442084312 ; pred:  tensor([0.0296, 0.8490, 0.0769, 0.0052, 0.0036, 0.0267, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  14.632072448730469 ; mask density:  0.025681694969534874 ; pred:  tensor([0.0296, 0.8491, 0.0769, 0.0052, 0.0036, 0.0267, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  14.329146385192871 ; mask density:  0.02523037977516651 ; pred:  tensor([0.0296, 0.8491, 0.0769, 0.0052, 0.0036, 0.0267, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  14.039162635803223 ; mask density:  0.024798527359962463 ; pred:  tensor([0.0296, 0.8491, 0.0768, 0.0052, 0.0036, 0.0266, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  13.761262893676758 ; mask density:  0.02438485249876976 ; pred:  tensor([0.0296, 0.8493, 0.0768, 0.0052, 0.0036, 0.0266, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  13.494678497314453 ; mask density:  0.02398584969341755 ; pred:  tensor([0.0296, 0.8495, 0.0766, 0.0052, 0.0036, 0.0265, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  13.238688468933105 ; mask density:  0.023600688204169273 ; pred:  tensor([0.0297, 0.8495, 0.0766, 0.0052, 0.0036, 0.0265, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  12.99262809753418 ; mask density:  0.023230938240885735 ; pred:  tensor([0.0296, 0.8495, 0.0767, 0.0052, 0.0036, 0.0265, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  12.755908012390137 ; mask density:  0.022875681519508362 ; pred:  tensor([0.0296, 0.8495, 0.0767, 0.0052, 0.0036, 0.0265, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  12.527953147888184 ; mask density:  0.022535070776939392 ; pred:  tensor([0.0296, 0.8495, 0.0767, 0.0052, 0.0036, 0.0264, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  12.308266639709473 ; mask density:  0.022205766290426254 ; pred:  tensor([0.0296, 0.8495, 0.0767, 0.0052, 0.0036, 0.0264, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  12.096356391906738 ; mask density:  0.021887149661779404 ; pred:  tensor([0.0296, 0.8494, 0.0768, 0.0052, 0.0036, 0.0264, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  11.891776084899902 ; mask density:  0.021580994129180908 ; pred:  tensor([0.0296, 0.8493, 0.0770, 0.0052, 0.0036, 0.0264, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  11.694141387939453 ; mask density:  0.02128651551902294 ; pred:  tensor([0.0295, 0.8492, 0.0772, 0.0052, 0.0036, 0.0264, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  11.503064155578613 ; mask density:  0.021000711247324944 ; pred:  tensor([0.0295, 0.8491, 0.0773, 0.0052, 0.0036, 0.0264, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  11.318192481994629 ; mask density:  0.020725488662719727 ; pred:  tensor([0.0295, 0.8490, 0.0774, 0.0052, 0.0036, 0.0263, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  11.139204978942871 ; mask density:  0.0204577948898077 ; pred:  tensor([0.0295, 0.8490, 0.0775, 0.0052, 0.0036, 0.0263, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  10.965792655944824 ; mask density:  0.02019624598324299 ; pred:  tensor([0.0295, 0.8489, 0.0776, 0.0052, 0.0036, 0.0263, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  10.797677993774414 ; mask density:  0.019943008199334145 ; pred:  tensor([0.0294, 0.8488, 0.0777, 0.0052, 0.0036, 0.0263, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  10.63459300994873 ; mask density:  0.01969628781080246 ; pred:  tensor([0.0294, 0.8488, 0.0777, 0.0052, 0.0036, 0.0262, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  10.476297378540039 ; mask density:  0.019455835223197937 ; pred:  tensor([0.0294, 0.8488, 0.0778, 0.0052, 0.0036, 0.0263, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  10.322568893432617 ; mask density:  0.019223826006054878 ; pred:  tensor([0.0294, 0.8487, 0.0778, 0.0052, 0.0036, 0.0263, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  10.173176765441895 ; mask density:  0.01899978518486023 ; pred:  tensor([0.0294, 0.8487, 0.0778, 0.0052, 0.0036, 0.0263, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  10.027941703796387 ; mask density:  0.018780861049890518 ; pred:  tensor([0.0294, 0.8489, 0.0777, 0.0052, 0.0036, 0.0262, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  9.886666297912598 ; mask density:  0.018566878512501717 ; pred:  tensor([0.0294, 0.8490, 0.0776, 0.0052, 0.0036, 0.0262, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  9.749174118041992 ; mask density:  0.018357684835791588 ; pred:  tensor([0.0294, 0.8490, 0.0776, 0.0052, 0.0036, 0.0262, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  9.615300178527832 ; mask density:  0.01815313845872879 ; pred:  tensor([0.0294, 0.8489, 0.0776, 0.0052, 0.0036, 0.0262, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  9.484906196594238 ; mask density:  0.017956702038645744 ; pred:  tensor([0.0294, 0.8488, 0.0777, 0.0052, 0.0036, 0.0262, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  9.357831954956055 ; mask density:  0.017767872661352158 ; pred:  tensor([0.0294, 0.8488, 0.0777, 0.0052, 0.0036, 0.0263, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  9.23393726348877 ; mask density:  0.017583724111318588 ; pred:  tensor([0.0294, 0.8489, 0.0777, 0.0052, 0.0036, 0.0262, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  13.273715257644653\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_640graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  645 108\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  117.80598449707031 ; mask density:  0.7125201225280762 ; pred:  tensor([0.0695, 0.3829, 0.4071, 0.1065, 0.0073, 0.0062, 0.0125, 0.0079],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  114.4487533569336 ; mask density:  0.6937622427940369 ; pred:  tensor([0.0592, 0.4711, 0.3667, 0.0728, 0.0064, 0.0072, 0.0111, 0.0055],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  110.98452758789062 ; mask density:  0.674095869064331 ; pred:  tensor([0.0496, 0.5606, 0.3144, 0.0477, 0.0055, 0.0084, 0.0099, 0.0038],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  107.43280029296875 ; mask density:  0.6537200212478638 ; pred:  tensor([0.0427, 0.6362, 0.2629, 0.0319, 0.0048, 0.0096, 0.0091, 0.0028],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  103.79209899902344 ; mask density:  0.6325790286064148 ; pred:  tensor([0.0381, 0.6949, 0.2187, 0.0223, 0.0042, 0.0110, 0.0085, 0.0021],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  100.06443786621094 ; mask density:  0.610704243183136 ; pred:  tensor([0.0351, 0.7354, 0.1861, 0.0167, 0.0038, 0.0126, 0.0085, 0.0018],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  96.24836730957031 ; mask density:  0.5882164835929871 ; pred:  tensor([0.0333, 0.7657, 0.1601, 0.0131, 0.0035, 0.0142, 0.0085, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  92.3545913696289 ; mask density:  0.5652434229850769 ; pred:  tensor([0.0323, 0.7881, 0.1395, 0.0106, 0.0033, 0.0160, 0.0087, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  88.39830780029297 ; mask density:  0.5419127941131592 ; pred:  tensor([0.0319, 0.8038, 0.1241, 0.0090, 0.0032, 0.0178, 0.0089, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  84.39653778076172 ; mask density:  0.5183578729629517 ; pred:  tensor([0.0318, 0.8143, 0.1130, 0.0079, 0.0031, 0.0195, 0.0093, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  80.36820983886719 ; mask density:  0.49471354484558105 ; pred:  tensor([0.0319, 0.8211, 0.1050, 0.0072, 0.0030, 0.0210, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  76.33336639404297 ; mask density:  0.4711209237575531 ; pred:  tensor([0.0322, 0.8259, 0.0991, 0.0067, 0.0030, 0.0222, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  72.31437683105469 ; mask density:  0.44772064685821533 ; pred:  tensor([0.0324, 0.8291, 0.0948, 0.0064, 0.0030, 0.0232, 0.0100, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  68.33428192138672 ; mask density:  0.4246479272842407 ; pred:  tensor([0.0326, 0.8310, 0.0920, 0.0062, 0.0029, 0.0239, 0.0102, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  64.41558837890625 ; mask density:  0.4020305871963501 ; pred:  tensor([0.0328, 0.8321, 0.0903, 0.0060, 0.0029, 0.0244, 0.0103, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  60.58009719848633 ; mask density:  0.3799864947795868 ; pred:  tensor([0.0330, 0.8326, 0.0896, 0.0060, 0.0029, 0.0245, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  56.84785079956055 ; mask density:  0.35862427949905396 ; pred:  tensor([0.0330, 0.8329, 0.0892, 0.0060, 0.0028, 0.0245, 0.0104, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  53.237335205078125 ; mask density:  0.3380372226238251 ; pred:  tensor([0.0331, 0.8332, 0.0890, 0.0060, 0.0028, 0.0245, 0.0104, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  49.764583587646484 ; mask density:  0.3182995915412903 ; pred:  tensor([0.0331, 0.8336, 0.0887, 0.0059, 0.0028, 0.0244, 0.0103, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  46.44291305541992 ; mask density:  0.29946741461753845 ; pred:  tensor([0.0332, 0.8345, 0.0879, 0.0059, 0.0028, 0.0244, 0.0103, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  43.28315353393555 ; mask density:  0.28158998489379883 ; pred:  tensor([0.0332, 0.8354, 0.0871, 0.0058, 0.0028, 0.0244, 0.0102, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  40.293270111083984 ; mask density:  0.2646971046924591 ; pred:  tensor([0.0332, 0.8362, 0.0864, 0.0058, 0.0027, 0.0244, 0.0101, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  37.4778938293457 ; mask density:  0.24880188703536987 ; pred:  tensor([0.0332, 0.8368, 0.0859, 0.0058, 0.0027, 0.0244, 0.0101, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  34.83845138549805 ; mask density:  0.23390182852745056 ; pred:  tensor([0.0332, 0.8375, 0.0850, 0.0057, 0.0027, 0.0247, 0.0101, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  32.374427795410156 ; mask density:  0.21998386085033417 ; pred:  tensor([0.0331, 0.8382, 0.0842, 0.0056, 0.0027, 0.0249, 0.0102, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  30.082910537719727 ; mask density:  0.20702184736728668 ; pred:  tensor([0.0331, 0.8387, 0.0835, 0.0056, 0.0027, 0.0252, 0.0102, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  27.9591064453125 ; mask density:  0.19498290121555328 ; pred:  tensor([0.0330, 0.8391, 0.0829, 0.0055, 0.0027, 0.0253, 0.0102, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  25.996511459350586 ; mask density:  0.18382446467876434 ; pred:  tensor([0.0330, 0.8394, 0.0825, 0.0055, 0.0027, 0.0255, 0.0103, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  24.18765640258789 ; mask density:  0.1735011786222458 ; pred:  tensor([0.0330, 0.8395, 0.0821, 0.0055, 0.0027, 0.0258, 0.0104, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  22.524211883544922 ; mask density:  0.16396306455135345 ; pred:  tensor([0.0330, 0.8394, 0.0819, 0.0055, 0.0027, 0.0260, 0.0105, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  20.997190475463867 ; mask density:  0.15515904128551483 ; pred:  tensor([0.0329, 0.8391, 0.0818, 0.0055, 0.0027, 0.0262, 0.0106, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  19.597375869750977 ; mask density:  0.14703871309757233 ; pred:  tensor([0.0329, 0.8388, 0.0820, 0.0055, 0.0026, 0.0263, 0.0107, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  18.31566619873047 ; mask density:  0.1395513415336609 ; pred:  tensor([0.0328, 0.8383, 0.0824, 0.0055, 0.0026, 0.0264, 0.0108, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  17.14263343811035 ; mask density:  0.13264669477939606 ; pred:  tensor([0.0327, 0.8379, 0.0830, 0.0056, 0.0026, 0.0263, 0.0108, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  16.069847106933594 ; mask density:  0.12628011405467987 ; pred:  tensor([0.0325, 0.8374, 0.0838, 0.0056, 0.0026, 0.0261, 0.0108, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  15.088976860046387 ; mask density:  0.12040834128856659 ; pred:  tensor([0.0323, 0.8368, 0.0848, 0.0057, 0.0026, 0.0259, 0.0108, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  14.192103385925293 ; mask density:  0.1149907261133194 ; pred:  tensor([0.0320, 0.8361, 0.0860, 0.0058, 0.0025, 0.0256, 0.0108, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  13.37166690826416 ; mask density:  0.10999425500631332 ; pred:  tensor([0.0319, 0.8354, 0.0870, 0.0059, 0.0025, 0.0254, 0.0108, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  12.620626449584961 ; mask density:  0.10538521409034729 ; pred:  tensor([0.0317, 0.8348, 0.0879, 0.0060, 0.0025, 0.0252, 0.0108, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  11.932937622070312 ; mask density:  0.10113276541233063 ; pred:  tensor([0.0316, 0.8342, 0.0889, 0.0061, 0.0025, 0.0249, 0.0108, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  11.302886009216309 ; mask density:  0.09720970690250397 ; pred:  tensor([0.0315, 0.8333, 0.0901, 0.0062, 0.0024, 0.0247, 0.0108, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  10.725179672241211 ; mask density:  0.09359190613031387 ; pred:  tensor([0.0313, 0.8322, 0.0915, 0.0063, 0.0024, 0.0244, 0.0108, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  10.19498348236084 ; mask density:  0.09025668352842331 ; pred:  tensor([0.0311, 0.8310, 0.0931, 0.0064, 0.0024, 0.0241, 0.0107, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  9.707788467407227 ; mask density:  0.0871829017996788 ; pred:  tensor([0.0310, 0.8297, 0.0949, 0.0066, 0.0024, 0.0237, 0.0107, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  9.259581565856934 ; mask density:  0.08435512334108353 ; pred:  tensor([0.0308, 0.8282, 0.0968, 0.0067, 0.0024, 0.0233, 0.0106, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  8.846678733825684 ; mask density:  0.08175616711378098 ; pred:  tensor([0.0306, 0.8269, 0.0986, 0.0069, 0.0024, 0.0230, 0.0106, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  8.465747833251953 ; mask density:  0.0793701708316803 ; pred:  tensor([0.0304, 0.8256, 0.1003, 0.0070, 0.0024, 0.0227, 0.0105, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  8.113727569580078 ; mask density:  0.07718201726675034 ; pred:  tensor([0.0302, 0.8245, 0.1017, 0.0071, 0.0024, 0.0224, 0.0105, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  7.787802696228027 ; mask density:  0.07517166435718536 ; pred:  tensor([0.0301, 0.8237, 0.1028, 0.0071, 0.0024, 0.0222, 0.0104, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  7.4856672286987305 ; mask density:  0.07332558184862137 ; pred:  tensor([0.0300, 0.8232, 0.1035, 0.0072, 0.0024, 0.0221, 0.0104, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  7.204837799072266 ; mask density:  0.07162914425134659 ; pred:  tensor([0.0300, 0.8232, 0.1037, 0.0072, 0.0024, 0.0220, 0.0103, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  6.943659782409668 ; mask density:  0.07006639242172241 ; pred:  tensor([0.0300, 0.8235, 0.1036, 0.0071, 0.0024, 0.0220, 0.0103, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  6.700336933135986 ; mask density:  0.06862346827983856 ; pred:  tensor([0.0300, 0.8241, 0.1032, 0.0071, 0.0025, 0.0219, 0.0102, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  6.473239898681641 ; mask density:  0.06728550046682358 ; pred:  tensor([0.0300, 0.8249, 0.1025, 0.0070, 0.0025, 0.0219, 0.0102, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  6.261087894439697 ; mask density:  0.06604413688182831 ; pred:  tensor([0.0300, 0.8259, 0.1016, 0.0069, 0.0025, 0.0219, 0.0101, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  6.062613487243652 ; mask density:  0.06488902121782303 ; pred:  tensor([0.0300, 0.8270, 0.1006, 0.0068, 0.0025, 0.0220, 0.0101, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  5.87650728225708 ; mask density:  0.06380897015333176 ; pred:  tensor([0.0300, 0.8281, 0.0994, 0.0067, 0.0025, 0.0221, 0.0100, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  5.701974391937256 ; mask density:  0.06279192119836807 ; pred:  tensor([0.0301, 0.8291, 0.0984, 0.0066, 0.0025, 0.0222, 0.0100, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  5.538086414337158 ; mask density:  0.06182805821299553 ; pred:  tensor([0.0301, 0.8300, 0.0975, 0.0065, 0.0025, 0.0223, 0.0100, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  5.383918762207031 ; mask density:  0.060908589512109756 ; pred:  tensor([0.0301, 0.8307, 0.0967, 0.0065, 0.0026, 0.0225, 0.0100, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  5.238661766052246 ; mask density:  0.06002619490027428 ; pred:  tensor([0.0300, 0.8314, 0.0960, 0.0064, 0.0026, 0.0226, 0.0100, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  5.101761817932129 ; mask density:  0.05916754901409149 ; pred:  tensor([0.0300, 0.8318, 0.0956, 0.0064, 0.0026, 0.0226, 0.0100, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  4.972507953643799 ; mask density:  0.058325283229351044 ; pred:  tensor([0.0300, 0.8320, 0.0953, 0.0063, 0.0026, 0.0227, 0.0100, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  4.85029935836792 ; mask density:  0.05749695003032684 ; pred:  tensor([0.0299, 0.8320, 0.0953, 0.0063, 0.0026, 0.0228, 0.0100, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  4.734464168548584 ; mask density:  0.05668057128787041 ; pred:  tensor([0.0299, 0.8320, 0.0953, 0.0063, 0.0026, 0.0228, 0.0100, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  4.6244988441467285 ; mask density:  0.05587571859359741 ; pred:  tensor([0.0298, 0.8319, 0.0953, 0.0063, 0.0027, 0.0229, 0.0100, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  4.519986629486084 ; mask density:  0.055081501603126526 ; pred:  tensor([0.0298, 0.8318, 0.0955, 0.0063, 0.0027, 0.0229, 0.0100, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  4.4204487800598145 ; mask density:  0.054293084889650345 ; pred:  tensor([0.0297, 0.8317, 0.0956, 0.0063, 0.0027, 0.0229, 0.0100, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  4.325558185577393 ; mask density:  0.05351146683096886 ; pred:  tensor([0.0296, 0.8316, 0.0957, 0.0063, 0.0027, 0.0230, 0.0101, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  4.235020637512207 ; mask density:  0.05273689329624176 ; pred:  tensor([0.0296, 0.8313, 0.0959, 0.0063, 0.0027, 0.0230, 0.0101, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  4.148499011993408 ; mask density:  0.05196525156497955 ; pred:  tensor([0.0295, 0.8311, 0.0961, 0.0063, 0.0027, 0.0230, 0.0101, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  4.065680980682373 ; mask density:  0.051197733730077744 ; pred:  tensor([0.0295, 0.8308, 0.0963, 0.0063, 0.0027, 0.0231, 0.0102, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  3.986377239227295 ; mask density:  0.05043553560972214 ; pred:  tensor([0.0294, 0.8305, 0.0965, 0.0063, 0.0027, 0.0232, 0.0102, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  3.9103612899780273 ; mask density:  0.04968006908893585 ; pred:  tensor([0.0294, 0.8302, 0.0967, 0.0063, 0.0028, 0.0232, 0.0102, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  3.8373923301696777 ; mask density:  0.04893007129430771 ; pred:  tensor([0.0293, 0.8298, 0.0970, 0.0064, 0.0028, 0.0233, 0.0103, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  3.767273426055908 ; mask density:  0.04818763583898544 ; pred:  tensor([0.0293, 0.8295, 0.0972, 0.0064, 0.0028, 0.0234, 0.0103, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  3.6999144554138184 ; mask density:  0.0474560484290123 ; pred:  tensor([0.0292, 0.8291, 0.0975, 0.0064, 0.0028, 0.0235, 0.0104, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  3.635134220123291 ; mask density:  0.04673706740140915 ; pred:  tensor([0.0292, 0.8286, 0.0979, 0.0064, 0.0028, 0.0236, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  3.5727312564849854 ; mask density:  0.04603169113397598 ; pred:  tensor([0.0292, 0.8281, 0.0982, 0.0064, 0.0028, 0.0236, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  3.512488842010498 ; mask density:  0.0453406386077404 ; pred:  tensor([0.0291, 0.8277, 0.0985, 0.0064, 0.0028, 0.0237, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  3.4543731212615967 ; mask density:  0.04466456547379494 ; pred:  tensor([0.0291, 0.8272, 0.0988, 0.0065, 0.0028, 0.0238, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  3.3982198238372803 ; mask density:  0.04400123283267021 ; pred:  tensor([0.0290, 0.8269, 0.0991, 0.0065, 0.0029, 0.0238, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  3.343921422958374 ; mask density:  0.0433499738574028 ; pred:  tensor([0.0290, 0.8265, 0.0993, 0.0065, 0.0029, 0.0239, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  3.291414499282837 ; mask density:  0.04271195828914642 ; pred:  tensor([0.0289, 0.8262, 0.0996, 0.0065, 0.0029, 0.0240, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  3.240626335144043 ; mask density:  0.04208805784583092 ; pred:  tensor([0.0289, 0.8258, 0.0997, 0.0065, 0.0029, 0.0242, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  3.1914758682250977 ; mask density:  0.04147863760590553 ; pred:  tensor([0.0289, 0.8254, 0.0998, 0.0065, 0.0029, 0.0243, 0.0109, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  3.1438393592834473 ; mask density:  0.040882717818021774 ; pred:  tensor([0.0288, 0.8251, 0.0999, 0.0065, 0.0030, 0.0245, 0.0110, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  3.097658634185791 ; mask density:  0.04030177742242813 ; pred:  tensor([0.0288, 0.8248, 0.1000, 0.0065, 0.0030, 0.0247, 0.0111, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  3.0528464317321777 ; mask density:  0.039735060185194016 ; pred:  tensor([0.0288, 0.8245, 0.1001, 0.0065, 0.0030, 0.0248, 0.0112, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  3.0094356536865234 ; mask density:  0.03918422758579254 ; pred:  tensor([0.0287, 0.8242, 0.1002, 0.0065, 0.0030, 0.0250, 0.0112, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  2.9673397541046143 ; mask density:  0.03865006938576698 ; pred:  tensor([0.0287, 0.8238, 0.1004, 0.0065, 0.0031, 0.0251, 0.0113, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  2.926506519317627 ; mask density:  0.03813284635543823 ; pred:  tensor([0.0286, 0.8235, 0.1005, 0.0065, 0.0031, 0.0253, 0.0113, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  2.8868463039398193 ; mask density:  0.03763285279273987 ; pred:  tensor([0.0286, 0.8233, 0.1005, 0.0065, 0.0031, 0.0254, 0.0114, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  2.8483104705810547 ; mask density:  0.03715013712644577 ; pred:  tensor([0.0285, 0.8230, 0.1005, 0.0065, 0.0031, 0.0256, 0.0114, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  2.810844898223877 ; mask density:  0.03668477013707161 ; pred:  tensor([0.0285, 0.8229, 0.1005, 0.0065, 0.0031, 0.0257, 0.0115, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  2.774411678314209 ; mask density:  0.03623666986823082 ; pred:  tensor([0.0285, 0.8228, 0.1005, 0.0065, 0.0032, 0.0258, 0.0115, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  2.738966464996338 ; mask density:  0.03580554947257042 ; pred:  tensor([0.0285, 0.8227, 0.1004, 0.0065, 0.0032, 0.0260, 0.0116, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  2.70447039604187 ; mask density:  0.03539097681641579 ; pred:  tensor([0.0285, 0.8227, 0.1003, 0.0065, 0.0032, 0.0261, 0.0116, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  2.6708831787109375 ; mask density:  0.03499232977628708 ; pred:  tensor([0.0285, 0.8227, 0.1002, 0.0064, 0.0032, 0.0262, 0.0116, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  2.638169527053833 ; mask density:  0.034608859568834305 ; pred:  tensor([0.0285, 0.8227, 0.1000, 0.0064, 0.0032, 0.0262, 0.0116, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  9.663482904434204\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_645graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  650 67\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  28.430675506591797 ; mask density:  0.712208092212677 ; pred:  tensor([0.0216, 0.6954, 0.2287, 0.0144, 0.0030, 0.0184, 0.0166, 0.0018],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  27.650611877441406 ; mask density:  0.6921893954277039 ; pred:  tensor([0.0226, 0.7224, 0.2032, 0.0129, 0.0031, 0.0189, 0.0152, 0.0017],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  26.844017028808594 ; mask density:  0.671333372592926 ; pred:  tensor([0.0233, 0.7431, 0.1838, 0.0118, 0.0031, 0.0192, 0.0140, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  26.01117515563965 ; mask density:  0.6496933698654175 ; pred:  tensor([0.0239, 0.7597, 0.1683, 0.0110, 0.0031, 0.0195, 0.0130, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  25.15105438232422 ; mask density:  0.627324104309082 ; pred:  tensor([0.0244, 0.7737, 0.1553, 0.0102, 0.0031, 0.0196, 0.0120, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  24.27109146118164 ; mask density:  0.604295551776886 ; pred:  tensor([0.0248, 0.7826, 0.1470, 0.0098, 0.0032, 0.0198, 0.0115, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  23.368989944458008 ; mask density:  0.5807065367698669 ; pred:  tensor([0.0251, 0.7900, 0.1398, 0.0093, 0.0032, 0.0201, 0.0111, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  22.44738006591797 ; mask density:  0.5566656589508057 ; pred:  tensor([0.0254, 0.7965, 0.1331, 0.0089, 0.0032, 0.0207, 0.0108, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  21.51024627685547 ; mask density:  0.5323058366775513 ; pred:  tensor([0.0257, 0.8019, 0.1273, 0.0085, 0.0033, 0.0213, 0.0107, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  20.561861038208008 ; mask density:  0.5077545642852783 ; pred:  tensor([0.0259, 0.8062, 0.1226, 0.0082, 0.0033, 0.0219, 0.0106, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  19.606361389160156 ; mask density:  0.4831418991088867 ; pred:  tensor([0.0261, 0.8097, 0.1186, 0.0080, 0.0034, 0.0224, 0.0105, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  18.647451400756836 ; mask density:  0.45860952138900757 ; pred:  tensor([0.0263, 0.8133, 0.1147, 0.0077, 0.0034, 0.0229, 0.0103, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  17.69062042236328 ; mask density:  0.43430283665657043 ; pred:  tensor([0.0264, 0.8163, 0.1114, 0.0075, 0.0034, 0.0234, 0.0102, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  16.740577697753906 ; mask density:  0.4103642404079437 ; pred:  tensor([0.0266, 0.8193, 0.1081, 0.0073, 0.0035, 0.0238, 0.0101, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  15.804115295410156 ; mask density:  0.38694676756858826 ; pred:  tensor([0.0266, 0.8214, 0.1057, 0.0072, 0.0035, 0.0243, 0.0101, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  14.885546684265137 ; mask density:  0.3640955090522766 ; pred:  tensor([0.0268, 0.8236, 0.1031, 0.0070, 0.0035, 0.0248, 0.0100, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  13.989830017089844 ; mask density:  0.34194216132164 ; pred:  tensor([0.0270, 0.8255, 0.1007, 0.0069, 0.0036, 0.0253, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  13.121729850769043 ; mask density:  0.32059550285339355 ; pred:  tensor([0.0272, 0.8272, 0.0986, 0.0068, 0.0036, 0.0257, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  12.287287712097168 ; mask density:  0.30016034841537476 ; pred:  tensor([0.0272, 0.8268, 0.0982, 0.0068, 0.0037, 0.0264, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  11.488870620727539 ; mask density:  0.2807219326496124 ; pred:  tensor([0.0270, 0.8256, 0.0991, 0.0068, 0.0038, 0.0268, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  10.729147911071777 ; mask density:  0.26229825615882874 ; pred:  tensor([0.0269, 0.8234, 0.1004, 0.0070, 0.0039, 0.0274, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  10.00920295715332 ; mask density:  0.24492205679416656 ; pred:  tensor([0.0268, 0.8215, 0.1004, 0.0070, 0.0041, 0.0290, 0.0099, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  9.329476356506348 ; mask density:  0.22861379384994507 ; pred:  tensor([0.0269, 0.8207, 0.0994, 0.0069, 0.0043, 0.0305, 0.0099, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  8.690363883972168 ; mask density:  0.2133733332157135 ; pred:  tensor([0.0270, 0.8211, 0.0974, 0.0068, 0.0045, 0.0320, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  8.09229564666748 ; mask density:  0.19918206334114075 ; pred:  tensor([0.0271, 0.8222, 0.0948, 0.0066, 0.0046, 0.0334, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  7.535150527954102 ; mask density:  0.1860244870185852 ; pred:  tensor([0.0273, 0.8235, 0.0920, 0.0064, 0.0047, 0.0347, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  7.018050193786621 ; mask density:  0.17385514080524445 ; pred:  tensor([0.0276, 0.8250, 0.0893, 0.0062, 0.0048, 0.0359, 0.0101, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  6.539604663848877 ; mask density:  0.16263791918754578 ; pred:  tensor([0.0279, 0.8264, 0.0867, 0.0060, 0.0048, 0.0368, 0.0101, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  6.09835147857666 ; mask density:  0.15233036875724792 ; pred:  tensor([0.0283, 0.8275, 0.0845, 0.0058, 0.0048, 0.0377, 0.0101, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  5.692371845245361 ; mask density:  0.14288969337940216 ; pred:  tensor([0.0286, 0.8284, 0.0826, 0.0057, 0.0048, 0.0384, 0.0102, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  5.319640636444092 ; mask density:  0.13426430523395538 ; pred:  tensor([0.0289, 0.8290, 0.0810, 0.0056, 0.0048, 0.0391, 0.0103, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  4.977923393249512 ; mask density:  0.12639519572257996 ; pred:  tensor([0.0292, 0.8294, 0.0795, 0.0055, 0.0048, 0.0398, 0.0104, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  4.664878845214844 ; mask density:  0.1192220151424408 ; pred:  tensor([0.0295, 0.8297, 0.0784, 0.0054, 0.0048, 0.0403, 0.0105, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  4.378331184387207 ; mask density:  0.11269132047891617 ; pred:  tensor([0.0298, 0.8299, 0.0773, 0.0054, 0.0048, 0.0408, 0.0107, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  4.116141319274902 ; mask density:  0.10674791038036346 ; pred:  tensor([0.0300, 0.8300, 0.0765, 0.0053, 0.0048, 0.0413, 0.0108, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  3.876267671585083 ; mask density:  0.10133986175060272 ; pred:  tensor([0.0302, 0.8301, 0.0758, 0.0053, 0.0048, 0.0416, 0.0109, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  3.6568071842193604 ; mask density:  0.09641898423433304 ; pred:  tensor([0.0304, 0.8302, 0.0753, 0.0053, 0.0048, 0.0418, 0.0109, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  3.455965280532837 ; mask density:  0.09194508194923401 ; pred:  tensor([0.0305, 0.8302, 0.0750, 0.0052, 0.0048, 0.0420, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  3.272068738937378 ; mask density:  0.0878768116235733 ; pred:  tensor([0.0306, 0.8303, 0.0747, 0.0052, 0.0048, 0.0421, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  3.103595495223999 ; mask density:  0.08417650312185287 ; pred:  tensor([0.0306, 0.8303, 0.0746, 0.0052, 0.0048, 0.0421, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  2.9491360187530518 ; mask density:  0.08080998808145523 ; pred:  tensor([0.0306, 0.8304, 0.0746, 0.0052, 0.0048, 0.0421, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  2.807399034500122 ; mask density:  0.07774622738361359 ; pred:  tensor([0.0306, 0.8305, 0.0746, 0.0052, 0.0047, 0.0419, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  2.6772005558013916 ; mask density:  0.07495684176683426 ; pred:  tensor([0.0306, 0.8307, 0.0747, 0.0052, 0.0047, 0.0418, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  2.557474136352539 ; mask density:  0.0724165067076683 ; pred:  tensor([0.0306, 0.8308, 0.0749, 0.0052, 0.0047, 0.0415, 0.0109, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  2.4472482204437256 ; mask density:  0.0701022744178772 ; pred:  tensor([0.0305, 0.8310, 0.0752, 0.0052, 0.0047, 0.0412, 0.0109, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  2.3456404209136963 ; mask density:  0.06799312680959702 ; pred:  tensor([0.0304, 0.8312, 0.0755, 0.0052, 0.0046, 0.0408, 0.0108, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  2.251857042312622 ; mask density:  0.06606989353895187 ; pred:  tensor([0.0303, 0.8315, 0.0758, 0.0052, 0.0046, 0.0404, 0.0108, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  2.165132999420166 ; mask density:  0.0643162876367569 ; pred:  tensor([0.0302, 0.8318, 0.0762, 0.0052, 0.0046, 0.0399, 0.0107, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  2.0848166942596436 ; mask density:  0.06271574646234512 ; pred:  tensor([0.0301, 0.8323, 0.0766, 0.0053, 0.0045, 0.0393, 0.0106, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  2.010361433029175 ; mask density:  0.061252422630786896 ; pred:  tensor([0.0300, 0.8328, 0.0771, 0.0053, 0.0045, 0.0386, 0.0105, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  1.9412554502487183 ; mask density:  0.05991167947649956 ; pred:  tensor([0.0299, 0.8334, 0.0776, 0.0053, 0.0044, 0.0378, 0.0104, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  1.877020001411438 ; mask density:  0.05867965891957283 ; pred:  tensor([0.0298, 0.8340, 0.0780, 0.0053, 0.0043, 0.0370, 0.0103, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  1.8172600269317627 ; mask density:  0.05753929540514946 ; pred:  tensor([0.0297, 0.8346, 0.0785, 0.0053, 0.0043, 0.0362, 0.0102, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  1.7615816593170166 ; mask density:  0.05647890642285347 ; pred:  tensor([0.0296, 0.8352, 0.0789, 0.0053, 0.0042, 0.0354, 0.0101, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  1.7096455097198486 ; mask density:  0.055487751960754395 ; pred:  tensor([0.0295, 0.8358, 0.0794, 0.0053, 0.0041, 0.0347, 0.0100, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  1.6611182689666748 ; mask density:  0.05455615371465683 ; pred:  tensor([0.0294, 0.8363, 0.0798, 0.0054, 0.0041, 0.0340, 0.0100, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  1.6156795024871826 ; mask density:  0.053675610572099686 ; pred:  tensor([0.0293, 0.8367, 0.0802, 0.0054, 0.0040, 0.0333, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  1.573108196258545 ; mask density:  0.052828945219516754 ; pred:  tensor([0.0292, 0.8371, 0.0806, 0.0054, 0.0040, 0.0327, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  1.533156394958496 ; mask density:  0.05201076343655586 ; pred:  tensor([0.0292, 0.8374, 0.0810, 0.0054, 0.0039, 0.0322, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  1.4955828189849854 ; mask density:  0.05121738836169243 ; pred:  tensor([0.0291, 0.8377, 0.0813, 0.0054, 0.0039, 0.0318, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  1.4601871967315674 ; mask density:  0.05044608190655708 ; pred:  tensor([0.0291, 0.8379, 0.0815, 0.0054, 0.0038, 0.0314, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  1.4267910718917847 ; mask density:  0.04969510808587074 ; pred:  tensor([0.0291, 0.8381, 0.0816, 0.0054, 0.0038, 0.0312, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  1.3952362537384033 ; mask density:  0.04896354675292969 ; pred:  tensor([0.0291, 0.8383, 0.0816, 0.0054, 0.0037, 0.0310, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  1.3654015064239502 ; mask density:  0.04825090616941452 ; pred:  tensor([0.0292, 0.8385, 0.0816, 0.0054, 0.0037, 0.0308, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  1.3373868465423584 ; mask density:  0.04758265241980553 ; pred:  tensor([0.0292, 0.8384, 0.0817, 0.0053, 0.0037, 0.0308, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  1.3107893466949463 ; mask density:  0.046955399215221405 ; pred:  tensor([0.0293, 0.8386, 0.0816, 0.0053, 0.0037, 0.0307, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  1.2854721546173096 ; mask density:  0.04636557027697563 ; pred:  tensor([0.0293, 0.8392, 0.0813, 0.0053, 0.0036, 0.0304, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  1.261370062828064 ; mask density:  0.04580935090780258 ; pred:  tensor([0.0294, 0.8399, 0.0810, 0.0053, 0.0036, 0.0300, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  1.2384276390075684 ; mask density:  0.04528290778398514 ; pred:  tensor([0.0295, 0.8407, 0.0806, 0.0053, 0.0035, 0.0296, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  1.2166191339492798 ; mask density:  0.04476587846875191 ; pred:  tensor([0.0296, 0.8416, 0.0802, 0.0052, 0.0035, 0.0291, 0.0096, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  1.195949673652649 ; mask density:  0.04425728693604469 ; pred:  tensor([0.0297, 0.8422, 0.0799, 0.0052, 0.0034, 0.0289, 0.0096, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  1.176171898841858 ; mask density:  0.04375690594315529 ; pred:  tensor([0.0297, 0.8425, 0.0797, 0.0052, 0.0034, 0.0287, 0.0096, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  1.157208800315857 ; mask density:  0.043265145272016525 ; pred:  tensor([0.0298, 0.8427, 0.0795, 0.0052, 0.0034, 0.0287, 0.0096, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  1.1390891075134277 ; mask density:  0.0428006686270237 ; pred:  tensor([0.0299, 0.8426, 0.0794, 0.0052, 0.0034, 0.0288, 0.0096, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  1.121780514717102 ; mask density:  0.04236256703734398 ; pred:  tensor([0.0299, 0.8426, 0.0793, 0.0051, 0.0034, 0.0289, 0.0097, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  1.105170726776123 ; mask density:  0.04194970428943634 ; pred:  tensor([0.0300, 0.8426, 0.0792, 0.0051, 0.0034, 0.0289, 0.0097, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  1.089215874671936 ; mask density:  0.04156074672937393 ; pred:  tensor([0.0300, 0.8426, 0.0790, 0.0051, 0.0034, 0.0289, 0.0097, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  1.073874592781067 ; mask density:  0.04119425266981125 ; pred:  tensor([0.0301, 0.8427, 0.0789, 0.0051, 0.0034, 0.0289, 0.0097, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  1.0591102838516235 ; mask density:  0.04084867611527443 ; pred:  tensor([0.0301, 0.8429, 0.0787, 0.0051, 0.0033, 0.0289, 0.0098, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  1.0448893308639526 ; mask density:  0.04052244871854782 ; pred:  tensor([0.0302, 0.8431, 0.0786, 0.0051, 0.0033, 0.0288, 0.0098, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  1.031180500984192 ; mask density:  0.04021402448415756 ; pred:  tensor([0.0302, 0.8433, 0.0784, 0.0051, 0.0033, 0.0288, 0.0098, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  1.0179550647735596 ; mask density:  0.03992189094424248 ; pred:  tensor([0.0302, 0.8435, 0.0783, 0.0051, 0.0033, 0.0287, 0.0098, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  1.0051884651184082 ; mask density:  0.03963251784443855 ; pred:  tensor([0.0302, 0.8437, 0.0782, 0.0051, 0.0033, 0.0286, 0.0097, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  0.9928489923477173 ; mask density:  0.03932857885956764 ; pred:  tensor([0.0302, 0.8437, 0.0781, 0.0051, 0.0033, 0.0287, 0.0097, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  0.980922520160675 ; mask density:  0.03904346376657486 ; pred:  tensor([0.0302, 0.8433, 0.0782, 0.0051, 0.0033, 0.0289, 0.0098, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  0.9693912267684937 ; mask density:  0.038777101784944534 ; pred:  tensor([0.0302, 0.8430, 0.0782, 0.0051, 0.0034, 0.0291, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  0.9582260847091675 ; mask density:  0.03852907568216324 ; pred:  tensor([0.0302, 0.8428, 0.0783, 0.0051, 0.0034, 0.0293, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  0.9474040269851685 ; mask density:  0.03829870745539665 ; pred:  tensor([0.0302, 0.8426, 0.0783, 0.0051, 0.0034, 0.0294, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  0.9369021058082581 ; mask density:  0.03807901591062546 ; pred:  tensor([0.0302, 0.8426, 0.0783, 0.0051, 0.0034, 0.0295, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  0.9267009496688843 ; mask density:  0.03786968067288399 ; pred:  tensor([0.0302, 0.8426, 0.0783, 0.0051, 0.0034, 0.0295, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  0.916793704032898 ; mask density:  0.037669822573661804 ; pred:  tensor([0.0302, 0.8426, 0.0782, 0.0051, 0.0034, 0.0295, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  0.9071688652038574 ; mask density:  0.03747875615954399 ; pred:  tensor([0.0302, 0.8427, 0.0782, 0.0051, 0.0034, 0.0294, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  0.8978129625320435 ; mask density:  0.03729568421840668 ; pred:  tensor([0.0302, 0.8429, 0.0781, 0.0051, 0.0034, 0.0294, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  0.8888092041015625 ; mask density:  0.03710026666522026 ; pred:  tensor([0.0302, 0.8430, 0.0781, 0.0051, 0.0034, 0.0293, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  0.8799762725830078 ; mask density:  0.03689466789364815 ; pred:  tensor([0.0302, 0.8429, 0.0781, 0.0051, 0.0034, 0.0294, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  0.8713100552558899 ; mask density:  0.03670111671090126 ; pred:  tensor([0.0302, 0.8427, 0.0781, 0.0051, 0.0034, 0.0295, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  0.862960159778595 ; mask density:  0.036519818007946014 ; pred:  tensor([0.0302, 0.8425, 0.0782, 0.0051, 0.0034, 0.0297, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  0.8548154830932617 ; mask density:  0.036351528018713 ; pred:  tensor([0.0302, 0.8423, 0.0782, 0.0051, 0.0034, 0.0298, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  0.8468619585037231 ; mask density:  0.036195579916238785 ; pred:  tensor([0.0302, 0.8423, 0.0782, 0.0051, 0.0034, 0.0298, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  0.8390944600105286 ; mask density:  0.03605097159743309 ; pred:  tensor([0.0302, 0.8424, 0.0781, 0.0051, 0.0034, 0.0297, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  9.418246030807495\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_650graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  655 109\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  56.7255859375 ; mask density:  0.7104787230491638 ; pred:  tensor([0.0205, 0.6933, 0.2314, 0.0137, 0.0029, 0.0186, 0.0177, 0.0019],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  55.16592788696289 ; mask density:  0.690104603767395 ; pred:  tensor([0.0214, 0.7207, 0.2068, 0.0125, 0.0029, 0.0185, 0.0156, 0.0017],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  53.54462814331055 ; mask density:  0.6688793897628784 ; pred:  tensor([0.0222, 0.7446, 0.1854, 0.0114, 0.0029, 0.0182, 0.0138, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  51.863067626953125 ; mask density:  0.6468616724014282 ; pred:  tensor([0.0229, 0.7662, 0.1660, 0.0104, 0.0028, 0.0180, 0.0122, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  50.12676239013672 ; mask density:  0.6241225600242615 ; pred:  tensor([0.0236, 0.7838, 0.1501, 0.0095, 0.0028, 0.0179, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  48.343814849853516 ; mask density:  0.6007430553436279 ; pred:  tensor([0.0241, 0.7951, 0.1396, 0.0089, 0.0028, 0.0179, 0.0103, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  46.513710021972656 ; mask density:  0.5768136978149414 ; pred:  tensor([0.0245, 0.8049, 0.1304, 0.0084, 0.0028, 0.0180, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  44.643497467041016 ; mask density:  0.5524389147758484 ; pred:  tensor([0.0249, 0.8130, 0.1224, 0.0079, 0.0028, 0.0183, 0.0094, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  42.74089050292969 ; mask density:  0.527739942073822 ; pred:  tensor([0.0254, 0.8196, 0.1156, 0.0075, 0.0028, 0.0188, 0.0092, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  40.815059661865234 ; mask density:  0.5028430819511414 ; pred:  tensor([0.0258, 0.8247, 0.1100, 0.0071, 0.0028, 0.0194, 0.0090, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  38.87593460083008 ; mask density:  0.47789281606674194 ; pred:  tensor([0.0261, 0.8281, 0.1056, 0.0069, 0.0029, 0.0204, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  36.93402099609375 ; mask density:  0.45303499698638916 ; pred:  tensor([0.0264, 0.8300, 0.1026, 0.0067, 0.0031, 0.0215, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  34.99976348876953 ; mask density:  0.4284156858921051 ; pred:  tensor([0.0267, 0.8308, 0.1003, 0.0065, 0.0032, 0.0227, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  33.08467483520508 ; mask density:  0.40419280529022217 ; pred:  tensor([0.0269, 0.8303, 0.0987, 0.0065, 0.0033, 0.0245, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  31.200088500976562 ; mask density:  0.3805195391178131 ; pred:  tensor([0.0269, 0.8285, 0.0986, 0.0065, 0.0035, 0.0261, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  29.35504722595215 ; mask density:  0.3575262129306793 ; pred:  tensor([0.0268, 0.8269, 0.0987, 0.0065, 0.0037, 0.0274, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  27.558534622192383 ; mask density:  0.33532434701919556 ; pred:  tensor([0.0268, 0.8261, 0.0983, 0.0064, 0.0038, 0.0284, 0.0090, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  25.819602966308594 ; mask density:  0.3140159845352173 ; pred:  tensor([0.0268, 0.8257, 0.0976, 0.0064, 0.0039, 0.0294, 0.0090, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  24.146028518676758 ; mask density:  0.29368191957473755 ; pred:  tensor([0.0268, 0.8258, 0.0966, 0.0064, 0.0039, 0.0301, 0.0091, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  22.54423713684082 ; mask density:  0.2743432819843292 ; pred:  tensor([0.0269, 0.8266, 0.0952, 0.0063, 0.0040, 0.0307, 0.0091, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  21.01951026916504 ; mask density:  0.25604480504989624 ; pred:  tensor([0.0271, 0.8277, 0.0935, 0.0062, 0.0040, 0.0312, 0.0091, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  19.575822830200195 ; mask density:  0.23881593346595764 ; pred:  tensor([0.0272, 0.8290, 0.0919, 0.0061, 0.0040, 0.0315, 0.0091, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  18.215559005737305 ; mask density:  0.22266440093517303 ; pred:  tensor([0.0274, 0.8305, 0.0903, 0.0060, 0.0041, 0.0316, 0.0090, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  16.939741134643555 ; mask density:  0.2075778841972351 ; pred:  tensor([0.0275, 0.8320, 0.0887, 0.0060, 0.0041, 0.0316, 0.0090, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  15.748138427734375 ; mask density:  0.19353652000427246 ; pred:  tensor([0.0276, 0.8335, 0.0871, 0.0059, 0.0041, 0.0316, 0.0089, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  14.639488220214844 ; mask density:  0.18050740659236908 ; pred:  tensor([0.0278, 0.8350, 0.0857, 0.0058, 0.0041, 0.0316, 0.0089, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  13.611368179321289 ; mask density:  0.16844086349010468 ; pred:  tensor([0.0279, 0.8366, 0.0841, 0.0057, 0.0041, 0.0315, 0.0088, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  12.660859107971191 ; mask density:  0.1572946012020111 ; pred:  tensor([0.0281, 0.8381, 0.0826, 0.0057, 0.0041, 0.0315, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  11.784466743469238 ; mask density:  0.14701353013515472 ; pred:  tensor([0.0282, 0.8396, 0.0811, 0.0056, 0.0042, 0.0315, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  10.978236198425293 ; mask density:  0.13754801452159882 ; pred:  tensor([0.0285, 0.8408, 0.0797, 0.0055, 0.0042, 0.0316, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  10.238069534301758 ; mask density:  0.12884876132011414 ; pred:  tensor([0.0287, 0.8417, 0.0784, 0.0054, 0.0042, 0.0318, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  9.55944538116455 ; mask density:  0.12086457014083862 ; pred:  tensor([0.0289, 0.8422, 0.0774, 0.0054, 0.0043, 0.0321, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  8.93801498413086 ; mask density:  0.11354735493659973 ; pred:  tensor([0.0290, 0.8425, 0.0766, 0.0053, 0.0043, 0.0325, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  8.369439125061035 ; mask density:  0.10684774816036224 ; pred:  tensor([0.0292, 0.8425, 0.0760, 0.0053, 0.0044, 0.0329, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  7.849442481994629 ; mask density:  0.1007179543375969 ; pred:  tensor([0.0293, 0.8423, 0.0755, 0.0053, 0.0044, 0.0334, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  7.374134540557861 ; mask density:  0.0951153114438057 ; pred:  tensor([0.0293, 0.8419, 0.0755, 0.0053, 0.0045, 0.0338, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  6.939461708068848 ; mask density:  0.08999744802713394 ; pred:  tensor([0.0294, 0.8413, 0.0754, 0.0053, 0.0045, 0.0343, 0.0086, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  6.541945934295654 ; mask density:  0.0853242501616478 ; pred:  tensor([0.0294, 0.8407, 0.0755, 0.0052, 0.0045, 0.0348, 0.0087, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  6.178250312805176 ; mask density:  0.08105814456939697 ; pred:  tensor([0.0294, 0.8401, 0.0756, 0.0052, 0.0046, 0.0352, 0.0087, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  5.845273017883301 ; mask density:  0.07716303318738937 ; pred:  tensor([0.0294, 0.8394, 0.0758, 0.0052, 0.0046, 0.0357, 0.0087, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  5.540222644805908 ; mask density:  0.07360943406820297 ; pred:  tensor([0.0294, 0.8386, 0.0760, 0.0053, 0.0046, 0.0361, 0.0088, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  5.260463237762451 ; mask density:  0.07036642730236053 ; pred:  tensor([0.0294, 0.8379, 0.0763, 0.0053, 0.0047, 0.0365, 0.0088, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  5.0036492347717285 ; mask density:  0.0674065425992012 ; pred:  tensor([0.0294, 0.8373, 0.0766, 0.0053, 0.0047, 0.0368, 0.0088, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  4.76784086227417 ; mask density:  0.06470315158367157 ; pred:  tensor([0.0294, 0.8366, 0.0770, 0.0053, 0.0047, 0.0371, 0.0089, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  4.550873756408691 ; mask density:  0.06223189830780029 ; pred:  tensor([0.0293, 0.8361, 0.0773, 0.0053, 0.0047, 0.0372, 0.0089, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  4.35100793838501 ; mask density:  0.059970926493406296 ; pred:  tensor([0.0293, 0.8357, 0.0776, 0.0053, 0.0047, 0.0373, 0.0089, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  4.166622161865234 ; mask density:  0.057899631559848785 ; pred:  tensor([0.0293, 0.8355, 0.0779, 0.0053, 0.0047, 0.0372, 0.0089, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  3.9962778091430664 ; mask density:  0.05599920079112053 ; pred:  tensor([0.0292, 0.8355, 0.0782, 0.0053, 0.0047, 0.0370, 0.0089, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  3.8386824131011963 ; mask density:  0.05425243079662323 ; pred:  tensor([0.0292, 0.8357, 0.0784, 0.0053, 0.0046, 0.0368, 0.0089, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  3.692667007446289 ; mask density:  0.0526433140039444 ; pred:  tensor([0.0291, 0.8360, 0.0785, 0.0053, 0.0046, 0.0364, 0.0088, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  3.5571889877319336 ; mask density:  0.05115766078233719 ; pred:  tensor([0.0291, 0.8365, 0.0787, 0.0053, 0.0046, 0.0359, 0.0088, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  3.4313106536865234 ; mask density:  0.04978259280323982 ; pred:  tensor([0.0291, 0.8370, 0.0788, 0.0053, 0.0045, 0.0354, 0.0087, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  3.3141822814941406 ; mask density:  0.048506561666727066 ; pred:  tensor([0.0290, 0.8376, 0.0788, 0.0053, 0.0045, 0.0349, 0.0087, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  3.2050368785858154 ; mask density:  0.04731918126344681 ; pred:  tensor([0.0290, 0.8382, 0.0789, 0.0053, 0.0044, 0.0344, 0.0086, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  3.1031832695007324 ; mask density:  0.046211279928684235 ; pred:  tensor([0.0290, 0.8389, 0.0789, 0.0053, 0.0044, 0.0338, 0.0086, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  3.0080060958862305 ; mask density:  0.045174770057201385 ; pred:  tensor([0.0290, 0.8395, 0.0789, 0.0053, 0.0043, 0.0333, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  2.9189260005950928 ; mask density:  0.0442025363445282 ; pred:  tensor([0.0290, 0.8401, 0.0789, 0.0053, 0.0043, 0.0328, 0.0084, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  2.835414409637451 ; mask density:  0.04328877851366997 ; pred:  tensor([0.0289, 0.8408, 0.0789, 0.0053, 0.0043, 0.0323, 0.0084, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  2.7570035457611084 ; mask density:  0.042428042739629745 ; pred:  tensor([0.0289, 0.8414, 0.0789, 0.0053, 0.0042, 0.0318, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  2.683285713195801 ; mask density:  0.04161557927727699 ; pred:  tensor([0.0289, 0.8419, 0.0789, 0.0053, 0.0042, 0.0314, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  2.6138575077056885 ; mask density:  0.040847618132829666 ; pred:  tensor([0.0289, 0.8425, 0.0789, 0.0053, 0.0042, 0.0309, 0.0082, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  2.5483856201171875 ; mask density:  0.040120530873537064 ; pred:  tensor([0.0289, 0.8430, 0.0788, 0.0053, 0.0041, 0.0305, 0.0082, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  2.4865598678588867 ; mask density:  0.039431117475032806 ; pred:  tensor([0.0289, 0.8434, 0.0788, 0.0053, 0.0041, 0.0302, 0.0081, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  2.428096055984497 ; mask density:  0.038776546716690063 ; pred:  tensor([0.0289, 0.8438, 0.0789, 0.0053, 0.0041, 0.0298, 0.0081, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  2.372734785079956 ; mask density:  0.038154296576976776 ; pred:  tensor([0.0289, 0.8442, 0.0789, 0.0053, 0.0041, 0.0295, 0.0081, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  2.3202857971191406 ; mask density:  0.03756047785282135 ; pred:  tensor([0.0288, 0.8445, 0.0790, 0.0053, 0.0040, 0.0292, 0.0080, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  2.2704479694366455 ; mask density:  0.03699321672320366 ; pred:  tensor([0.0288, 0.8446, 0.0792, 0.0054, 0.0040, 0.0290, 0.0080, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  2.2230231761932373 ; mask density:  0.0364508219063282 ; pred:  tensor([0.0288, 0.8445, 0.0795, 0.0054, 0.0040, 0.0288, 0.0080, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  2.1779110431671143 ; mask density:  0.0359334871172905 ; pred:  tensor([0.0287, 0.8443, 0.0799, 0.0054, 0.0040, 0.0287, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  2.1349432468414307 ; mask density:  0.035439517349004745 ; pred:  tensor([0.0287, 0.8441, 0.0803, 0.0054, 0.0040, 0.0286, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  2.093961238861084 ; mask density:  0.03496712073683739 ; pred:  tensor([0.0286, 0.8439, 0.0806, 0.0054, 0.0040, 0.0285, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  2.054828643798828 ; mask density:  0.034515030682086945 ; pred:  tensor([0.0286, 0.8437, 0.0810, 0.0054, 0.0040, 0.0284, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  2.0174150466918945 ; mask density:  0.03408204764127731 ; pred:  tensor([0.0285, 0.8435, 0.0813, 0.0055, 0.0040, 0.0283, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  1.9816051721572876 ; mask density:  0.033667054027318954 ; pred:  tensor([0.0285, 0.8434, 0.0816, 0.0055, 0.0040, 0.0281, 0.0078, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  1.9472932815551758 ; mask density:  0.03326898440718651 ; pred:  tensor([0.0284, 0.8433, 0.0819, 0.0055, 0.0040, 0.0280, 0.0078, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  1.9143826961517334 ; mask density:  0.03288687393069267 ; pred:  tensor([0.0284, 0.8432, 0.0821, 0.0055, 0.0040, 0.0279, 0.0078, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  1.8827848434448242 ; mask density:  0.032519761472940445 ; pred:  tensor([0.0284, 0.8432, 0.0823, 0.0055, 0.0040, 0.0278, 0.0078, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  1.8524168729782104 ; mask density:  0.0321667455136776 ; pred:  tensor([0.0283, 0.8433, 0.0824, 0.0055, 0.0040, 0.0276, 0.0078, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  1.8232042789459229 ; mask density:  0.03182697668671608 ; pred:  tensor([0.0283, 0.8434, 0.0825, 0.0055, 0.0040, 0.0275, 0.0077, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  1.7950782775878906 ; mask density:  0.03149966895580292 ; pred:  tensor([0.0283, 0.8435, 0.0826, 0.0055, 0.0039, 0.0273, 0.0077, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  1.7679755687713623 ; mask density:  0.031184062361717224 ; pred:  tensor([0.0283, 0.8436, 0.0826, 0.0055, 0.0039, 0.0272, 0.0077, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  1.741837978363037 ; mask density:  0.030879467725753784 ; pred:  tensor([0.0283, 0.8438, 0.0827, 0.0055, 0.0039, 0.0270, 0.0077, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  1.7166111469268799 ; mask density:  0.03058522194623947 ; pred:  tensor([0.0283, 0.8440, 0.0827, 0.0055, 0.0039, 0.0269, 0.0076, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  1.692245364189148 ; mask density:  0.030300719663500786 ; pred:  tensor([0.0283, 0.8442, 0.0827, 0.0055, 0.0039, 0.0267, 0.0076, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  1.6686935424804688 ; mask density:  0.03002539649605751 ; pred:  tensor([0.0283, 0.8444, 0.0827, 0.0055, 0.0039, 0.0266, 0.0076, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  1.6459124088287354 ; mask density:  0.029758727177977562 ; pred:  tensor([0.0283, 0.8446, 0.0826, 0.0055, 0.0039, 0.0264, 0.0076, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  1.623884677886963 ; mask density:  0.02949868142604828 ; pred:  tensor([0.0283, 0.8447, 0.0826, 0.0055, 0.0039, 0.0263, 0.0076, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  1.602522373199463 ; mask density:  0.02924502082169056 ; pred:  tensor([0.0283, 0.8447, 0.0828, 0.0056, 0.0039, 0.0262, 0.0075, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  1.5818017721176147 ; mask density:  0.02899903990328312 ; pred:  tensor([0.0282, 0.8445, 0.0830, 0.0056, 0.0039, 0.0262, 0.0075, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  1.5617256164550781 ; mask density:  0.028760358691215515 ; pred:  tensor([0.0282, 0.8444, 0.0832, 0.0056, 0.0039, 0.0262, 0.0075, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  1.5422426462173462 ; mask density:  0.02852860651910305 ; pred:  tensor([0.0282, 0.8442, 0.0834, 0.0056, 0.0039, 0.0262, 0.0075, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  1.5233211517333984 ; mask density:  0.02830370143055916 ; pred:  tensor([0.0282, 0.8441, 0.0835, 0.0056, 0.0039, 0.0261, 0.0075, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  1.5049355030059814 ; mask density:  0.028085291385650635 ; pred:  tensor([0.0282, 0.8441, 0.0836, 0.0056, 0.0039, 0.0261, 0.0075, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  1.4870609045028687 ; mask density:  0.027873056009411812 ; pred:  tensor([0.0281, 0.8441, 0.0837, 0.0056, 0.0039, 0.0260, 0.0075, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  1.4696743488311768 ; mask density:  0.027666689828038216 ; pred:  tensor([0.0281, 0.8441, 0.0837, 0.0056, 0.0039, 0.0260, 0.0075, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  1.4527547359466553 ; mask density:  0.027465905994176865 ; pred:  tensor([0.0281, 0.8441, 0.0837, 0.0056, 0.0039, 0.0259, 0.0075, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  1.4362815618515015 ; mask density:  0.02727043442428112 ; pred:  tensor([0.0281, 0.8442, 0.0837, 0.0056, 0.0039, 0.0259, 0.0075, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  1.420236587524414 ; mask density:  0.027080027386546135 ; pred:  tensor([0.0281, 0.8443, 0.0837, 0.0056, 0.0039, 0.0258, 0.0075, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  1.4046021699905396 ; mask density:  0.026894448325037956 ; pred:  tensor([0.0281, 0.8444, 0.0837, 0.0056, 0.0039, 0.0257, 0.0075, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  1.3893616199493408 ; mask density:  0.026713473722338676 ; pred:  tensor([0.0281, 0.8446, 0.0836, 0.0056, 0.0039, 0.0257, 0.0074, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  9.361642599105835\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_655graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  660 85\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  273.6053161621094 ; mask density:  0.7104463577270508 ; pred:  tensor([0.0425, 0.7921, 0.0642, 0.0050, 0.0043, 0.0704, 0.0198, 0.0017],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  266.0960693359375 ; mask density:  0.689803957939148 ; pred:  tensor([0.0411, 0.8148, 0.0628, 0.0048, 0.0037, 0.0548, 0.0166, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  258.264404296875 ; mask density:  0.6683353185653687 ; pred:  tensor([0.0398, 0.8318, 0.0615, 0.0046, 0.0033, 0.0437, 0.0140, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  250.12400817871094 ; mask density:  0.6460584402084351 ; pred:  tensor([0.0380, 0.8438, 0.0615, 0.0046, 0.0030, 0.0360, 0.0120, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  241.6927490234375 ; mask density:  0.623052716255188 ; pred:  tensor([0.0363, 0.8515, 0.0620, 0.0045, 0.0030, 0.0313, 0.0103, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  232.9942626953125 ; mask density:  0.5994014739990234 ; pred:  tensor([0.0347, 0.8567, 0.0630, 0.0046, 0.0030, 0.0281, 0.0090, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  224.0590057373047 ; mask density:  0.5752033591270447 ; pred:  tensor([0.0334, 0.8598, 0.0644, 0.0046, 0.0030, 0.0257, 0.0081, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  214.92205810546875 ; mask density:  0.550570011138916 ; pred:  tensor([0.0324, 0.8614, 0.0662, 0.0048, 0.0031, 0.0238, 0.0074, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  205.62423706054688 ; mask density:  0.5256252884864807 ; pred:  tensor([0.0317, 0.8620, 0.0680, 0.0049, 0.0032, 0.0224, 0.0069, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  196.2108917236328 ; mask density:  0.5005025267601013 ; pred:  tensor([0.0311, 0.8617, 0.0701, 0.0051, 0.0033, 0.0212, 0.0065, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  186.73109436035156 ; mask density:  0.4753415584564209 ; pred:  tensor([0.0306, 0.8611, 0.0721, 0.0052, 0.0033, 0.0204, 0.0063, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  177.23728942871094 ; mask density:  0.4502868354320526 ; pred:  tensor([0.0302, 0.8602, 0.0741, 0.0054, 0.0034, 0.0197, 0.0061, 0.0009],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  167.78309631347656 ; mask density:  0.4254821538925171 ; pred:  tensor([0.0299, 0.8594, 0.0754, 0.0055, 0.0035, 0.0194, 0.0060, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  158.4228973388672 ; mask density:  0.4010690748691559 ; pred:  tensor([0.0299, 0.8593, 0.0755, 0.0055, 0.0036, 0.0194, 0.0059, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  149.21131896972656 ; mask density:  0.37718281149864197 ; pred:  tensor([0.0299, 0.8593, 0.0752, 0.0055, 0.0038, 0.0196, 0.0058, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  140.20016479492188 ; mask density:  0.3539538085460663 ; pred:  tensor([0.0300, 0.8594, 0.0746, 0.0055, 0.0039, 0.0199, 0.0057, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  131.43824768066406 ; mask density:  0.33149731159210205 ; pred:  tensor([0.0301, 0.8593, 0.0741, 0.0055, 0.0040, 0.0203, 0.0057, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  122.96900939941406 ; mask density:  0.3099149167537689 ; pred:  tensor([0.0301, 0.8589, 0.0738, 0.0055, 0.0042, 0.0208, 0.0057, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  114.83018493652344 ; mask density:  0.2892909348011017 ; pred:  tensor([0.0300, 0.8583, 0.0738, 0.0055, 0.0043, 0.0213, 0.0058, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  107.05307006835938 ; mask density:  0.2696894705295563 ; pred:  tensor([0.0298, 0.8574, 0.0743, 0.0055, 0.0043, 0.0218, 0.0059, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  99.66179656982422 ; mask density:  0.25115710496902466 ; pred:  tensor([0.0296, 0.8561, 0.0751, 0.0055, 0.0044, 0.0222, 0.0059, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  92.67330169677734 ; mask density:  0.23371876776218414 ; pred:  tensor([0.0295, 0.8550, 0.0759, 0.0056, 0.0045, 0.0226, 0.0060, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  86.09801483154297 ; mask density:  0.2173846811056137 ; pred:  tensor([0.0292, 0.8535, 0.0770, 0.0057, 0.0046, 0.0229, 0.0060, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  79.93922424316406 ; mask density:  0.20214737951755524 ; pred:  tensor([0.0290, 0.8520, 0.0783, 0.0057, 0.0046, 0.0231, 0.0061, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  74.19437408447266 ; mask density:  0.1879863739013672 ; pred:  tensor([0.0287, 0.8506, 0.0797, 0.0058, 0.0046, 0.0233, 0.0062, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  68.855712890625 ; mask density:  0.17486609518527985 ; pred:  tensor([0.0285, 0.8494, 0.0808, 0.0059, 0.0046, 0.0235, 0.0063, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  63.910884857177734 ; mask density:  0.16274242103099823 ; pred:  tensor([0.0283, 0.8485, 0.0814, 0.0059, 0.0046, 0.0238, 0.0064, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  59.344478607177734 ; mask density:  0.15156660974025726 ; pred:  tensor([0.0283, 0.8479, 0.0816, 0.0059, 0.0045, 0.0242, 0.0066, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  55.13859939575195 ; mask density:  0.1412879079580307 ; pred:  tensor([0.0283, 0.8472, 0.0818, 0.0058, 0.0044, 0.0246, 0.0068, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  51.273338317871094 ; mask density:  0.13185034692287445 ; pred:  tensor([0.0283, 0.8463, 0.0822, 0.0058, 0.0044, 0.0250, 0.0069, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  47.727073669433594 ; mask density:  0.12319713085889816 ; pred:  tensor([0.0283, 0.8457, 0.0822, 0.0058, 0.0043, 0.0255, 0.0071, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  44.4782600402832 ; mask density:  0.11527203023433685 ; pred:  tensor([0.0284, 0.8451, 0.0822, 0.0058, 0.0043, 0.0259, 0.0072, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  41.505218505859375 ; mask density:  0.10801930725574493 ; pred:  tensor([0.0284, 0.8446, 0.0824, 0.0058, 0.0042, 0.0262, 0.0074, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  38.786537170410156 ; mask density:  0.1013844758272171 ; pred:  tensor([0.0284, 0.8442, 0.0826, 0.0057, 0.0042, 0.0263, 0.0075, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  36.301612854003906 ; mask density:  0.09531577676534653 ; pred:  tensor([0.0284, 0.8439, 0.0828, 0.0057, 0.0041, 0.0265, 0.0076, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  34.03070068359375 ; mask density:  0.08976378291845322 ; pred:  tensor([0.0283, 0.8437, 0.0829, 0.0057, 0.0041, 0.0265, 0.0076, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  31.955368041992188 ; mask density:  0.08468346297740936 ; pred:  tensor([0.0283, 0.8437, 0.0830, 0.0057, 0.0040, 0.0266, 0.0077, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  30.05848503112793 ; mask density:  0.08003272116184235 ; pred:  tensor([0.0283, 0.8437, 0.0831, 0.0056, 0.0040, 0.0265, 0.0077, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  28.32367706298828 ; mask density:  0.0757710412144661 ; pred:  tensor([0.0283, 0.8440, 0.0828, 0.0056, 0.0039, 0.0265, 0.0078, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  26.73609733581543 ; mask density:  0.07186487317085266 ; pred:  tensor([0.0284, 0.8446, 0.0823, 0.0055, 0.0039, 0.0264, 0.0078, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  25.282358169555664 ; mask density:  0.06828193366527557 ; pred:  tensor([0.0284, 0.8450, 0.0819, 0.0055, 0.0038, 0.0263, 0.0079, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  23.949783325195312 ; mask density:  0.06499231606721878 ; pred:  tensor([0.0285, 0.8453, 0.0816, 0.0055, 0.0038, 0.0263, 0.0080, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  22.72698211669922 ; mask density:  0.0619688518345356 ; pred:  tensor([0.0285, 0.8454, 0.0815, 0.0054, 0.0037, 0.0263, 0.0080, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  21.60361671447754 ; mask density:  0.05918685719370842 ; pred:  tensor([0.0285, 0.8453, 0.0815, 0.0054, 0.0037, 0.0263, 0.0081, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  20.57038688659668 ; mask density:  0.056623540818691254 ; pred:  tensor([0.0285, 0.8450, 0.0817, 0.0054, 0.0037, 0.0264, 0.0082, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  19.618881225585938 ; mask density:  0.05425528064370155 ; pred:  tensor([0.0285, 0.8448, 0.0818, 0.0054, 0.0036, 0.0265, 0.0082, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  18.7413272857666 ; mask density:  0.052069514989852905 ; pred:  tensor([0.0286, 0.8446, 0.0817, 0.0054, 0.0036, 0.0267, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  17.931007385253906 ; mask density:  0.050049759447574615 ; pred:  tensor([0.0286, 0.8446, 0.0816, 0.0054, 0.0036, 0.0268, 0.0084, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  17.181528091430664 ; mask density:  0.04818098992109299 ; pred:  tensor([0.0287, 0.8449, 0.0812, 0.0053, 0.0036, 0.0268, 0.0084, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  16.48752784729004 ; mask density:  0.04644911736249924 ; pred:  tensor([0.0288, 0.8453, 0.0808, 0.0053, 0.0035, 0.0267, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  15.843811988830566 ; mask density:  0.044841911643743515 ; pred:  tensor([0.0288, 0.8456, 0.0805, 0.0053, 0.0035, 0.0266, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  15.24582576751709 ; mask density:  0.043348364531993866 ; pred:  tensor([0.0289, 0.8459, 0.0803, 0.0053, 0.0035, 0.0266, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  14.689473152160645 ; mask density:  0.041958507150411606 ; pred:  tensor([0.0289, 0.8460, 0.0802, 0.0053, 0.0035, 0.0266, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  14.171051979064941 ; mask density:  0.04066336899995804 ; pred:  tensor([0.0290, 0.8460, 0.0801, 0.0053, 0.0035, 0.0266, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  13.687237739562988 ; mask density:  0.0394548699259758 ; pred:  tensor([0.0290, 0.8459, 0.0802, 0.0052, 0.0034, 0.0266, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  13.235034942626953 ; mask density:  0.038325726985931396 ; pred:  tensor([0.0290, 0.8457, 0.0802, 0.0052, 0.0034, 0.0267, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  12.811738967895508 ; mask density:  0.03726935759186745 ; pred:  tensor([0.0290, 0.8454, 0.0804, 0.0053, 0.0034, 0.0268, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  12.414900779724121 ; mask density:  0.03627939149737358 ; pred:  tensor([0.0290, 0.8451, 0.0806, 0.0053, 0.0034, 0.0269, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  12.042315483093262 ; mask density:  0.035350456833839417 ; pred:  tensor([0.0289, 0.8448, 0.0808, 0.0053, 0.0034, 0.0270, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  11.691996574401855 ; mask density:  0.03447766974568367 ; pred:  tensor([0.0289, 0.8444, 0.0811, 0.0053, 0.0034, 0.0271, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  11.362289428710938 ; mask density:  0.03365602344274521 ; pred:  tensor([0.0289, 0.8439, 0.0814, 0.0053, 0.0034, 0.0272, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  11.051342010498047 ; mask density:  0.03288141265511513 ; pred:  tensor([0.0289, 0.8437, 0.0816, 0.0053, 0.0034, 0.0272, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  10.757633209228516 ; mask density:  0.03214992582798004 ; pred:  tensor([0.0289, 0.8438, 0.0815, 0.0053, 0.0034, 0.0272, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  10.479826927185059 ; mask density:  0.03145786002278328 ; pred:  tensor([0.0289, 0.8442, 0.0814, 0.0053, 0.0034, 0.0270, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  10.21682357788086 ; mask density:  0.030802231281995773 ; pred:  tensor([0.0289, 0.8447, 0.0811, 0.0053, 0.0034, 0.0267, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  9.967536926269531 ; mask density:  0.030179928988218307 ; pred:  tensor([0.0289, 0.8451, 0.0810, 0.0053, 0.0034, 0.0266, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  9.730853080749512 ; mask density:  0.02958817221224308 ; pred:  tensor([0.0289, 0.8453, 0.0809, 0.0053, 0.0034, 0.0264, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  9.505826950073242 ; mask density:  0.029027074575424194 ; pred:  tensor([0.0289, 0.8454, 0.0809, 0.0053, 0.0034, 0.0264, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  9.29159927368164 ; mask density:  0.02849414013326168 ; pred:  tensor([0.0289, 0.8455, 0.0808, 0.0053, 0.0034, 0.0263, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  9.087421417236328 ; mask density:  0.027987079694867134 ; pred:  tensor([0.0289, 0.8456, 0.0808, 0.0053, 0.0034, 0.0263, 0.0086, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  8.892599105834961 ; mask density:  0.027503786608576775 ; pred:  tensor([0.0289, 0.8456, 0.0809, 0.0053, 0.0034, 0.0262, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  8.706487655639648 ; mask density:  0.027042321860790253 ; pred:  tensor([0.0289, 0.8456, 0.0810, 0.0053, 0.0034, 0.0262, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  8.528501510620117 ; mask density:  0.02660091407597065 ; pred:  tensor([0.0289, 0.8456, 0.0811, 0.0053, 0.0034, 0.0261, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  8.358197212219238 ; mask density:  0.026177620515227318 ; pred:  tensor([0.0289, 0.8454, 0.0813, 0.0053, 0.0034, 0.0261, 0.0085, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  8.194944381713867 ; mask density:  0.025770938023924828 ; pred:  tensor([0.0289, 0.8456, 0.0813, 0.0053, 0.0034, 0.0260, 0.0084, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  8.038251876831055 ; mask density:  0.025379423052072525 ; pred:  tensor([0.0289, 0.8460, 0.0811, 0.0053, 0.0034, 0.0258, 0.0084, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  7.887853145599365 ; mask density:  0.025001995265483856 ; pred:  tensor([0.0290, 0.8465, 0.0808, 0.0053, 0.0034, 0.0256, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  7.743378162384033 ; mask density:  0.0246328953653574 ; pred:  tensor([0.0290, 0.8469, 0.0806, 0.0053, 0.0034, 0.0254, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  7.604241371154785 ; mask density:  0.024272367358207703 ; pred:  tensor([0.0290, 0.8468, 0.0806, 0.0053, 0.0034, 0.0255, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  7.470218181610107 ; mask density:  0.023924926295876503 ; pred:  tensor([0.0290, 0.8464, 0.0807, 0.0053, 0.0035, 0.0257, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  7.341230869293213 ; mask density:  0.023589778691530228 ; pred:  tensor([0.0290, 0.8458, 0.0810, 0.0053, 0.0035, 0.0260, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  7.216766834259033 ; mask density:  0.023266304284334183 ; pred:  tensor([0.0290, 0.8456, 0.0811, 0.0053, 0.0035, 0.0261, 0.0084, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  7.09652042388916 ; mask density:  0.022953800857067108 ; pred:  tensor([0.0290, 0.8457, 0.0809, 0.0053, 0.0035, 0.0261, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  6.980266571044922 ; mask density:  0.02265153080224991 ; pred:  tensor([0.0290, 0.8462, 0.0806, 0.0053, 0.0035, 0.0261, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  6.867995262145996 ; mask density:  0.02235892042517662 ; pred:  tensor([0.0291, 0.8467, 0.0802, 0.0053, 0.0035, 0.0259, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  6.759426593780518 ; mask density:  0.022071586921811104 ; pred:  tensor([0.0291, 0.8471, 0.0798, 0.0052, 0.0035, 0.0258, 0.0082, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  6.654217720031738 ; mask density:  0.02178838662803173 ; pred:  tensor([0.0292, 0.8472, 0.0797, 0.0052, 0.0035, 0.0259, 0.0082, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  6.552160739898682 ; mask density:  0.021513253450393677 ; pred:  tensor([0.0292, 0.8469, 0.0797, 0.0052, 0.0035, 0.0261, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  6.453216552734375 ; mask density:  0.021246131509542465 ; pred:  tensor([0.0292, 0.8466, 0.0797, 0.0052, 0.0035, 0.0263, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  6.357350826263428 ; mask density:  0.02098696492612362 ; pred:  tensor([0.0292, 0.8462, 0.0798, 0.0052, 0.0035, 0.0266, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  6.264287948608398 ; mask density:  0.02073557861149311 ; pred:  tensor([0.0293, 0.8460, 0.0798, 0.0052, 0.0035, 0.0267, 0.0084, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  6.173852920532227 ; mask density:  0.020491737872362137 ; pred:  tensor([0.0293, 0.8462, 0.0795, 0.0052, 0.0035, 0.0267, 0.0084, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  6.085929870605469 ; mask density:  0.02025516703724861 ; pred:  tensor([0.0294, 0.8466, 0.0791, 0.0052, 0.0035, 0.0267, 0.0084, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  6.0004191398620605 ; mask density:  0.02002555876970291 ; pred:  tensor([0.0295, 0.8472, 0.0786, 0.0052, 0.0035, 0.0266, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  5.917265892028809 ; mask density:  0.01980268396437168 ; pred:  tensor([0.0295, 0.8479, 0.0781, 0.0052, 0.0035, 0.0265, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  5.836434841156006 ; mask density:  0.019586393609642982 ; pred:  tensor([0.0296, 0.8484, 0.0777, 0.0051, 0.0035, 0.0264, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  5.757724285125732 ; mask density:  0.019376365467905998 ; pred:  tensor([0.0296, 0.8487, 0.0774, 0.0051, 0.0035, 0.0263, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  5.681035041809082 ; mask density:  0.019172072410583496 ; pred:  tensor([0.0297, 0.8489, 0.0772, 0.0051, 0.0035, 0.0262, 0.0082, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  5.606280326843262 ; mask density:  0.018973305821418762 ; pred:  tensor([0.0297, 0.8490, 0.0771, 0.0051, 0.0035, 0.0262, 0.0082, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  5.533509731292725 ; mask density:  0.018779831007122993 ; pred:  tensor([0.0297, 0.8489, 0.0772, 0.0051, 0.0035, 0.0263, 0.0083, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  10.330157041549683\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_660graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  665 94\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  48.94431686401367 ; mask density:  0.7109224200248718 ; pred:  tensor([0.0211, 0.6772, 0.2424, 0.0147, 0.0033, 0.0200, 0.0193, 0.0021],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  47.59527587890625 ; mask density:  0.6908271908760071 ; pred:  tensor([0.0222, 0.7089, 0.2134, 0.0132, 0.0033, 0.0201, 0.0170, 0.0019],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  46.198936462402344 ; mask density:  0.6699255108833313 ; pred:  tensor([0.0230, 0.7341, 0.1904, 0.0120, 0.0034, 0.0202, 0.0153, 0.0018],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  44.757747650146484 ; mask density:  0.6481393575668335 ; pred:  tensor([0.0237, 0.7523, 0.1737, 0.0111, 0.0034, 0.0201, 0.0141, 0.0017],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  43.26892852783203 ; mask density:  0.6255946755409241 ; pred:  tensor([0.0243, 0.7671, 0.1600, 0.0103, 0.0034, 0.0202, 0.0131, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  41.73528289794922 ; mask density:  0.6023886203765869 ; pred:  tensor([0.0248, 0.7797, 0.1483, 0.0097, 0.0034, 0.0204, 0.0123, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  40.16230010986328 ; mask density:  0.5786342024803162 ; pred:  tensor([0.0252, 0.7895, 0.1389, 0.0091, 0.0034, 0.0206, 0.0118, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  38.55390548706055 ; mask density:  0.5544424057006836 ; pred:  tensor([0.0256, 0.7980, 0.1307, 0.0086, 0.0034, 0.0210, 0.0113, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  36.91675567626953 ; mask density:  0.5299294590950012 ; pred:  tensor([0.0259, 0.8056, 0.1234, 0.0082, 0.0034, 0.0212, 0.0109, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  35.259552001953125 ; mask density:  0.5052537322044373 ; pred:  tensor([0.0262, 0.8116, 0.1175, 0.0078, 0.0035, 0.0216, 0.0105, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  33.59064483642578 ; mask density:  0.48055291175842285 ; pred:  tensor([0.0265, 0.8162, 0.1126, 0.0075, 0.0035, 0.0222, 0.0103, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  31.918655395507812 ; mask density:  0.45597684383392334 ; pred:  tensor([0.0267, 0.8199, 0.1082, 0.0072, 0.0035, 0.0230, 0.0103, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  30.253149032592773 ; mask density:  0.43167123198509216 ; pred:  tensor([0.0269, 0.8225, 0.1047, 0.0070, 0.0036, 0.0238, 0.0103, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  28.603759765625 ; mask density:  0.40778422355651855 ; pred:  tensor([0.0271, 0.8241, 0.1020, 0.0068, 0.0036, 0.0248, 0.0103, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  26.979196548461914 ; mask density:  0.3844257891178131 ; pred:  tensor([0.0273, 0.8253, 0.0996, 0.0066, 0.0037, 0.0260, 0.0103, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  25.388662338256836 ; mask density:  0.3617250621318817 ; pred:  tensor([0.0275, 0.8259, 0.0976, 0.0065, 0.0038, 0.0272, 0.0103, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  23.84026527404785 ; mask density:  0.3397921025753021 ; pred:  tensor([0.0276, 0.8263, 0.0959, 0.0064, 0.0039, 0.0283, 0.0104, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  22.343053817749023 ; mask density:  0.31874749064445496 ; pred:  tensor([0.0276, 0.8254, 0.0952, 0.0063, 0.0040, 0.0296, 0.0105, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  20.902379989624023 ; mask density:  0.29867011308670044 ; pred:  tensor([0.0276, 0.8246, 0.0946, 0.0063, 0.0041, 0.0308, 0.0107, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  19.523412704467773 ; mask density:  0.27961465716362 ; pred:  tensor([0.0276, 0.8242, 0.0939, 0.0063, 0.0042, 0.0317, 0.0108, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  18.210542678833008 ; mask density:  0.26162025332450867 ; pred:  tensor([0.0277, 0.8243, 0.0928, 0.0062, 0.0043, 0.0325, 0.0108, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  16.967174530029297 ; mask density:  0.24470572173595428 ; pred:  tensor([0.0278, 0.8248, 0.0918, 0.0062, 0.0043, 0.0330, 0.0109, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  15.795486450195312 ; mask density:  0.22884753346443176 ; pred:  tensor([0.0279, 0.8253, 0.0908, 0.0061, 0.0043, 0.0334, 0.0109, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  14.696547508239746 ; mask density:  0.2140415608882904 ; pred:  tensor([0.0280, 0.8257, 0.0900, 0.0061, 0.0043, 0.0338, 0.0109, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  13.669981002807617 ; mask density:  0.20025287568569183 ; pred:  tensor([0.0280, 0.8261, 0.0894, 0.0061, 0.0043, 0.0340, 0.0109, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  12.714642524719238 ; mask density:  0.18745315074920654 ; pred:  tensor([0.0280, 0.8265, 0.0890, 0.0061, 0.0043, 0.0340, 0.0108, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  11.828598022460938 ; mask density:  0.17560628056526184 ; pred:  tensor([0.0280, 0.8270, 0.0887, 0.0060, 0.0043, 0.0339, 0.0108, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  11.009328842163086 ; mask density:  0.16466988623142242 ; pred:  tensor([0.0280, 0.8275, 0.0885, 0.0060, 0.0043, 0.0337, 0.0107, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  10.253862380981445 ; mask density:  0.15458451211452484 ; pred:  tensor([0.0279, 0.8280, 0.0884, 0.0061, 0.0042, 0.0334, 0.0106, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  9.5587158203125 ; mask density:  0.14530043303966522 ; pred:  tensor([0.0278, 0.8285, 0.0885, 0.0061, 0.0042, 0.0331, 0.0105, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  8.920194625854492 ; mask density:  0.1367652416229248 ; pred:  tensor([0.0277, 0.8291, 0.0886, 0.0061, 0.0042, 0.0327, 0.0103, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  8.334650039672852 ; mask density:  0.12892918288707733 ; pred:  tensor([0.0276, 0.8295, 0.0888, 0.0061, 0.0042, 0.0323, 0.0102, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  7.798280239105225 ; mask density:  0.12173780798912048 ; pred:  tensor([0.0274, 0.8299, 0.0892, 0.0061, 0.0042, 0.0318, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  7.3071722984313965 ; mask density:  0.11512182652950287 ; pred:  tensor([0.0273, 0.8305, 0.0894, 0.0062, 0.0042, 0.0313, 0.0099, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  6.857340335845947 ; mask density:  0.10903836041688919 ; pred:  tensor([0.0273, 0.8315, 0.0889, 0.0061, 0.0042, 0.0310, 0.0098, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  6.445928573608398 ; mask density:  0.10345498472452164 ; pred:  tensor([0.0273, 0.8324, 0.0886, 0.0061, 0.0041, 0.0305, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  6.06948184967041 ; mask density:  0.09831537306308746 ; pred:  tensor([0.0274, 0.8335, 0.0883, 0.0061, 0.0041, 0.0299, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  5.724743366241455 ; mask density:  0.09357816725969315 ; pred:  tensor([0.0275, 0.8349, 0.0876, 0.0061, 0.0041, 0.0294, 0.0093, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  5.408719539642334 ; mask density:  0.08921021968126297 ; pred:  tensor([0.0276, 0.8366, 0.0861, 0.0060, 0.0041, 0.0291, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  5.119302749633789 ; mask density:  0.08518187701702118 ; pred:  tensor([0.0278, 0.8383, 0.0848, 0.0059, 0.0040, 0.0289, 0.0091, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  4.854078769683838 ; mask density:  0.08146907389163971 ; pred:  tensor([0.0280, 0.8399, 0.0835, 0.0058, 0.0040, 0.0287, 0.0091, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  4.6109442710876465 ; mask density:  0.07804493606090546 ; pred:  tensor([0.0281, 0.8412, 0.0825, 0.0057, 0.0039, 0.0285, 0.0090, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  4.387768745422363 ; mask density:  0.07488842308521271 ; pred:  tensor([0.0282, 0.8423, 0.0816, 0.0056, 0.0039, 0.0282, 0.0090, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  4.182765007019043 ; mask density:  0.07197535783052444 ; pred:  tensor([0.0283, 0.8432, 0.0809, 0.0055, 0.0039, 0.0280, 0.0090, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  3.9941866397857666 ; mask density:  0.06928407400846481 ; pred:  tensor([0.0283, 0.8440, 0.0804, 0.0055, 0.0038, 0.0278, 0.0090, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  3.8204708099365234 ; mask density:  0.06679382920265198 ; pred:  tensor([0.0284, 0.8445, 0.0801, 0.0054, 0.0038, 0.0276, 0.0090, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  3.6602065563201904 ; mask density:  0.064485564827919 ; pred:  tensor([0.0284, 0.8449, 0.0799, 0.0054, 0.0037, 0.0274, 0.0090, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  3.512160062789917 ; mask density:  0.06234293803572655 ; pred:  tensor([0.0285, 0.8451, 0.0801, 0.0054, 0.0037, 0.0272, 0.0090, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  3.3751935958862305 ; mask density:  0.06034925952553749 ; pred:  tensor([0.0285, 0.8451, 0.0803, 0.0054, 0.0037, 0.0270, 0.0090, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  3.2483747005462646 ; mask density:  0.058495353907346725 ; pred:  tensor([0.0285, 0.8447, 0.0807, 0.0054, 0.0036, 0.0270, 0.0090, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  3.1307015419006348 ; mask density:  0.05676925927400589 ; pred:  tensor([0.0284, 0.8442, 0.0812, 0.0054, 0.0036, 0.0270, 0.0090, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  3.021364212036133 ; mask density:  0.055160801857709885 ; pred:  tensor([0.0284, 0.8435, 0.0818, 0.0054, 0.0036, 0.0270, 0.0091, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  2.9196574687957764 ; mask density:  0.05366002023220062 ; pred:  tensor([0.0283, 0.8426, 0.0826, 0.0054, 0.0036, 0.0271, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  2.824899196624756 ; mask density:  0.05225508660078049 ; pred:  tensor([0.0283, 0.8416, 0.0834, 0.0055, 0.0036, 0.0272, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  2.7363319396972656 ; mask density:  0.05093852058053017 ; pred:  tensor([0.0282, 0.8407, 0.0841, 0.0055, 0.0035, 0.0274, 0.0093, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  2.6535532474517822 ; mask density:  0.049702826887369156 ; pred:  tensor([0.0282, 0.8398, 0.0848, 0.0055, 0.0035, 0.0275, 0.0094, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  2.5760130882263184 ; mask density:  0.048539821058511734 ; pred:  tensor([0.0281, 0.8390, 0.0855, 0.0055, 0.0035, 0.0277, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  2.5033044815063477 ; mask density:  0.04744379594922066 ; pred:  tensor([0.0281, 0.8382, 0.0861, 0.0055, 0.0035, 0.0279, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  2.435056209564209 ; mask density:  0.04640733823180199 ; pred:  tensor([0.0281, 0.8374, 0.0866, 0.0055, 0.0035, 0.0280, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  2.3708431720733643 ; mask density:  0.04542657360434532 ; pred:  tensor([0.0282, 0.8367, 0.0868, 0.0055, 0.0035, 0.0283, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  2.3104143142700195 ; mask density:  0.04450257867574692 ; pred:  tensor([0.0282, 0.8361, 0.0870, 0.0055, 0.0035, 0.0286, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  2.254124641418457 ; mask density:  0.043631404638290405 ; pred:  tensor([0.0283, 0.8350, 0.0876, 0.0055, 0.0035, 0.0289, 0.0101, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  2.200930118560791 ; mask density:  0.04280906543135643 ; pred:  tensor([0.0283, 0.8343, 0.0879, 0.0055, 0.0034, 0.0291, 0.0102, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  2.150489568710327 ; mask density:  0.04203125834465027 ; pred:  tensor([0.0284, 0.8341, 0.0878, 0.0055, 0.0034, 0.0291, 0.0103, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  2.1026275157928467 ; mask density:  0.04129451885819435 ; pred:  tensor([0.0285, 0.8344, 0.0875, 0.0054, 0.0034, 0.0291, 0.0104, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  2.0571935176849365 ; mask density:  0.04059582203626633 ; pred:  tensor([0.0287, 0.8349, 0.0871, 0.0054, 0.0033, 0.0289, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  2.0140371322631836 ; mask density:  0.039932191371917725 ; pred:  tensor([0.0288, 0.8356, 0.0865, 0.0054, 0.0033, 0.0287, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  1.9730663299560547 ; mask density:  0.0392969474196434 ; pred:  tensor([0.0290, 0.8364, 0.0857, 0.0053, 0.0032, 0.0286, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  1.9341331720352173 ; mask density:  0.038688600063323975 ; pred:  tensor([0.0291, 0.8372, 0.0850, 0.0053, 0.0032, 0.0286, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  1.8970355987548828 ; mask density:  0.03810589760541916 ; pred:  tensor([0.0292, 0.8378, 0.0843, 0.0053, 0.0032, 0.0286, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  1.8616372346878052 ; mask density:  0.037547770887613297 ; pred:  tensor([0.0293, 0.8383, 0.0837, 0.0052, 0.0032, 0.0286, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  1.8278135061264038 ; mask density:  0.037013355642557144 ; pred:  tensor([0.0294, 0.8388, 0.0831, 0.0052, 0.0032, 0.0287, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  1.795452356338501 ; mask density:  0.036501895636320114 ; pred:  tensor([0.0295, 0.8391, 0.0826, 0.0052, 0.0032, 0.0288, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  1.7644554376602173 ; mask density:  0.036012765020132065 ; pred:  tensor([0.0296, 0.8393, 0.0822, 0.0052, 0.0032, 0.0289, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  1.734727382659912 ; mask density:  0.03554598614573479 ; pred:  tensor([0.0297, 0.8394, 0.0818, 0.0051, 0.0032, 0.0291, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  1.7061935663223267 ; mask density:  0.03510100767016411 ; pred:  tensor([0.0297, 0.8394, 0.0815, 0.0051, 0.0032, 0.0293, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  1.6788703203201294 ; mask density:  0.03468207269906998 ; pred:  tensor([0.0298, 0.8393, 0.0812, 0.0051, 0.0032, 0.0296, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  1.6527385711669922 ; mask density:  0.03428807854652405 ; pred:  tensor([0.0298, 0.8391, 0.0810, 0.0051, 0.0033, 0.0300, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  1.6275620460510254 ; mask density:  0.03391788527369499 ; pred:  tensor([0.0298, 0.8388, 0.0810, 0.0051, 0.0033, 0.0303, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  1.6032315492630005 ; mask density:  0.03357020765542984 ; pred:  tensor([0.0298, 0.8384, 0.0811, 0.0051, 0.0033, 0.0305, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  1.5797936916351318 ; mask density:  0.03324376419186592 ; pred:  tensor([0.0297, 0.8378, 0.0813, 0.0051, 0.0033, 0.0308, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  1.5572435855865479 ; mask density:  0.03293716534972191 ; pred:  tensor([0.0297, 0.8374, 0.0815, 0.0051, 0.0033, 0.0309, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  1.5354481935501099 ; mask density:  0.032648999243974686 ; pred:  tensor([0.0297, 0.8373, 0.0817, 0.0051, 0.0033, 0.0310, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  1.5143611431121826 ; mask density:  0.032377827912569046 ; pred:  tensor([0.0297, 0.8373, 0.0817, 0.0052, 0.0033, 0.0309, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  1.4939448833465576 ; mask density:  0.03212219476699829 ; pred:  tensor([0.0297, 0.8375, 0.0817, 0.0052, 0.0033, 0.0308, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  1.4741696119308472 ; mask density:  0.03188062831759453 ; pred:  tensor([0.0297, 0.8379, 0.0816, 0.0052, 0.0033, 0.0305, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  1.4550108909606934 ; mask density:  0.03165167570114136 ; pred:  tensor([0.0297, 0.8384, 0.0815, 0.0051, 0.0033, 0.0303, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  1.4364471435546875 ; mask density:  0.03143389895558357 ; pred:  tensor([0.0297, 0.8390, 0.0813, 0.0051, 0.0033, 0.0299, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  1.4185714721679688 ; mask density:  0.03122587688267231 ; pred:  tensor([0.0297, 0.8395, 0.0812, 0.0051, 0.0033, 0.0296, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  1.401160717010498 ; mask density:  0.031026260927319527 ; pred:  tensor([0.0297, 0.8398, 0.0812, 0.0051, 0.0033, 0.0294, 0.0103, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  1.3841779232025146 ; mask density:  0.030833762139081955 ; pred:  tensor([0.0296, 0.8400, 0.0813, 0.0051, 0.0032, 0.0291, 0.0103, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  1.3677794933319092 ; mask density:  0.03064722754061222 ; pred:  tensor([0.0296, 0.8398, 0.0817, 0.0052, 0.0032, 0.0290, 0.0103, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  1.3518834114074707 ; mask density:  0.03046063520014286 ; pred:  tensor([0.0295, 0.8397, 0.0820, 0.0052, 0.0032, 0.0288, 0.0103, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  1.3364204168319702 ; mask density:  0.030273739248514175 ; pred:  tensor([0.0295, 0.8395, 0.0823, 0.0052, 0.0032, 0.0288, 0.0103, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  1.3213541507720947 ; mask density:  0.030091553926467896 ; pred:  tensor([0.0294, 0.8393, 0.0826, 0.0052, 0.0032, 0.0287, 0.0103, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  1.306697964668274 ; mask density:  0.029913699254393578 ; pred:  tensor([0.0294, 0.8392, 0.0827, 0.0052, 0.0032, 0.0287, 0.0103, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  1.2924153804779053 ; mask density:  0.029739851132035255 ; pred:  tensor([0.0294, 0.8392, 0.0828, 0.0052, 0.0033, 0.0287, 0.0103, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  1.2784903049468994 ; mask density:  0.0295697133988142 ; pred:  tensor([0.0293, 0.8393, 0.0828, 0.0052, 0.0033, 0.0286, 0.0102, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  1.2649075984954834 ; mask density:  0.029403051361441612 ; pred:  tensor([0.0293, 0.8395, 0.0828, 0.0052, 0.0033, 0.0285, 0.0102, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  1.2516560554504395 ; mask density:  0.02923966944217682 ; pred:  tensor([0.0293, 0.8398, 0.0826, 0.0052, 0.0033, 0.0284, 0.0101, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  9.377153396606445\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_665graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  670 90\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  41.740478515625 ; mask density:  0.7107417583465576 ; pred:  tensor([0.0219, 0.6623, 0.2547, 0.0151, 0.0030, 0.0200, 0.0209, 0.0021],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  40.593727111816406 ; mask density:  0.6908663511276245 ; pred:  tensor([0.0228, 0.6872, 0.2319, 0.0140, 0.0030, 0.0200, 0.0192, 0.0019],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  39.4041748046875 ; mask density:  0.6701169610023499 ; pred:  tensor([0.0235, 0.7083, 0.2127, 0.0129, 0.0030, 0.0201, 0.0177, 0.0018],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  38.17292022705078 ; mask density:  0.6485431790351868 ; pred:  tensor([0.0241, 0.7260, 0.1969, 0.0121, 0.0030, 0.0200, 0.0163, 0.0017],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  36.90150833129883 ; mask density:  0.6262155771255493 ; pred:  tensor([0.0245, 0.7410, 0.1835, 0.0114, 0.0030, 0.0199, 0.0150, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  35.59071731567383 ; mask density:  0.6032155752182007 ; pred:  tensor([0.0250, 0.7554, 0.1708, 0.0107, 0.0030, 0.0197, 0.0138, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  34.24666976928711 ; mask density:  0.5796424150466919 ; pred:  tensor([0.0254, 0.7677, 0.1599, 0.0101, 0.0030, 0.0197, 0.0128, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  32.873905181884766 ; mask density:  0.5556110143661499 ; pred:  tensor([0.0257, 0.7782, 0.1504, 0.0096, 0.0030, 0.0198, 0.0120, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  31.478105545043945 ; mask density:  0.5312288999557495 ; pred:  tensor([0.0259, 0.7871, 0.1424, 0.0091, 0.0030, 0.0199, 0.0114, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  30.06552505493164 ; mask density:  0.5066404342651367 ; pred:  tensor([0.0261, 0.7946, 0.1354, 0.0086, 0.0030, 0.0200, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  28.643178939819336 ; mask density:  0.4819846749305725 ; pred:  tensor([0.0262, 0.8009, 0.1295, 0.0082, 0.0030, 0.0202, 0.0107, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  27.218935012817383 ; mask density:  0.4574085772037506 ; pred:  tensor([0.0263, 0.8057, 0.1249, 0.0079, 0.0029, 0.0205, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  25.800857543945312 ; mask density:  0.4330558776855469 ; pred:  tensor([0.0264, 0.8091, 0.1214, 0.0077, 0.0029, 0.0208, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  24.395118713378906 ; mask density:  0.4090573191642761 ; pred:  tensor([0.0267, 0.8126, 0.1174, 0.0074, 0.0029, 0.0214, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  23.009288787841797 ; mask density:  0.38555216789245605 ; pred:  tensor([0.0271, 0.8165, 0.1127, 0.0071, 0.0029, 0.0221, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  21.653825759887695 ; mask density:  0.3626769185066223 ; pred:  tensor([0.0274, 0.8187, 0.1094, 0.0069, 0.0030, 0.0229, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  20.33438491821289 ; mask density:  0.34054094552993774 ; pred:  tensor([0.0278, 0.8205, 0.1064, 0.0068, 0.0030, 0.0238, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  19.05769157409668 ; mask density:  0.319247841835022 ; pred:  tensor([0.0281, 0.8217, 0.1037, 0.0066, 0.0031, 0.0249, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  17.828384399414062 ; mask density:  0.29888269305229187 ; pred:  tensor([0.0283, 0.8235, 0.1008, 0.0065, 0.0032, 0.0260, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  16.65167808532715 ; mask density:  0.2795099914073944 ; pred:  tensor([0.0285, 0.8253, 0.0982, 0.0063, 0.0033, 0.0268, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  15.53236198425293 ; mask density:  0.2611843943595886 ; pred:  tensor([0.0284, 0.8266, 0.0966, 0.0063, 0.0034, 0.0272, 0.0103, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  14.472639083862305 ; mask density:  0.24393406510353088 ; pred:  tensor([0.0285, 0.8276, 0.0951, 0.0062, 0.0034, 0.0278, 0.0103, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  13.473640441894531 ; mask density:  0.22777312994003296 ; pred:  tensor([0.0286, 0.8290, 0.0933, 0.0062, 0.0035, 0.0282, 0.0102, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  12.536458969116211 ; mask density:  0.21268191933631897 ; pred:  tensor([0.0287, 0.8304, 0.0916, 0.0061, 0.0035, 0.0284, 0.0102, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  11.660924911499023 ; mask density:  0.19864316284656525 ; pred:  tensor([0.0288, 0.8317, 0.0901, 0.0060, 0.0035, 0.0286, 0.0101, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  10.846090316772461 ; mask density:  0.1856265813112259 ; pred:  tensor([0.0288, 0.8330, 0.0888, 0.0060, 0.0035, 0.0286, 0.0101, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  10.090411186218262 ; mask density:  0.17359821498394012 ; pred:  tensor([0.0288, 0.8342, 0.0879, 0.0059, 0.0035, 0.0285, 0.0101, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  9.391680717468262 ; mask density:  0.162501260638237 ; pred:  tensor([0.0288, 0.8353, 0.0870, 0.0059, 0.0035, 0.0285, 0.0100, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  8.747037887573242 ; mask density:  0.15228596329689026 ; pred:  tensor([0.0289, 0.8365, 0.0856, 0.0058, 0.0035, 0.0286, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  8.15392780303955 ; mask density:  0.14289860427379608 ; pred:  tensor([0.0290, 0.8375, 0.0844, 0.0057, 0.0036, 0.0287, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  7.60917854309082 ; mask density:  0.13428381085395813 ; pred:  tensor([0.0292, 0.8384, 0.0833, 0.0057, 0.0036, 0.0288, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  7.109621047973633 ; mask density:  0.12638500332832336 ; pred:  tensor([0.0293, 0.8393, 0.0824, 0.0056, 0.0036, 0.0288, 0.0098, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  6.652039527893066 ; mask density:  0.11914623528718948 ; pred:  tensor([0.0294, 0.8400, 0.0816, 0.0056, 0.0036, 0.0289, 0.0098, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  6.233107089996338 ; mask density:  0.11250297725200653 ; pred:  tensor([0.0295, 0.8407, 0.0809, 0.0056, 0.0036, 0.0289, 0.0098, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  5.849391937255859 ; mask density:  0.10639993101358414 ; pred:  tensor([0.0296, 0.8416, 0.0797, 0.0055, 0.0036, 0.0290, 0.0097, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  5.49835205078125 ; mask density:  0.10079269111156464 ; pred:  tensor([0.0298, 0.8424, 0.0787, 0.0054, 0.0036, 0.0292, 0.0097, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  5.177243709564209 ; mask density:  0.09563831239938736 ; pred:  tensor([0.0300, 0.8431, 0.0779, 0.0054, 0.0037, 0.0293, 0.0096, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  4.883437633514404 ; mask density:  0.09089799225330353 ; pred:  tensor([0.0301, 0.8436, 0.0771, 0.0054, 0.0037, 0.0294, 0.0096, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  4.614545822143555 ; mask density:  0.0865342766046524 ; pred:  tensor([0.0302, 0.8440, 0.0766, 0.0053, 0.0037, 0.0295, 0.0095, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  4.368336200714111 ; mask density:  0.0825154259800911 ; pred:  tensor([0.0303, 0.8442, 0.0762, 0.0053, 0.0037, 0.0297, 0.0095, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  4.142710208892822 ; mask density:  0.07880935072898865 ; pred:  tensor([0.0303, 0.8442, 0.0761, 0.0053, 0.0037, 0.0298, 0.0095, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  3.935619354248047 ; mask density:  0.07538296282291412 ; pred:  tensor([0.0303, 0.8442, 0.0759, 0.0052, 0.0037, 0.0300, 0.0095, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  3.7453365325927734 ; mask density:  0.07221292704343796 ; pred:  tensor([0.0303, 0.8442, 0.0757, 0.0052, 0.0037, 0.0301, 0.0095, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  3.5703983306884766 ; mask density:  0.06927462667226791 ; pred:  tensor([0.0304, 0.8442, 0.0756, 0.0052, 0.0038, 0.0303, 0.0095, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  3.409390687942505 ; mask density:  0.06655000895261765 ; pred:  tensor([0.0304, 0.8440, 0.0756, 0.0052, 0.0038, 0.0305, 0.0095, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  3.2609949111938477 ; mask density:  0.06401938199996948 ; pred:  tensor([0.0304, 0.8438, 0.0754, 0.0051, 0.0038, 0.0309, 0.0095, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  3.124077558517456 ; mask density:  0.0616682693362236 ; pred:  tensor([0.0305, 0.8436, 0.0751, 0.0051, 0.0038, 0.0313, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  2.9976139068603516 ; mask density:  0.05948594957590103 ; pred:  tensor([0.0305, 0.8434, 0.0749, 0.0051, 0.0038, 0.0317, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  2.8806869983673096 ; mask density:  0.05745959281921387 ; pred:  tensor([0.0306, 0.8431, 0.0746, 0.0050, 0.0038, 0.0321, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  2.7723495960235596 ; mask density:  0.05558120459318161 ; pred:  tensor([0.0307, 0.8429, 0.0744, 0.0050, 0.0038, 0.0324, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  2.671963930130005 ; mask density:  0.053842704743146896 ; pred:  tensor([0.0309, 0.8426, 0.0739, 0.0050, 0.0038, 0.0329, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  2.578720808029175 ; mask density:  0.05223211646080017 ; pred:  tensor([0.0311, 0.8424, 0.0736, 0.0049, 0.0038, 0.0332, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  2.492154359817505 ; mask density:  0.05074867978692055 ; pred:  tensor([0.0313, 0.8421, 0.0733, 0.0049, 0.0038, 0.0335, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  2.411508560180664 ; mask density:  0.049380090087652206 ; pred:  tensor([0.0315, 0.8420, 0.0730, 0.0049, 0.0037, 0.0337, 0.0100, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  2.3361289501190186 ; mask density:  0.048115022480487823 ; pred:  tensor([0.0317, 0.8423, 0.0726, 0.0048, 0.0037, 0.0336, 0.0101, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  2.265585422515869 ; mask density:  0.04694320261478424 ; pred:  tensor([0.0319, 0.8428, 0.0722, 0.0048, 0.0036, 0.0334, 0.0101, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  2.1995019912719727 ; mask density:  0.04585537314414978 ; pred:  tensor([0.0321, 0.8435, 0.0718, 0.0048, 0.0035, 0.0330, 0.0101, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  2.1375439167022705 ; mask density:  0.04484324902296066 ; pred:  tensor([0.0323, 0.8443, 0.0714, 0.0048, 0.0035, 0.0324, 0.0101, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  2.0794053077697754 ; mask density:  0.043899402022361755 ; pred:  tensor([0.0324, 0.8452, 0.0710, 0.0047, 0.0034, 0.0319, 0.0101, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  2.0248007774353027 ; mask density:  0.043015964329242706 ; pred:  tensor([0.0326, 0.8461, 0.0707, 0.0047, 0.0033, 0.0313, 0.0101, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  1.9736428260803223 ; mask density:  0.04218415915966034 ; pred:  tensor([0.0328, 0.8467, 0.0702, 0.0047, 0.0033, 0.0310, 0.0101, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  1.9254982471466064 ; mask density:  0.041399408131837845 ; pred:  tensor([0.0329, 0.8472, 0.0699, 0.0047, 0.0033, 0.0308, 0.0101, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  1.8800641298294067 ; mask density:  0.040657635778188705 ; pred:  tensor([0.0330, 0.8476, 0.0696, 0.0046, 0.0032, 0.0307, 0.0101, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  1.8371003866195679 ; mask density:  0.03995523601770401 ; pred:  tensor([0.0331, 0.8478, 0.0694, 0.0046, 0.0032, 0.0306, 0.0101, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  1.7964391708374023 ; mask density:  0.03928899019956589 ; pred:  tensor([0.0332, 0.8478, 0.0693, 0.0046, 0.0032, 0.0306, 0.0102, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  1.7577661275863647 ; mask density:  0.038656190037727356 ; pred:  tensor([0.0332, 0.8477, 0.0693, 0.0046, 0.0032, 0.0306, 0.0102, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  1.720933437347412 ; mask density:  0.03805474191904068 ; pred:  tensor([0.0333, 0.8475, 0.0694, 0.0046, 0.0032, 0.0307, 0.0103, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  1.685936689376831 ; mask density:  0.03748255968093872 ; pred:  tensor([0.0333, 0.8471, 0.0695, 0.0046, 0.0032, 0.0308, 0.0103, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  1.6526496410369873 ; mask density:  0.036937836557626724 ; pred:  tensor([0.0333, 0.8467, 0.0697, 0.0046, 0.0032, 0.0310, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  1.6210886240005493 ; mask density:  0.036418985575437546 ; pred:  tensor([0.0333, 0.8461, 0.0698, 0.0046, 0.0032, 0.0313, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  1.590958595275879 ; mask density:  0.035924527794122696 ; pred:  tensor([0.0334, 0.8456, 0.0700, 0.0046, 0.0032, 0.0315, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  1.5621610879898071 ; mask density:  0.03545308858156204 ; pred:  tensor([0.0334, 0.8452, 0.0701, 0.0046, 0.0032, 0.0317, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  1.5346043109893799 ; mask density:  0.035003360360860825 ; pred:  tensor([0.0334, 0.8448, 0.0702, 0.0046, 0.0032, 0.0319, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  1.5082017183303833 ; mask density:  0.03457411378622055 ; pred:  tensor([0.0334, 0.8446, 0.0703, 0.0046, 0.0032, 0.0321, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  1.4829115867614746 ; mask density:  0.034167367964982986 ; pred:  tensor([0.0334, 0.8444, 0.0703, 0.0046, 0.0033, 0.0323, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  1.458618402481079 ; mask density:  0.033781662583351135 ; pred:  tensor([0.0333, 0.8444, 0.0703, 0.0046, 0.0033, 0.0323, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  1.4352129697799683 ; mask density:  0.03341551870107651 ; pred:  tensor([0.0333, 0.8446, 0.0701, 0.0046, 0.0033, 0.0323, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  1.4126957654953003 ; mask density:  0.033067453652620316 ; pred:  tensor([0.0333, 0.8451, 0.0700, 0.0046, 0.0033, 0.0321, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  1.391183614730835 ; mask density:  0.03273287042975426 ; pred:  tensor([0.0333, 0.8454, 0.0698, 0.0046, 0.0033, 0.0319, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  1.370507001876831 ; mask density:  0.032410863786935806 ; pred:  tensor([0.0333, 0.8456, 0.0696, 0.0046, 0.0033, 0.0319, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  1.3505758047103882 ; mask density:  0.03210063651204109 ; pred:  tensor([0.0334, 0.8458, 0.0695, 0.0046, 0.0033, 0.0318, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  1.3313438892364502 ; mask density:  0.0318014919757843 ; pred:  tensor([0.0334, 0.8459, 0.0694, 0.0046, 0.0033, 0.0318, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  1.3127695322036743 ; mask density:  0.0315127931535244 ; pred:  tensor([0.0334, 0.8460, 0.0694, 0.0046, 0.0033, 0.0318, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  1.2948154211044312 ; mask density:  0.031234011054039 ; pred:  tensor([0.0333, 0.8460, 0.0694, 0.0046, 0.0033, 0.0318, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  1.2774490118026733 ; mask density:  0.030964666977524757 ; pred:  tensor([0.0333, 0.8460, 0.0694, 0.0046, 0.0033, 0.0318, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  1.2606401443481445 ; mask density:  0.030704332515597343 ; pred:  tensor([0.0333, 0.8459, 0.0695, 0.0046, 0.0033, 0.0318, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  1.2443618774414062 ; mask density:  0.03045264445245266 ; pred:  tensor([0.0333, 0.8458, 0.0695, 0.0046, 0.0033, 0.0319, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  1.228631854057312 ; mask density:  0.030213002115488052 ; pred:  tensor([0.0333, 0.8456, 0.0696, 0.0046, 0.0033, 0.0320, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  1.2133638858795166 ; mask density:  0.02998471073806286 ; pred:  tensor([0.0332, 0.8455, 0.0697, 0.0046, 0.0033, 0.0320, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  1.1985299587249756 ; mask density:  0.029767027124762535 ; pred:  tensor([0.0332, 0.8455, 0.0697, 0.0046, 0.0033, 0.0320, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  1.1841111183166504 ; mask density:  0.029559189453721046 ; pred:  tensor([0.0332, 0.8457, 0.0698, 0.0046, 0.0033, 0.0318, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  1.1701322793960571 ; mask density:  0.029356548562645912 ; pred:  tensor([0.0332, 0.8459, 0.0698, 0.0046, 0.0033, 0.0317, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  1.1565608978271484 ; mask density:  0.029158707708120346 ; pred:  tensor([0.0332, 0.8460, 0.0698, 0.0046, 0.0033, 0.0316, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  1.143362045288086 ; mask density:  0.028965353965759277 ; pred:  tensor([0.0332, 0.8461, 0.0698, 0.0046, 0.0033, 0.0315, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  1.1305184364318848 ; mask density:  0.02877623587846756 ; pred:  tensor([0.0332, 0.8461, 0.0699, 0.0046, 0.0033, 0.0315, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  1.1180145740509033 ; mask density:  0.028591152280569077 ; pred:  tensor([0.0331, 0.8460, 0.0700, 0.0046, 0.0033, 0.0315, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  1.1058365106582642 ; mask density:  0.028409963473677635 ; pred:  tensor([0.0331, 0.8460, 0.0701, 0.0046, 0.0033, 0.0314, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  1.093973994255066 ; mask density:  0.028232602402567863 ; pred:  tensor([0.0331, 0.8459, 0.0702, 0.0046, 0.0033, 0.0314, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  1.0824626684188843 ; mask density:  0.02805900014936924 ; pred:  tensor([0.0331, 0.8458, 0.0703, 0.0046, 0.0033, 0.0315, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  1.0711995363235474 ; mask density:  0.027889054268598557 ; pred:  tensor([0.0330, 0.8458, 0.0703, 0.0046, 0.0033, 0.0315, 0.0104, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  9.525731086730957\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_670graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  675 71\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  30.097177505493164 ; mask density:  0.7129121422767639 ; pred:  tensor([0.0173, 0.4818, 0.4368, 0.0348, 0.0037, 0.0088, 0.0141, 0.0029],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  29.221769332885742 ; mask density:  0.6944296360015869 ; pred:  tensor([0.0184, 0.5337, 0.3888, 0.0303, 0.0037, 0.0094, 0.0130, 0.0026],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  28.32851791381836 ; mask density:  0.675071120262146 ; pred:  tensor([0.0194, 0.5814, 0.3444, 0.0262, 0.0038, 0.0101, 0.0122, 0.0024],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  27.416790008544922 ; mask density:  0.65489661693573 ; pred:  tensor([0.0203, 0.6238, 0.3049, 0.0229, 0.0038, 0.0107, 0.0114, 0.0022],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  26.487232208251953 ; mask density:  0.6339614391326904 ; pred:  tensor([0.0212, 0.6601, 0.2707, 0.0202, 0.0038, 0.0112, 0.0106, 0.0021],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  25.54102325439453 ; mask density:  0.6123509407043457 ; pred:  tensor([0.0220, 0.6897, 0.2428, 0.0181, 0.0039, 0.0117, 0.0099, 0.0019],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  24.57572364807129 ; mask density:  0.5901556015014648 ; pred:  tensor([0.0227, 0.7152, 0.2187, 0.0163, 0.0039, 0.0122, 0.0093, 0.0018],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  23.592960357666016 ; mask density:  0.5674958229064941 ; pred:  tensor([0.0233, 0.7374, 0.1976, 0.0147, 0.0039, 0.0127, 0.0088, 0.0017],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  22.5931453704834 ; mask density:  0.5445228815078735 ; pred:  tensor([0.0238, 0.7590, 0.1772, 0.0131, 0.0039, 0.0132, 0.0083, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  21.5833740234375 ; mask density:  0.5213191509246826 ; pred:  tensor([0.0243, 0.7778, 0.1593, 0.0117, 0.0038, 0.0138, 0.0078, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  20.568784713745117 ; mask density:  0.49801766872406006 ; pred:  tensor([0.0247, 0.7935, 0.1441, 0.0106, 0.0038, 0.0144, 0.0074, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  19.552427291870117 ; mask density:  0.47475606203079224 ; pred:  tensor([0.0251, 0.8079, 0.1302, 0.0095, 0.0038, 0.0151, 0.0070, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  18.540658950805664 ; mask density:  0.4516734182834625 ; pred:  tensor([0.0255, 0.8203, 0.1180, 0.0086, 0.0038, 0.0159, 0.0067, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  17.53989028930664 ; mask density:  0.4289052188396454 ; pred:  tensor([0.0259, 0.8300, 0.1082, 0.0079, 0.0038, 0.0166, 0.0065, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  16.556804656982422 ; mask density:  0.40658900141716003 ; pred:  tensor([0.0263, 0.8362, 0.1018, 0.0074, 0.0037, 0.0172, 0.0064, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  15.595185279846191 ; mask density:  0.38484108448028564 ; pred:  tensor([0.0265, 0.8401, 0.0975, 0.0070, 0.0037, 0.0178, 0.0064, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  14.658292770385742 ; mask density:  0.36376556754112244 ; pred:  tensor([0.0267, 0.8433, 0.0936, 0.0067, 0.0037, 0.0185, 0.0064, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  13.752546310424805 ; mask density:  0.3434600830078125 ; pred:  tensor([0.0267, 0.8445, 0.0916, 0.0065, 0.0037, 0.0193, 0.0066, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  12.881473541259766 ; mask density:  0.32400867342948914 ; pred:  tensor([0.0267, 0.8441, 0.0909, 0.0064, 0.0037, 0.0203, 0.0068, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  12.047222137451172 ; mask density:  0.3054739832878113 ; pred:  tensor([0.0267, 0.8434, 0.0902, 0.0063, 0.0038, 0.0214, 0.0071, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  11.252744674682617 ; mask density:  0.2879025340080261 ; pred:  tensor([0.0267, 0.8422, 0.0900, 0.0063, 0.0038, 0.0226, 0.0073, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  10.500275611877441 ; mask density:  0.27132511138916016 ; pred:  tensor([0.0267, 0.8404, 0.0898, 0.0063, 0.0040, 0.0242, 0.0075, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  9.791106224060059 ; mask density:  0.2557608187198639 ; pred:  tensor([0.0267, 0.8381, 0.0902, 0.0063, 0.0041, 0.0258, 0.0077, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  9.125555038452148 ; mask density:  0.24120411276817322 ; pred:  tensor([0.0266, 0.8356, 0.0907, 0.0063, 0.0042, 0.0274, 0.0080, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  8.503103256225586 ; mask density:  0.2276337891817093 ; pred:  tensor([0.0267, 0.8335, 0.0910, 0.0063, 0.0043, 0.0288, 0.0082, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  7.9232282638549805 ; mask density:  0.2150229811668396 ; pred:  tensor([0.0267, 0.8316, 0.0912, 0.0063, 0.0044, 0.0302, 0.0084, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  7.384848117828369 ; mask density:  0.20333410799503326 ; pred:  tensor([0.0267, 0.8300, 0.0913, 0.0063, 0.0045, 0.0314, 0.0086, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  6.88646936416626 ; mask density:  0.19252482056617737 ; pred:  tensor([0.0268, 0.8287, 0.0912, 0.0062, 0.0046, 0.0325, 0.0088, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  6.426176071166992 ; mask density:  0.18254461884498596 ; pred:  tensor([0.0269, 0.8279, 0.0906, 0.0062, 0.0046, 0.0336, 0.0089, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  6.002203941345215 ; mask density:  0.17334318161010742 ; pred:  tensor([0.0271, 0.8273, 0.0900, 0.0061, 0.0047, 0.0345, 0.0091, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  5.6123857498168945 ; mask density:  0.16486872732639313 ; pred:  tensor([0.0272, 0.8270, 0.0892, 0.0061, 0.0047, 0.0353, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  5.2545037269592285 ; mask density:  0.15706869959831238 ; pred:  tensor([0.0274, 0.8269, 0.0885, 0.0060, 0.0047, 0.0359, 0.0094, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  4.926326274871826 ; mask density:  0.14989154040813446 ; pred:  tensor([0.0275, 0.8271, 0.0877, 0.0059, 0.0047, 0.0363, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  4.625650405883789 ; mask density:  0.14328613877296448 ; pred:  tensor([0.0277, 0.8274, 0.0869, 0.0059, 0.0046, 0.0367, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  4.3503217697143555 ; mask density:  0.13720263540744781 ; pred:  tensor([0.0279, 0.8279, 0.0860, 0.0058, 0.0046, 0.0369, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  4.0982985496521 ; mask density:  0.13159452378749847 ; pred:  tensor([0.0280, 0.8285, 0.0852, 0.0058, 0.0046, 0.0370, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  3.8675880432128906 ; mask density:  0.12641865015029907 ; pred:  tensor([0.0282, 0.8291, 0.0844, 0.0057, 0.0045, 0.0370, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  3.656348705291748 ; mask density:  0.12163487076759338 ; pred:  tensor([0.0283, 0.8299, 0.0837, 0.0057, 0.0045, 0.0369, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  3.462855339050293 ; mask density:  0.1172061488032341 ; pred:  tensor([0.0285, 0.8307, 0.0831, 0.0056, 0.0045, 0.0367, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  3.2855072021484375 ; mask density:  0.11309842020273209 ; pred:  tensor([0.0286, 0.8315, 0.0825, 0.0056, 0.0044, 0.0364, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  3.122828483581543 ; mask density:  0.1092805564403534 ; pred:  tensor([0.0287, 0.8324, 0.0819, 0.0055, 0.0044, 0.0361, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  2.9734644889831543 ; mask density:  0.10572439432144165 ; pred:  tensor([0.0288, 0.8332, 0.0814, 0.0055, 0.0043, 0.0358, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  2.836182117462158 ; mask density:  0.1024043932557106 ; pred:  tensor([0.0289, 0.8341, 0.0809, 0.0055, 0.0042, 0.0354, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  2.709949493408203 ; mask density:  0.09929732233285904 ; pred:  tensor([0.0290, 0.8348, 0.0806, 0.0054, 0.0042, 0.0351, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  2.593670129776001 ; mask density:  0.09638199955224991 ; pred:  tensor([0.0291, 0.8354, 0.0803, 0.0054, 0.0041, 0.0347, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  2.486401081085205 ; mask density:  0.09363973140716553 ; pred:  tensor([0.0291, 0.8360, 0.0800, 0.0054, 0.0041, 0.0344, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  2.387310028076172 ; mask density:  0.09105384349822998 ; pred:  tensor([0.0292, 0.8365, 0.0799, 0.0054, 0.0041, 0.0340, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  2.295619249343872 ; mask density:  0.0886094719171524 ; pred:  tensor([0.0292, 0.8370, 0.0797, 0.0054, 0.0040, 0.0337, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  2.2106540203094482 ; mask density:  0.08629193902015686 ; pred:  tensor([0.0293, 0.8374, 0.0797, 0.0054, 0.0040, 0.0335, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  2.131765365600586 ; mask density:  0.08408965170383453 ; pred:  tensor([0.0293, 0.8377, 0.0796, 0.0053, 0.0039, 0.0332, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  2.0584161281585693 ; mask density:  0.08199251443147659 ; pred:  tensor([0.0293, 0.8380, 0.0796, 0.0053, 0.0039, 0.0330, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  1.9901121854782104 ; mask density:  0.07999174296855927 ; pred:  tensor([0.0294, 0.8382, 0.0796, 0.0053, 0.0039, 0.0328, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  1.9263871908187866 ; mask density:  0.07807814329862595 ; pred:  tensor([0.0294, 0.8384, 0.0796, 0.0053, 0.0038, 0.0325, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  1.8668469190597534 ; mask density:  0.0762454941868782 ; pred:  tensor([0.0294, 0.8386, 0.0797, 0.0053, 0.0038, 0.0323, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  1.8111363649368286 ; mask density:  0.07448647171258926 ; pred:  tensor([0.0294, 0.8388, 0.0798, 0.0053, 0.0038, 0.0321, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  1.7589281797409058 ; mask density:  0.07279691100120544 ; pred:  tensor([0.0294, 0.8389, 0.0799, 0.0053, 0.0038, 0.0320, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  1.7099413871765137 ; mask density:  0.07117339968681335 ; pred:  tensor([0.0294, 0.8389, 0.0800, 0.0053, 0.0037, 0.0318, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  1.6641592979431152 ; mask density:  0.06961974501609802 ; pred:  tensor([0.0294, 0.8388, 0.0802, 0.0053, 0.0037, 0.0318, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  1.6210930347442627 ; mask density:  0.06813303381204605 ; pred:  tensor([0.0294, 0.8387, 0.0804, 0.0053, 0.0037, 0.0316, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  1.5805184841156006 ; mask density:  0.06671181321144104 ; pred:  tensor([0.0295, 0.8388, 0.0805, 0.0053, 0.0037, 0.0315, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  1.5422511100769043 ; mask density:  0.06535356491804123 ; pred:  tensor([0.0295, 0.8390, 0.0807, 0.0053, 0.0036, 0.0312, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  1.5061007738113403 ; mask density:  0.06405601650476456 ; pred:  tensor([0.0295, 0.8393, 0.0807, 0.0053, 0.0036, 0.0309, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  1.471912145614624 ; mask density:  0.0628167912364006 ; pred:  tensor([0.0295, 0.8397, 0.0807, 0.0053, 0.0035, 0.0305, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  1.4395407438278198 ; mask density:  0.0616338811814785 ; pred:  tensor([0.0295, 0.8402, 0.0807, 0.0052, 0.0035, 0.0302, 0.0095, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  1.4088659286499023 ; mask density:  0.060505326837301254 ; pred:  tensor([0.0296, 0.8407, 0.0806, 0.0052, 0.0035, 0.0298, 0.0095, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  1.3797725439071655 ; mask density:  0.05942919850349426 ; pred:  tensor([0.0296, 0.8412, 0.0805, 0.0052, 0.0034, 0.0294, 0.0095, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  1.3521544933319092 ; mask density:  0.05840347334742546 ; pred:  tensor([0.0296, 0.8417, 0.0804, 0.0052, 0.0034, 0.0291, 0.0095, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  1.3259128332138062 ; mask density:  0.057426076382398605 ; pred:  tensor([0.0297, 0.8422, 0.0803, 0.0052, 0.0034, 0.0287, 0.0095, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  1.300955057144165 ; mask density:  0.056494876742362976 ; pred:  tensor([0.0297, 0.8427, 0.0801, 0.0052, 0.0033, 0.0284, 0.0094, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  1.277194857597351 ; mask density:  0.055607669055461884 ; pred:  tensor([0.0297, 0.8432, 0.0800, 0.0052, 0.0033, 0.0281, 0.0094, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  1.2545506954193115 ; mask density:  0.05476219952106476 ; pred:  tensor([0.0298, 0.8436, 0.0799, 0.0052, 0.0033, 0.0278, 0.0094, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  1.2329661846160889 ; mask density:  0.0539562925696373 ; pred:  tensor([0.0298, 0.8439, 0.0798, 0.0051, 0.0032, 0.0276, 0.0094, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  1.2124192714691162 ; mask density:  0.05318766459822655 ; pred:  tensor([0.0299, 0.8442, 0.0797, 0.0051, 0.0032, 0.0273, 0.0094, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  1.1928215026855469 ; mask density:  0.052450526505708694 ; pred:  tensor([0.0299, 0.8445, 0.0795, 0.0051, 0.0032, 0.0272, 0.0094, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  1.1740707159042358 ; mask density:  0.05174309015274048 ; pred:  tensor([0.0300, 0.8448, 0.0793, 0.0051, 0.0032, 0.0271, 0.0094, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  1.1560866832733154 ; mask density:  0.05106361582875252 ; pred:  tensor([0.0300, 0.8450, 0.0791, 0.0051, 0.0032, 0.0271, 0.0094, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  1.1388187408447266 ; mask density:  0.05041048303246498 ; pred:  tensor([0.0300, 0.8452, 0.0790, 0.0051, 0.0032, 0.0271, 0.0094, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  1.122218132019043 ; mask density:  0.049782175570726395 ; pred:  tensor([0.0301, 0.8454, 0.0788, 0.0051, 0.0032, 0.0270, 0.0094, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  1.1062427759170532 ; mask density:  0.049177270382642746 ; pred:  tensor([0.0301, 0.8455, 0.0786, 0.0051, 0.0032, 0.0271, 0.0094, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  1.0908533334732056 ; mask density:  0.04859444499015808 ; pred:  tensor([0.0301, 0.8456, 0.0785, 0.0051, 0.0032, 0.0271, 0.0094, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  1.076015591621399 ; mask density:  0.048032667487859726 ; pred:  tensor([0.0302, 0.8456, 0.0783, 0.0051, 0.0032, 0.0272, 0.0094, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  1.0616989135742188 ; mask density:  0.04749078303575516 ; pred:  tensor([0.0302, 0.8456, 0.0782, 0.0051, 0.0032, 0.0272, 0.0094, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  1.0478732585906982 ; mask density:  0.04696774110198021 ; pred:  tensor([0.0302, 0.8456, 0.0781, 0.0050, 0.0032, 0.0273, 0.0094, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  1.0345102548599243 ; mask density:  0.04646286368370056 ; pred:  tensor([0.0303, 0.8456, 0.0780, 0.0050, 0.0032, 0.0274, 0.0094, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  1.0215851068496704 ; mask density:  0.04597528651356697 ; pred:  tensor([0.0303, 0.8455, 0.0779, 0.0050, 0.0032, 0.0275, 0.0095, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  1.009086012840271 ; mask density:  0.04550407826900482 ; pred:  tensor([0.0303, 0.8455, 0.0778, 0.0050, 0.0032, 0.0276, 0.0095, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  0.9969944357872009 ; mask density:  0.04504850134253502 ; pred:  tensor([0.0303, 0.8453, 0.0777, 0.0050, 0.0032, 0.0278, 0.0095, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  0.9852791428565979 ; mask density:  0.04460799694061279 ; pred:  tensor([0.0303, 0.8450, 0.0778, 0.0050, 0.0032, 0.0279, 0.0096, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  0.9739935994148254 ; mask density:  0.04418525472283363 ; pred:  tensor([0.0304, 0.8447, 0.0778, 0.0050, 0.0032, 0.0282, 0.0096, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  0.9630435705184937 ; mask density:  0.04377928003668785 ; pred:  tensor([0.0304, 0.8446, 0.0777, 0.0050, 0.0032, 0.0284, 0.0096, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  0.9524392485618591 ; mask density:  0.04338906332850456 ; pred:  tensor([0.0304, 0.8445, 0.0776, 0.0050, 0.0033, 0.0285, 0.0096, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  0.9421442747116089 ; mask density:  0.043013736605644226 ; pred:  tensor([0.0304, 0.8443, 0.0776, 0.0050, 0.0033, 0.0287, 0.0096, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  0.9321322441101074 ; mask density:  0.04265253618359566 ; pred:  tensor([0.0304, 0.8442, 0.0776, 0.0050, 0.0033, 0.0288, 0.0096, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  0.9223900437355042 ; mask density:  0.04230474680662155 ; pred:  tensor([0.0304, 0.8441, 0.0775, 0.0050, 0.0033, 0.0289, 0.0096, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  0.9129058122634888 ; mask density:  0.04196976125240326 ; pred:  tensor([0.0304, 0.8440, 0.0776, 0.0050, 0.0033, 0.0290, 0.0097, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  0.9036968946456909 ; mask density:  0.041647084057331085 ; pred:  tensor([0.0304, 0.8438, 0.0776, 0.0050, 0.0033, 0.0291, 0.0097, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  0.8947166204452515 ; mask density:  0.041336145251989365 ; pred:  tensor([0.0304, 0.8438, 0.0775, 0.0050, 0.0033, 0.0291, 0.0097, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  0.8859414458274841 ; mask density:  0.04103630781173706 ; pred:  tensor([0.0304, 0.8440, 0.0775, 0.0050, 0.0033, 0.0290, 0.0097, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  0.8774135708808899 ; mask density:  0.04074706509709358 ; pred:  tensor([0.0304, 0.8441, 0.0774, 0.0050, 0.0033, 0.0290, 0.0097, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  0.8690915107727051 ; mask density:  0.04046798124909401 ; pred:  tensor([0.0305, 0.8441, 0.0774, 0.0050, 0.0033, 0.0290, 0.0097, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  9.32258677482605\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_675graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  680 103\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  140.15249633789062 ; mask density:  0.7108104228973389 ; pred:  tensor([0.0285, 0.8387, 0.0954, 0.0072, 0.0051, 0.0178, 0.0060, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  136.32452392578125 ; mask density:  0.6901695728302002 ; pred:  tensor([0.0295, 0.8467, 0.0863, 0.0067, 0.0053, 0.0186, 0.0056, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  132.33253479003906 ; mask density:  0.6686527132987976 ; pred:  tensor([0.0300, 0.8509, 0.0813, 0.0063, 0.0054, 0.0193, 0.0055, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  128.1812744140625 ; mask density:  0.6463314294815063 ; pred:  tensor([0.0303, 0.8538, 0.0777, 0.0060, 0.0054, 0.0200, 0.0056, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  123.88029479980469 ; mask density:  0.6232808828353882 ; pred:  tensor([0.0304, 0.8555, 0.0753, 0.0057, 0.0053, 0.0208, 0.0057, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  119.44207000732422 ; mask density:  0.5995880365371704 ; pred:  tensor([0.0304, 0.8563, 0.0739, 0.0056, 0.0052, 0.0215, 0.0059, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  114.88201141357422 ; mask density:  0.5753535032272339 ; pred:  tensor([0.0303, 0.8564, 0.0732, 0.0055, 0.0051, 0.0222, 0.0061, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  110.21837615966797 ; mask density:  0.5506892204284668 ; pred:  tensor([0.0301, 0.8558, 0.0732, 0.0054, 0.0050, 0.0229, 0.0064, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  105.4724349975586 ; mask density:  0.5257191061973572 ; pred:  tensor([0.0298, 0.8544, 0.0740, 0.0054, 0.0049, 0.0237, 0.0067, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  100.6668701171875 ; mask density:  0.5005793571472168 ; pred:  tensor([0.0295, 0.8526, 0.0747, 0.0053, 0.0048, 0.0248, 0.0071, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  95.82713317871094 ; mask density:  0.4754139184951782 ; pred:  tensor([0.0293, 0.8502, 0.0756, 0.0053, 0.0048, 0.0261, 0.0075, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  90.9795150756836 ; mask density:  0.45035886764526367 ; pred:  tensor([0.0291, 0.8476, 0.0762, 0.0053, 0.0048, 0.0279, 0.0079, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  86.15133666992188 ; mask density:  0.42556458711624146 ; pred:  tensor([0.0292, 0.8451, 0.0764, 0.0053, 0.0048, 0.0297, 0.0083, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  81.37076568603516 ; mask density:  0.40117907524108887 ; pred:  tensor([0.0292, 0.8425, 0.0770, 0.0053, 0.0047, 0.0312, 0.0088, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  76.66529083251953 ; mask density:  0.3773355185985565 ; pred:  tensor([0.0293, 0.8400, 0.0775, 0.0053, 0.0047, 0.0327, 0.0093, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  72.06060791015625 ; mask density:  0.3541552722454071 ; pred:  tensor([0.0296, 0.8381, 0.0772, 0.0052, 0.0046, 0.0342, 0.0097, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  67.5814437866211 ; mask density:  0.33175379037857056 ; pred:  tensor([0.0300, 0.8367, 0.0767, 0.0052, 0.0046, 0.0354, 0.0101, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  63.250370025634766 ; mask density:  0.3102326989173889 ; pred:  tensor([0.0304, 0.8355, 0.0764, 0.0052, 0.0045, 0.0363, 0.0104, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  59.086585998535156 ; mask density:  0.28966930508613586 ; pred:  tensor([0.0307, 0.8347, 0.0761, 0.0052, 0.0045, 0.0368, 0.0107, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  55.106101989746094 ; mask density:  0.27012789249420166 ; pred:  tensor([0.0310, 0.8342, 0.0758, 0.0052, 0.0044, 0.0371, 0.0109, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  51.32151412963867 ; mask density:  0.25165197253227234 ; pred:  tensor([0.0312, 0.8340, 0.0758, 0.0052, 0.0043, 0.0371, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  47.741634368896484 ; mask density:  0.23426608741283417 ; pred:  tensor([0.0313, 0.8340, 0.0758, 0.0052, 0.0042, 0.0369, 0.0112, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  44.37174987792969 ; mask density:  0.21797750890254974 ; pred:  tensor([0.0314, 0.8344, 0.0759, 0.0052, 0.0042, 0.0364, 0.0113, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  41.21402359008789 ; mask density:  0.20277416706085205 ; pred:  tensor([0.0314, 0.8348, 0.0762, 0.0052, 0.0041, 0.0358, 0.0113, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  38.26731872558594 ; mask density:  0.18863824009895325 ; pred:  tensor([0.0312, 0.8353, 0.0765, 0.0052, 0.0041, 0.0352, 0.0112, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  35.527957916259766 ; mask density:  0.1755373179912567 ; pred:  tensor([0.0310, 0.8357, 0.0769, 0.0052, 0.0040, 0.0347, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  32.990142822265625 ; mask density:  0.16343441605567932 ; pred:  tensor([0.0308, 0.8359, 0.0776, 0.0053, 0.0040, 0.0341, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  30.646026611328125 ; mask density:  0.152278870344162 ; pred:  tensor([0.0306, 0.8360, 0.0783, 0.0053, 0.0040, 0.0337, 0.0109, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  28.486392974853516 ; mask density:  0.14201651513576508 ; pred:  tensor([0.0304, 0.8358, 0.0792, 0.0053, 0.0040, 0.0333, 0.0108, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  26.500871658325195 ; mask density:  0.13259270787239075 ; pred:  tensor([0.0301, 0.8355, 0.0803, 0.0054, 0.0040, 0.0328, 0.0107, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  24.678823471069336 ; mask density:  0.12395069748163223 ; pred:  tensor([0.0298, 0.8350, 0.0815, 0.0055, 0.0040, 0.0324, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  23.009117126464844 ; mask density:  0.11603552103042603 ; pred:  tensor([0.0294, 0.8344, 0.0828, 0.0055, 0.0040, 0.0320, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  21.48088836669922 ; mask density:  0.10879138857126236 ; pred:  tensor([0.0291, 0.8334, 0.0842, 0.0056, 0.0040, 0.0318, 0.0106, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  20.08296012878418 ; mask density:  0.10216419398784637 ; pred:  tensor([0.0288, 0.8326, 0.0856, 0.0057, 0.0041, 0.0315, 0.0105, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  18.80515480041504 ; mask density:  0.09610245376825333 ; pred:  tensor([0.0285, 0.8315, 0.0871, 0.0057, 0.0041, 0.0313, 0.0105, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  17.637271881103516 ; mask density:  0.09055681526660919 ; pred:  tensor([0.0282, 0.8305, 0.0885, 0.0058, 0.0041, 0.0311, 0.0105, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  16.569787979125977 ; mask density:  0.08548309653997421 ; pred:  tensor([0.0280, 0.8296, 0.0898, 0.0059, 0.0041, 0.0309, 0.0105, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  15.594706535339355 ; mask density:  0.08084184676408768 ; pred:  tensor([0.0278, 0.8281, 0.0910, 0.0059, 0.0041, 0.0313, 0.0106, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  14.70256519317627 ; mask density:  0.07659346610307693 ; pred:  tensor([0.0277, 0.8278, 0.0915, 0.0059, 0.0041, 0.0311, 0.0106, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  13.88560962677002 ; mask density:  0.07270201295614243 ; pred:  tensor([0.0277, 0.8286, 0.0914, 0.0059, 0.0040, 0.0306, 0.0104, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  13.137131690979004 ; mask density:  0.06913693994283676 ; pred:  tensor([0.0278, 0.8301, 0.0907, 0.0059, 0.0040, 0.0300, 0.0103, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  12.451016426086426 ; mask density:  0.06587056815624237 ; pred:  tensor([0.0280, 0.8320, 0.0896, 0.0058, 0.0039, 0.0294, 0.0101, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  11.821600914001465 ; mask density:  0.06287595629692078 ; pred:  tensor([0.0282, 0.8337, 0.0883, 0.0057, 0.0038, 0.0291, 0.0100, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  11.243507385253906 ; mask density:  0.06012726575136185 ; pred:  tensor([0.0284, 0.8353, 0.0869, 0.0057, 0.0038, 0.0289, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  10.712145805358887 ; mask density:  0.05759618431329727 ; pred:  tensor([0.0285, 0.8365, 0.0859, 0.0056, 0.0037, 0.0288, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  10.222610473632812 ; mask density:  0.05526230111718178 ; pred:  tensor([0.0287, 0.8372, 0.0851, 0.0056, 0.0037, 0.0289, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  9.770939826965332 ; mask density:  0.05310675874352455 ; pred:  tensor([0.0288, 0.8375, 0.0846, 0.0055, 0.0037, 0.0289, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  9.353601455688477 ; mask density:  0.051112350076436996 ; pred:  tensor([0.0288, 0.8375, 0.0845, 0.0055, 0.0037, 0.0291, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  8.96790599822998 ; mask density:  0.049268998205661774 ; pred:  tensor([0.0288, 0.8369, 0.0847, 0.0055, 0.0038, 0.0294, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  8.610909461975098 ; mask density:  0.04756038263440132 ; pred:  tensor([0.0287, 0.8361, 0.0850, 0.0055, 0.0038, 0.0298, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  8.279877662658691 ; mask density:  0.04597294703125954 ; pred:  tensor([0.0287, 0.8354, 0.0852, 0.0055, 0.0038, 0.0303, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  7.97231912612915 ; mask density:  0.04449468478560448 ; pred:  tensor([0.0287, 0.8350, 0.0852, 0.0055, 0.0039, 0.0306, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  7.686096668243408 ; mask density:  0.043116386979818344 ; pred:  tensor([0.0287, 0.8350, 0.0852, 0.0055, 0.0039, 0.0306, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  7.419344425201416 ; mask density:  0.04182923212647438 ; pred:  tensor([0.0287, 0.8352, 0.0851, 0.0055, 0.0039, 0.0305, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  7.170358180999756 ; mask density:  0.04062538966536522 ; pred:  tensor([0.0287, 0.8357, 0.0850, 0.0055, 0.0039, 0.0303, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  6.937589645385742 ; mask density:  0.03949859365820885 ; pred:  tensor([0.0287, 0.8362, 0.0848, 0.0055, 0.0038, 0.0300, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  6.719640731811523 ; mask density:  0.03844260424375534 ; pred:  tensor([0.0288, 0.8371, 0.0845, 0.0055, 0.0038, 0.0296, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  6.515371322631836 ; mask density:  0.03745424374938011 ; pred:  tensor([0.0288, 0.8379, 0.0841, 0.0055, 0.0037, 0.0291, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  6.323737621307373 ; mask density:  0.03652830049395561 ; pred:  tensor([0.0288, 0.8385, 0.0838, 0.0055, 0.0037, 0.0289, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  6.143596649169922 ; mask density:  0.035654403269290924 ; pred:  tensor([0.0288, 0.8388, 0.0836, 0.0055, 0.0037, 0.0288, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  5.97391414642334 ; mask density:  0.03482850641012192 ; pred:  tensor([0.0288, 0.8387, 0.0838, 0.0055, 0.0037, 0.0288, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  5.813684463500977 ; mask density:  0.03404694050550461 ; pred:  tensor([0.0287, 0.8384, 0.0842, 0.0055, 0.0038, 0.0288, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  5.66248083114624 ; mask density:  0.03331209719181061 ; pred:  tensor([0.0285, 0.8376, 0.0849, 0.0055, 0.0038, 0.0289, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  5.519813060760498 ; mask density:  0.03261689469218254 ; pred:  tensor([0.0284, 0.8366, 0.0855, 0.0056, 0.0038, 0.0293, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  5.384695529937744 ; mask density:  0.031958334147930145 ; pred:  tensor([0.0283, 0.8359, 0.0859, 0.0056, 0.0039, 0.0295, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  5.256476402282715 ; mask density:  0.031333841383457184 ; pred:  tensor([0.0283, 0.8358, 0.0861, 0.0056, 0.0039, 0.0295, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  5.134627342224121 ; mask density:  0.030741209164261818 ; pred:  tensor([0.0283, 0.8360, 0.0860, 0.0056, 0.0039, 0.0294, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  5.018717288970947 ; mask density:  0.03017852082848549 ; pred:  tensor([0.0283, 0.8367, 0.0858, 0.0056, 0.0039, 0.0291, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  4.908687114715576 ; mask density:  0.029640480875968933 ; pred:  tensor([0.0283, 0.8372, 0.0855, 0.0056, 0.0039, 0.0288, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  4.8039021492004395 ; mask density:  0.029125850647687912 ; pred:  tensor([0.0283, 0.8375, 0.0854, 0.0056, 0.0039, 0.0287, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  4.703803539276123 ; mask density:  0.028633639216423035 ; pred:  tensor([0.0283, 0.8375, 0.0854, 0.0056, 0.0039, 0.0286, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  4.608122825622559 ; mask density:  0.028162606060504913 ; pred:  tensor([0.0283, 0.8374, 0.0857, 0.0056, 0.0039, 0.0286, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  4.5165557861328125 ; mask density:  0.0277117807418108 ; pred:  tensor([0.0282, 0.8371, 0.0860, 0.0056, 0.0039, 0.0286, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  4.428864002227783 ; mask density:  0.02728303149342537 ; pred:  tensor([0.0281, 0.8367, 0.0865, 0.0057, 0.0039, 0.0286, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  4.34506893157959 ; mask density:  0.026874840259552002 ; pred:  tensor([0.0281, 0.8364, 0.0867, 0.0057, 0.0039, 0.0286, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  4.264594078063965 ; mask density:  0.02648581750690937 ; pred:  tensor([0.0281, 0.8365, 0.0867, 0.0057, 0.0039, 0.0286, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  4.1872453689575195 ; mask density:  0.02611786313354969 ; pred:  tensor([0.0281, 0.8369, 0.0864, 0.0057, 0.0039, 0.0284, 0.0094, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  4.113027572631836 ; mask density:  0.025762928649783134 ; pred:  tensor([0.0282, 0.8372, 0.0861, 0.0057, 0.0039, 0.0284, 0.0094, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  4.0415940284729 ; mask density:  0.025419922545552254 ; pred:  tensor([0.0282, 0.8374, 0.0859, 0.0056, 0.0039, 0.0284, 0.0094, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  3.9727377891540527 ; mask density:  0.02508780173957348 ; pred:  tensor([0.0282, 0.8374, 0.0859, 0.0056, 0.0039, 0.0285, 0.0094, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  3.906437397003174 ; mask density:  0.024768702685832977 ; pred:  tensor([0.0282, 0.8372, 0.0860, 0.0056, 0.0039, 0.0286, 0.0094, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  3.842512607574463 ; mask density:  0.024461550638079643 ; pred:  tensor([0.0282, 0.8372, 0.0859, 0.0056, 0.0039, 0.0287, 0.0094, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  3.7807230949401855 ; mask density:  0.024165455251932144 ; pred:  tensor([0.0282, 0.8375, 0.0857, 0.0056, 0.0039, 0.0286, 0.0093, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  3.7211105823516846 ; mask density:  0.023876545950770378 ; pred:  tensor([0.0283, 0.8380, 0.0853, 0.0056, 0.0039, 0.0284, 0.0093, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  3.663482189178467 ; mask density:  0.02359447069466114 ; pred:  tensor([0.0283, 0.8382, 0.0851, 0.0056, 0.0039, 0.0284, 0.0093, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  3.6076416969299316 ; mask density:  0.023318886756896973 ; pred:  tensor([0.0283, 0.8383, 0.0852, 0.0056, 0.0039, 0.0284, 0.0093, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  3.553640365600586 ; mask density:  0.023052629083395004 ; pred:  tensor([0.0283, 0.8380, 0.0854, 0.0056, 0.0039, 0.0284, 0.0093, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  3.501352548599243 ; mask density:  0.02279520034790039 ; pred:  tensor([0.0283, 0.8380, 0.0855, 0.0056, 0.0039, 0.0284, 0.0093, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  3.4505996704101562 ; mask density:  0.022549498826265335 ; pred:  tensor([0.0283, 0.8382, 0.0854, 0.0056, 0.0038, 0.0283, 0.0093, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  3.4013850688934326 ; mask density:  0.022314606234431267 ; pred:  tensor([0.0283, 0.8385, 0.0851, 0.0056, 0.0038, 0.0282, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  3.353611946105957 ; mask density:  0.022090164944529533 ; pred:  tensor([0.0283, 0.8388, 0.0849, 0.0056, 0.0038, 0.0282, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  3.3072962760925293 ; mask density:  0.021868130192160606 ; pred:  tensor([0.0284, 0.8389, 0.0846, 0.0056, 0.0038, 0.0283, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  3.262204170227051 ; mask density:  0.021645160391926765 ; pred:  tensor([0.0284, 0.8388, 0.0846, 0.0056, 0.0039, 0.0284, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  3.21835994720459 ; mask density:  0.02142179012298584 ; pred:  tensor([0.0284, 0.8386, 0.0848, 0.0056, 0.0039, 0.0284, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  3.1757946014404297 ; mask density:  0.02120468020439148 ; pred:  tensor([0.0283, 0.8383, 0.0851, 0.0056, 0.0039, 0.0284, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  3.1343250274658203 ; mask density:  0.020993860438466072 ; pred:  tensor([0.0283, 0.8383, 0.0852, 0.0056, 0.0039, 0.0283, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  3.093898057937622 ; mask density:  0.020792663097381592 ; pred:  tensor([0.0283, 0.8385, 0.0852, 0.0056, 0.0038, 0.0282, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  3.0545661449432373 ; mask density:  0.020601287484169006 ; pred:  tensor([0.0283, 0.8388, 0.0851, 0.0056, 0.0038, 0.0281, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  3.016245126724243 ; mask density:  0.020412810146808624 ; pred:  tensor([0.0283, 0.8389, 0.0849, 0.0056, 0.0038, 0.0281, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  2.978837013244629 ; mask density:  0.02022646740078926 ; pred:  tensor([0.0283, 0.8387, 0.0851, 0.0056, 0.0038, 0.0282, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  9.878286838531494\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_680graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  685 103\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  142.1809539794922 ; mask density:  0.7117004990577698 ; pred:  tensor([0.1198, 0.4609, 0.2661, 0.0622, 0.0030, 0.0238, 0.0561, 0.0081],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  138.08023071289062 ; mask density:  0.692496657371521 ; pred:  tensor([0.0682, 0.6145, 0.2143, 0.0265, 0.0023, 0.0281, 0.0421, 0.0040],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  133.95941162109375 ; mask density:  0.6722748875617981 ; pred:  tensor([0.0499, 0.7002, 0.1670, 0.0147, 0.0022, 0.0312, 0.0322, 0.0027],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  129.73512268066406 ; mask density:  0.6511704921722412 ; pred:  tensor([0.0427, 0.7469, 0.1370, 0.0101, 0.0023, 0.0328, 0.0260, 0.0021],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  125.38224029541016 ; mask density:  0.6292864680290222 ; pred:  tensor([0.0398, 0.7742, 0.1184, 0.0081, 0.0025, 0.0333, 0.0218, 0.0019],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  120.8988265991211 ; mask density:  0.6067227125167847 ; pred:  tensor([0.0385, 0.7923, 0.1055, 0.0069, 0.0026, 0.0335, 0.0190, 0.0017],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  116.29448699951172 ; mask density:  0.583584725856781 ; pred:  tensor([0.0375, 0.8063, 0.0960, 0.0061, 0.0028, 0.0332, 0.0166, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  111.58789825439453 ; mask density:  0.5599857568740845 ; pred:  tensor([0.0369, 0.8161, 0.0893, 0.0056, 0.0029, 0.0328, 0.0149, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  106.79804992675781 ; mask density:  0.5360461473464966 ; pred:  tensor([0.0367, 0.8235, 0.0841, 0.0052, 0.0030, 0.0324, 0.0136, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  101.9473876953125 ; mask density:  0.511895477771759 ; pred:  tensor([0.0365, 0.8295, 0.0799, 0.0050, 0.0031, 0.0321, 0.0125, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  97.06122589111328 ; mask density:  0.4876787066459656 ; pred:  tensor([0.0363, 0.8342, 0.0768, 0.0048, 0.0032, 0.0317, 0.0117, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  92.16729736328125 ; mask density:  0.4635399580001831 ; pred:  tensor([0.0359, 0.8371, 0.0755, 0.0047, 0.0032, 0.0312, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  87.29251861572266 ; mask density:  0.4396195113658905 ; pred:  tensor([0.0353, 0.8390, 0.0750, 0.0047, 0.0033, 0.0307, 0.0108, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  82.46444702148438 ; mask density:  0.4160551130771637 ; pred:  tensor([0.0347, 0.8407, 0.0748, 0.0047, 0.0032, 0.0301, 0.0105, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  77.71062469482422 ; mask density:  0.3929777145385742 ; pred:  tensor([0.0343, 0.8424, 0.0745, 0.0047, 0.0032, 0.0295, 0.0102, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  73.05791473388672 ; mask density:  0.3705115020275116 ; pred:  tensor([0.0338, 0.8440, 0.0745, 0.0047, 0.0032, 0.0286, 0.0099, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  68.53172302246094 ; mask density:  0.3487659990787506 ; pred:  tensor([0.0334, 0.8453, 0.0751, 0.0047, 0.0032, 0.0275, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  64.15457153320312 ; mask density:  0.32783809304237366 ; pred:  tensor([0.0330, 0.8464, 0.0759, 0.0048, 0.0031, 0.0264, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  59.94621658325195 ; mask density:  0.30780866742134094 ; pred:  tensor([0.0325, 0.8470, 0.0770, 0.0049, 0.0031, 0.0254, 0.0089, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  55.923030853271484 ; mask density:  0.28874069452285767 ; pred:  tensor([0.0320, 0.8472, 0.0782, 0.0050, 0.0031, 0.0245, 0.0087, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  52.09724044799805 ; mask density:  0.27067866921424866 ; pred:  tensor([0.0315, 0.8475, 0.0792, 0.0051, 0.0030, 0.0239, 0.0086, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  48.477779388427734 ; mask density:  0.2536488473415375 ; pred:  tensor([0.0311, 0.8479, 0.0799, 0.0052, 0.0030, 0.0233, 0.0085, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  45.070556640625 ; mask density:  0.23766176402568817 ; pred:  tensor([0.0306, 0.8481, 0.0807, 0.0053, 0.0030, 0.0229, 0.0083, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  41.87718963623047 ; mask density:  0.2227085530757904 ; pred:  tensor([0.0303, 0.8486, 0.0810, 0.0053, 0.0029, 0.0225, 0.0082, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  38.896846771240234 ; mask density:  0.20877163112163544 ; pred:  tensor([0.0300, 0.8491, 0.0812, 0.0054, 0.0029, 0.0221, 0.0082, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  36.125919342041016 ; mask density:  0.19582247734069824 ; pred:  tensor([0.0298, 0.8495, 0.0814, 0.0054, 0.0029, 0.0218, 0.0081, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  33.558475494384766 ; mask density:  0.18382294476032257 ; pred:  tensor([0.0295, 0.8499, 0.0817, 0.0055, 0.0029, 0.0215, 0.0080, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  31.18658447265625 ; mask density:  0.17273098230361938 ; pred:  tensor([0.0293, 0.8502, 0.0820, 0.0055, 0.0029, 0.0211, 0.0078, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  29.001625061035156 ; mask density:  0.16248995065689087 ; pred:  tensor([0.0290, 0.8500, 0.0828, 0.0056, 0.0029, 0.0208, 0.0078, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  26.99326515197754 ; mask density:  0.15304957330226898 ; pred:  tensor([0.0288, 0.8493, 0.0840, 0.0057, 0.0028, 0.0205, 0.0077, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  25.149765014648438 ; mask density:  0.1443379819393158 ; pred:  tensor([0.0287, 0.8488, 0.0849, 0.0058, 0.0028, 0.0202, 0.0076, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  23.45897102355957 ; mask density:  0.1362726092338562 ; pred:  tensor([0.0288, 0.8493, 0.0844, 0.0058, 0.0028, 0.0202, 0.0076, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  21.909900665283203 ; mask density:  0.128825381398201 ; pred:  tensor([0.0290, 0.8505, 0.0828, 0.0057, 0.0028, 0.0205, 0.0076, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  20.49287986755371 ; mask density:  0.12195328623056412 ; pred:  tensor([0.0293, 0.8514, 0.0814, 0.0056, 0.0028, 0.0208, 0.0077, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  19.197254180908203 ; mask density:  0.11562333256006241 ; pred:  tensor([0.0295, 0.8521, 0.0802, 0.0055, 0.0028, 0.0211, 0.0078, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  18.013038635253906 ; mask density:  0.10979251563549042 ; pred:  tensor([0.0297, 0.8525, 0.0795, 0.0054, 0.0028, 0.0213, 0.0078, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  16.93013572692871 ; mask density:  0.10442100465297699 ; pred:  tensor([0.0299, 0.8531, 0.0784, 0.0054, 0.0028, 0.0216, 0.0078, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  15.94040298461914 ; mask density:  0.0994751825928688 ; pred:  tensor([0.0300, 0.8531, 0.0780, 0.0053, 0.0028, 0.0219, 0.0079, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  15.03490924835205 ; mask density:  0.09492041170597076 ; pred:  tensor([0.0302, 0.8532, 0.0774, 0.0052, 0.0028, 0.0222, 0.0080, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  14.205864906311035 ; mask density:  0.09071982651948929 ; pred:  tensor([0.0303, 0.8533, 0.0766, 0.0052, 0.0028, 0.0226, 0.0081, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  13.446236610412598 ; mask density:  0.08684536069631577 ; pred:  tensor([0.0305, 0.8535, 0.0758, 0.0051, 0.0028, 0.0230, 0.0082, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  12.749754905700684 ; mask density:  0.08327022939920425 ; pred:  tensor([0.0307, 0.8535, 0.0752, 0.0051, 0.0028, 0.0233, 0.0083, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  12.110513687133789 ; mask density:  0.0799693614244461 ; pred:  tensor([0.0309, 0.8535, 0.0747, 0.0050, 0.0029, 0.0236, 0.0083, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  11.523199081420898 ; mask density:  0.07691967487335205 ; pred:  tensor([0.0311, 0.8533, 0.0744, 0.0050, 0.0029, 0.0239, 0.0084, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  10.982728004455566 ; mask density:  0.07409617304801941 ; pred:  tensor([0.0312, 0.8533, 0.0738, 0.0050, 0.0029, 0.0243, 0.0085, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  10.48507308959961 ; mask density:  0.07147889584302902 ; pred:  tensor([0.0314, 0.8530, 0.0735, 0.0049, 0.0029, 0.0247, 0.0086, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  10.026078224182129 ; mask density:  0.06904913485050201 ; pred:  tensor([0.0315, 0.8526, 0.0734, 0.0049, 0.0029, 0.0250, 0.0087, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  9.602070808410645 ; mask density:  0.06679055094718933 ; pred:  tensor([0.0317, 0.8522, 0.0732, 0.0049, 0.0029, 0.0253, 0.0088, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  9.209761619567871 ; mask density:  0.0646839588880539 ; pred:  tensor([0.0318, 0.8518, 0.0731, 0.0049, 0.0029, 0.0256, 0.0089, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  8.846084594726562 ; mask density:  0.06271637231111526 ; pred:  tensor([0.0319, 0.8516, 0.0729, 0.0049, 0.0029, 0.0258, 0.0090, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  8.50854206085205 ; mask density:  0.060875073075294495 ; pred:  tensor([0.0319, 0.8514, 0.0728, 0.0049, 0.0029, 0.0260, 0.0090, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  8.194683074951172 ; mask density:  0.05914759635925293 ; pred:  tensor([0.0320, 0.8514, 0.0727, 0.0049, 0.0029, 0.0261, 0.0090, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  7.902431011199951 ; mask density:  0.05752670019865036 ; pred:  tensor([0.0320, 0.8514, 0.0726, 0.0048, 0.0030, 0.0261, 0.0090, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  7.630025863647461 ; mask density:  0.056004080921411514 ; pred:  tensor([0.0320, 0.8514, 0.0726, 0.0048, 0.0030, 0.0262, 0.0090, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  7.375667095184326 ; mask density:  0.05457254871726036 ; pred:  tensor([0.0320, 0.8513, 0.0727, 0.0048, 0.0030, 0.0262, 0.0090, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  7.1378068923950195 ; mask density:  0.05322553589940071 ; pred:  tensor([0.0319, 0.8513, 0.0728, 0.0049, 0.0030, 0.0262, 0.0090, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  6.915043354034424 ; mask density:  0.05195683613419533 ; pred:  tensor([0.0319, 0.8512, 0.0729, 0.0049, 0.0030, 0.0261, 0.0090, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  6.7060747146606445 ; mask density:  0.050761111080646515 ; pred:  tensor([0.0318, 0.8512, 0.0730, 0.0049, 0.0030, 0.0261, 0.0090, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  6.509708881378174 ; mask density:  0.049632612615823746 ; pred:  tensor([0.0318, 0.8512, 0.0731, 0.0049, 0.0030, 0.0261, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  6.324975967407227 ; mask density:  0.04856683686375618 ; pred:  tensor([0.0317, 0.8512, 0.0733, 0.0049, 0.0030, 0.0260, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  6.150949478149414 ; mask density:  0.047559503465890884 ; pred:  tensor([0.0316, 0.8511, 0.0734, 0.0049, 0.0030, 0.0260, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  5.986751556396484 ; mask density:  0.04660612717270851 ; pred:  tensor([0.0315, 0.8511, 0.0736, 0.0049, 0.0030, 0.0260, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  5.831554889678955 ; mask density:  0.045702945441007614 ; pred:  tensor([0.0315, 0.8511, 0.0737, 0.0049, 0.0030, 0.0259, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  5.684732437133789 ; mask density:  0.04484650865197182 ; pred:  tensor([0.0314, 0.8510, 0.0739, 0.0049, 0.0030, 0.0259, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  5.545645713806152 ; mask density:  0.04403343051671982 ; pred:  tensor([0.0313, 0.8509, 0.0741, 0.0049, 0.0030, 0.0258, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  5.413702964782715 ; mask density:  0.04326056316494942 ; pred:  tensor([0.0313, 0.8508, 0.0743, 0.0049, 0.0030, 0.0258, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  5.288372993469238 ; mask density:  0.04252493381500244 ; pred:  tensor([0.0312, 0.8507, 0.0745, 0.0049, 0.0030, 0.0257, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  5.169174671173096 ; mask density:  0.04182378947734833 ; pred:  tensor([0.0311, 0.8506, 0.0748, 0.0050, 0.0030, 0.0257, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  5.055670261383057 ; mask density:  0.04115460813045502 ; pred:  tensor([0.0310, 0.8505, 0.0750, 0.0050, 0.0030, 0.0256, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  4.94747257232666 ; mask density:  0.04051477834582329 ; pred:  tensor([0.0310, 0.8503, 0.0753, 0.0050, 0.0030, 0.0256, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  4.844184875488281 ; mask density:  0.039902739226818085 ; pred:  tensor([0.0309, 0.8501, 0.0756, 0.0050, 0.0030, 0.0255, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  4.745463848114014 ; mask density:  0.03931630402803421 ; pred:  tensor([0.0308, 0.8499, 0.0759, 0.0050, 0.0030, 0.0255, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  4.650991439819336 ; mask density:  0.038752924650907516 ; pred:  tensor([0.0307, 0.8497, 0.0762, 0.0050, 0.0030, 0.0255, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  4.560515403747559 ; mask density:  0.03821059316396713 ; pred:  tensor([0.0307, 0.8494, 0.0765, 0.0050, 0.0030, 0.0255, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  4.473726749420166 ; mask density:  0.03768783435225487 ; pred:  tensor([0.0306, 0.8492, 0.0768, 0.0051, 0.0030, 0.0255, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  4.39042854309082 ; mask density:  0.03718330338597298 ; pred:  tensor([0.0305, 0.8490, 0.0771, 0.0051, 0.0030, 0.0255, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  4.310403823852539 ; mask density:  0.03669578582048416 ; pred:  tensor([0.0304, 0.8488, 0.0774, 0.0051, 0.0030, 0.0255, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  4.233430862426758 ; mask density:  0.036223918199539185 ; pred:  tensor([0.0303, 0.8485, 0.0777, 0.0051, 0.0030, 0.0255, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  4.1593241691589355 ; mask density:  0.035766441375017166 ; pred:  tensor([0.0303, 0.8483, 0.0780, 0.0051, 0.0030, 0.0255, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  4.088132381439209 ; mask density:  0.035322405397892 ; pred:  tensor([0.0302, 0.8479, 0.0784, 0.0051, 0.0031, 0.0255, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  4.019519329071045 ; mask density:  0.0348910316824913 ; pred:  tensor([0.0301, 0.8475, 0.0788, 0.0052, 0.0031, 0.0256, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  3.95332932472229 ; mask density:  0.03447156772017479 ; pred:  tensor([0.0300, 0.8471, 0.0792, 0.0052, 0.0031, 0.0256, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  3.889430522918701 ; mask density:  0.03406335040926933 ; pred:  tensor([0.0299, 0.8467, 0.0797, 0.0052, 0.0031, 0.0256, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  3.8277781009674072 ; mask density:  0.03366650640964508 ; pred:  tensor([0.0298, 0.8462, 0.0802, 0.0052, 0.0031, 0.0256, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  3.7683138847351074 ; mask density:  0.03328049182891846 ; pred:  tensor([0.0297, 0.8456, 0.0807, 0.0053, 0.0031, 0.0257, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  3.7107832431793213 ; mask density:  0.03290479630231857 ; pred:  tensor([0.0296, 0.8451, 0.0812, 0.0053, 0.0031, 0.0257, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  3.6550819873809814 ; mask density:  0.032538946717977524 ; pred:  tensor([0.0296, 0.8446, 0.0817, 0.0053, 0.0031, 0.0258, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  3.601106882095337 ; mask density:  0.03218245878815651 ; pred:  tensor([0.0295, 0.8442, 0.0821, 0.0053, 0.0031, 0.0258, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  3.5487751960754395 ; mask density:  0.0318349227309227 ; pred:  tensor([0.0294, 0.8438, 0.0825, 0.0053, 0.0031, 0.0258, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  3.4980008602142334 ; mask density:  0.031495802104473114 ; pred:  tensor([0.0293, 0.8435, 0.0829, 0.0054, 0.0031, 0.0258, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  3.448742389678955 ; mask density:  0.031164899468421936 ; pred:  tensor([0.0293, 0.8432, 0.0833, 0.0054, 0.0031, 0.0258, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  3.4009084701538086 ; mask density:  0.030841832980513573 ; pred:  tensor([0.0292, 0.8429, 0.0836, 0.0054, 0.0031, 0.0259, 0.0089, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  3.354410409927368 ; mask density:  0.030526256188750267 ; pred:  tensor([0.0291, 0.8427, 0.0839, 0.0054, 0.0032, 0.0258, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  3.3091862201690674 ; mask density:  0.030217811465263367 ; pred:  tensor([0.0290, 0.8425, 0.0841, 0.0054, 0.0032, 0.0258, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  3.265176296234131 ; mask density:  0.02991616539657116 ; pred:  tensor([0.0290, 0.8424, 0.0843, 0.0054, 0.0032, 0.0258, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  3.2223286628723145 ; mask density:  0.029621008783578873 ; pred:  tensor([0.0289, 0.8424, 0.0844, 0.0055, 0.0032, 0.0257, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  3.1805922985076904 ; mask density:  0.02933201752603054 ; pred:  tensor([0.0289, 0.8424, 0.0845, 0.0055, 0.0032, 0.0257, 0.0088, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  3.139920234680176 ; mask density:  0.029048901051282883 ; pred:  tensor([0.0288, 0.8424, 0.0846, 0.0055, 0.0032, 0.0256, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  3.1003434658050537 ; mask density:  0.028771759942173958 ; pred:  tensor([0.0288, 0.8425, 0.0847, 0.0055, 0.0032, 0.0256, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  3.061762571334839 ; mask density:  0.028500255197286606 ; pred:  tensor([0.0288, 0.8425, 0.0847, 0.0055, 0.0032, 0.0255, 0.0087, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  9.686540603637695\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_685graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  690 139\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  101.05368041992188 ; mask density:  0.7112370729446411 ; pred:  tensor([0.0217, 0.5832, 0.3255, 0.0265, 0.0029, 0.0149, 0.0224, 0.0030],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  98.20002746582031 ; mask density:  0.6908907890319824 ; pred:  tensor([0.0214, 0.6658, 0.2548, 0.0188, 0.0030, 0.0161, 0.0177, 0.0023],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  95.2764892578125 ; mask density:  0.6696580052375793 ; pred:  tensor([0.0215, 0.7204, 0.2066, 0.0145, 0.0031, 0.0170, 0.0148, 0.0020],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  92.26661682128906 ; mask density:  0.6476182341575623 ; pred:  tensor([0.0220, 0.7555, 0.1747, 0.0120, 0.0032, 0.0177, 0.0131, 0.0017],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  89.1658935546875 ; mask density:  0.6248509883880615 ; pred:  tensor([0.0225, 0.7780, 0.1538, 0.0105, 0.0033, 0.0183, 0.0119, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  85.9747314453125 ; mask density:  0.601444661617279 ; pred:  tensor([0.0231, 0.7938, 0.1389, 0.0095, 0.0034, 0.0188, 0.0111, 0.0015],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  82.70106506347656 ; mask density:  0.5774972438812256 ; pred:  tensor([0.0236, 0.8050, 0.1281, 0.0088, 0.0034, 0.0192, 0.0105, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  79.35550689697266 ; mask density:  0.5531182289123535 ; pred:  tensor([0.0240, 0.8135, 0.1198, 0.0082, 0.0034, 0.0195, 0.0101, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  75.95197296142578 ; mask density:  0.5284275412559509 ; pred:  tensor([0.0245, 0.8200, 0.1134, 0.0078, 0.0035, 0.0198, 0.0097, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  72.50606536865234 ; mask density:  0.50355464220047 ; pred:  tensor([0.0248, 0.8251, 0.1081, 0.0075, 0.0035, 0.0201, 0.0095, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  69.03558349609375 ; mask density:  0.47863608598709106 ; pred:  tensor([0.0252, 0.8291, 0.1038, 0.0072, 0.0035, 0.0206, 0.0093, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  65.55949401855469 ; mask density:  0.45381343364715576 ; pred:  tensor([0.0255, 0.8319, 0.1007, 0.0069, 0.0035, 0.0209, 0.0093, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  62.09679412841797 ; mask density:  0.4292299151420593 ; pred:  tensor([0.0258, 0.8341, 0.0981, 0.0068, 0.0035, 0.0213, 0.0092, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  58.66735076904297 ; mask density:  0.40502598881721497 ; pred:  tensor([0.0260, 0.8359, 0.0959, 0.0066, 0.0035, 0.0216, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  55.29066848754883 ; mask density:  0.3813362419605255 ; pred:  tensor([0.0263, 0.8375, 0.0940, 0.0065, 0.0035, 0.0219, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  51.98551559448242 ; mask density:  0.3582882583141327 ; pred:  tensor([0.0265, 0.8389, 0.0922, 0.0063, 0.0035, 0.0222, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  48.76957321166992 ; mask density:  0.33599671721458435 ; pred:  tensor([0.0267, 0.8404, 0.0904, 0.0062, 0.0035, 0.0224, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  45.65903854370117 ; mask density:  0.31456103920936584 ; pred:  tensor([0.0269, 0.8415, 0.0889, 0.0061, 0.0034, 0.0227, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  42.66789627075195 ; mask density:  0.2940636873245239 ; pred:  tensor([0.0271, 0.8425, 0.0876, 0.0060, 0.0034, 0.0229, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  39.80774688720703 ; mask density:  0.27456915378570557 ; pred:  tensor([0.0273, 0.8432, 0.0866, 0.0060, 0.0034, 0.0231, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  37.0876579284668 ; mask density:  0.2561233937740326 ; pred:  tensor([0.0274, 0.8438, 0.0857, 0.0059, 0.0034, 0.0233, 0.0092, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  34.514129638671875 ; mask density:  0.238755464553833 ; pred:  tensor([0.0276, 0.8441, 0.0851, 0.0059, 0.0034, 0.0235, 0.0093, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  32.091102600097656 ; mask density:  0.22247599065303802 ; pred:  tensor([0.0277, 0.8443, 0.0846, 0.0058, 0.0034, 0.0237, 0.0093, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  29.820087432861328 ; mask density:  0.2072797566652298 ; pred:  tensor([0.0278, 0.8444, 0.0842, 0.0058, 0.0034, 0.0239, 0.0094, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  27.700464248657227 ; mask density:  0.1931486576795578 ; pred:  tensor([0.0278, 0.8443, 0.0839, 0.0058, 0.0033, 0.0242, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  25.729602813720703 ; mask density:  0.18005326390266418 ; pred:  tensor([0.0279, 0.8441, 0.0838, 0.0058, 0.0033, 0.0244, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  23.90337562561035 ; mask density:  0.16795393824577332 ; pred:  tensor([0.0280, 0.8437, 0.0838, 0.0058, 0.0033, 0.0246, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  22.2161808013916 ; mask density:  0.1568034440279007 ; pred:  tensor([0.0280, 0.8432, 0.0840, 0.0058, 0.0033, 0.0248, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  20.661012649536133 ; mask density:  0.14654621481895447 ; pred:  tensor([0.0281, 0.8430, 0.0839, 0.0058, 0.0033, 0.0250, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  19.23039436340332 ; mask density:  0.13712716102600098 ; pred:  tensor([0.0282, 0.8432, 0.0834, 0.0057, 0.0033, 0.0252, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  17.91718101501465 ; mask density:  0.12849047780036926 ; pred:  tensor([0.0282, 0.8433, 0.0830, 0.0057, 0.0033, 0.0254, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  16.71354103088379 ; mask density:  0.12057995796203613 ; pred:  tensor([0.0283, 0.8434, 0.0826, 0.0057, 0.0033, 0.0257, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  15.611374855041504 ; mask density:  0.1133330687880516 ; pred:  tensor([0.0284, 0.8435, 0.0821, 0.0056, 0.0033, 0.0260, 0.0100, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  14.60278034210205 ; mask density:  0.1066952720284462 ; pred:  tensor([0.0285, 0.8437, 0.0814, 0.0056, 0.0033, 0.0263, 0.0100, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  13.680243492126465 ; mask density:  0.10061432421207428 ; pred:  tensor([0.0286, 0.8441, 0.0808, 0.0056, 0.0033, 0.0265, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  12.836710929870605 ; mask density:  0.09504295885562897 ; pred:  tensor([0.0287, 0.8445, 0.0803, 0.0055, 0.0034, 0.0266, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  12.06604290008545 ; mask density:  0.0899374857544899 ; pred:  tensor([0.0287, 0.8444, 0.0802, 0.0055, 0.0034, 0.0269, 0.0098, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  11.362210273742676 ; mask density:  0.08526260405778885 ; pred:  tensor([0.0285, 0.8434, 0.0811, 0.0056, 0.0035, 0.0270, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  10.718545913696289 ; mask density:  0.08097968250513077 ; pred:  tensor([0.0283, 0.8424, 0.0822, 0.0056, 0.0036, 0.0270, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  10.12960433959961 ; mask density:  0.07705491781234741 ; pred:  tensor([0.0281, 0.8413, 0.0833, 0.0057, 0.0036, 0.0271, 0.0097, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  9.590604782104492 ; mask density:  0.07345506548881531 ; pred:  tensor([0.0279, 0.8399, 0.0845, 0.0058, 0.0037, 0.0273, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  9.09662914276123 ; mask density:  0.07015102356672287 ; pred:  tensor([0.0277, 0.8385, 0.0856, 0.0059, 0.0038, 0.0277, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  8.643321990966797 ; mask density:  0.06711223721504211 ; pred:  tensor([0.0275, 0.8373, 0.0866, 0.0059, 0.0039, 0.0280, 0.0095, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  8.226755142211914 ; mask density:  0.06431408226490021 ; pred:  tensor([0.0274, 0.8362, 0.0872, 0.0060, 0.0040, 0.0285, 0.0096, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  7.843562126159668 ; mask density:  0.0617339201271534 ; pred:  tensor([0.0273, 0.8352, 0.0876, 0.0060, 0.0041, 0.0289, 0.0096, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  7.490631103515625 ; mask density:  0.05935259163379669 ; pred:  tensor([0.0272, 0.8344, 0.0880, 0.0060, 0.0042, 0.0294, 0.0096, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  7.1651082038879395 ; mask density:  0.05715203285217285 ; pred:  tensor([0.0271, 0.8337, 0.0881, 0.0060, 0.0042, 0.0298, 0.0097, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  6.864451885223389 ; mask density:  0.05511573702096939 ; pred:  tensor([0.0271, 0.8331, 0.0882, 0.0060, 0.0043, 0.0303, 0.0097, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  6.586313724517822 ; mask density:  0.05322858318686485 ; pred:  tensor([0.0271, 0.8326, 0.0882, 0.0060, 0.0044, 0.0306, 0.0097, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  6.328643798828125 ; mask density:  0.05147722736001015 ; pred:  tensor([0.0271, 0.8323, 0.0882, 0.0060, 0.0044, 0.0309, 0.0098, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  6.089565753936768 ; mask density:  0.04984946548938751 ; pred:  tensor([0.0271, 0.8321, 0.0881, 0.0060, 0.0044, 0.0312, 0.0098, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  5.867383003234863 ; mask density:  0.04833409935235977 ; pred:  tensor([0.0271, 0.8321, 0.0879, 0.0060, 0.0044, 0.0313, 0.0098, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  5.660581588745117 ; mask density:  0.04692106693983078 ; pred:  tensor([0.0271, 0.8322, 0.0877, 0.0060, 0.0045, 0.0314, 0.0099, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  5.467801570892334 ; mask density:  0.04560123756527901 ; pred:  tensor([0.0271, 0.8324, 0.0874, 0.0060, 0.0045, 0.0315, 0.0099, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  5.2878265380859375 ; mask density:  0.04436497762799263 ; pred:  tensor([0.0272, 0.8326, 0.0871, 0.0059, 0.0044, 0.0315, 0.0099, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  5.119562149047852 ; mask density:  0.043204985558986664 ; pred:  tensor([0.0272, 0.8330, 0.0867, 0.0059, 0.0044, 0.0314, 0.0099, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  4.962088584899902 ; mask density:  0.042115211486816406 ; pred:  tensor([0.0273, 0.8333, 0.0864, 0.0059, 0.0044, 0.0314, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  4.814535140991211 ; mask density:  0.04108979552984238 ; pred:  tensor([0.0273, 0.8335, 0.0862, 0.0059, 0.0044, 0.0314, 0.0100, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  4.675989627838135 ; mask density:  0.04012341424822807 ; pred:  tensor([0.0274, 0.8338, 0.0859, 0.0058, 0.0044, 0.0313, 0.0101, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  4.545727729797363 ; mask density:  0.0392114631831646 ; pred:  tensor([0.0274, 0.8340, 0.0857, 0.0058, 0.0043, 0.0313, 0.0101, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  4.423062801361084 ; mask density:  0.03834958374500275 ; pred:  tensor([0.0275, 0.8342, 0.0855, 0.0058, 0.0043, 0.0313, 0.0102, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  4.307389736175537 ; mask density:  0.03753388300538063 ; pred:  tensor([0.0276, 0.8343, 0.0853, 0.0058, 0.0043, 0.0312, 0.0102, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  4.198153972625732 ; mask density:  0.036760732531547546 ; pred:  tensor([0.0276, 0.8345, 0.0851, 0.0058, 0.0042, 0.0311, 0.0103, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  4.094852924346924 ; mask density:  0.036026861518621445 ; pred:  tensor([0.0277, 0.8346, 0.0850, 0.0057, 0.0042, 0.0311, 0.0104, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  3.997036933898926 ; mask density:  0.03532937914133072 ; pred:  tensor([0.0277, 0.8347, 0.0848, 0.0057, 0.0042, 0.0311, 0.0104, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  3.9042909145355225 ; mask density:  0.03466567397117615 ; pred:  tensor([0.0278, 0.8348, 0.0847, 0.0057, 0.0042, 0.0310, 0.0105, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  3.8162360191345215 ; mask density:  0.03403337299823761 ; pred:  tensor([0.0278, 0.8349, 0.0846, 0.0057, 0.0042, 0.0310, 0.0105, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  3.73252534866333 ; mask density:  0.03343021869659424 ; pred:  tensor([0.0279, 0.8349, 0.0845, 0.0057, 0.0041, 0.0310, 0.0106, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  3.652844190597534 ; mask density:  0.032854270190000534 ; pred:  tensor([0.0279, 0.8349, 0.0844, 0.0057, 0.0041, 0.0310, 0.0107, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  3.5769779682159424 ; mask density:  0.032304927706718445 ; pred:  tensor([0.0279, 0.8348, 0.0844, 0.0057, 0.0041, 0.0310, 0.0107, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  3.504657745361328 ; mask density:  0.031780313700437546 ; pred:  tensor([0.0280, 0.8347, 0.0843, 0.0057, 0.0041, 0.0311, 0.0108, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  3.435577630996704 ; mask density:  0.03127880021929741 ; pred:  tensor([0.0280, 0.8345, 0.0843, 0.0057, 0.0041, 0.0313, 0.0108, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  3.3695201873779297 ; mask density:  0.030798885971307755 ; pred:  tensor([0.0280, 0.8343, 0.0843, 0.0057, 0.0041, 0.0314, 0.0109, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  3.3062820434570312 ; mask density:  0.030339181423187256 ; pred:  tensor([0.0281, 0.8341, 0.0843, 0.0056, 0.0041, 0.0315, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  3.245680332183838 ; mask density:  0.029898418113589287 ; pred:  tensor([0.0281, 0.8339, 0.0842, 0.0056, 0.0041, 0.0317, 0.0110, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  3.187544822692871 ; mask density:  0.02947542443871498 ; pred:  tensor([0.0281, 0.8336, 0.0843, 0.0056, 0.0041, 0.0318, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  3.1317217350006104 ; mask density:  0.029069144278764725 ; pred:  tensor([0.0281, 0.8334, 0.0843, 0.0056, 0.0041, 0.0320, 0.0111, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  3.078066110610962 ; mask density:  0.02867858298122883 ; pred:  tensor([0.0281, 0.8331, 0.0843, 0.0056, 0.0041, 0.0321, 0.0112, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  3.0264475345611572 ; mask density:  0.028302835300564766 ; pred:  tensor([0.0282, 0.8329, 0.0843, 0.0056, 0.0041, 0.0323, 0.0113, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  2.976745367050171 ; mask density:  0.02794107422232628 ; pred:  tensor([0.0282, 0.8326, 0.0843, 0.0056, 0.0042, 0.0325, 0.0113, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  2.9288461208343506 ; mask density:  0.02759251929819584 ; pred:  tensor([0.0282, 0.8323, 0.0844, 0.0056, 0.0042, 0.0326, 0.0114, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  2.8826472759246826 ; mask density:  0.027256444096565247 ; pred:  tensor([0.0282, 0.8320, 0.0844, 0.0056, 0.0042, 0.0328, 0.0114, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  2.838052988052368 ; mask density:  0.0269322469830513 ; pred:  tensor([0.0282, 0.8317, 0.0845, 0.0056, 0.0042, 0.0330, 0.0115, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  2.7949740886688232 ; mask density:  0.026619208976626396 ; pred:  tensor([0.0282, 0.8315, 0.0845, 0.0056, 0.0042, 0.0331, 0.0115, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  2.753321886062622 ; mask density:  0.026316765695810318 ; pred:  tensor([0.0282, 0.8312, 0.0846, 0.0056, 0.0042, 0.0333, 0.0116, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  2.7130260467529297 ; mask density:  0.02602437511086464 ; pred:  tensor([0.0282, 0.8309, 0.0846, 0.0056, 0.0042, 0.0334, 0.0116, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  2.6740148067474365 ; mask density:  0.02574155107140541 ; pred:  tensor([0.0282, 0.8307, 0.0847, 0.0056, 0.0042, 0.0336, 0.0116, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  2.636221408843994 ; mask density:  0.025467826053500175 ; pred:  tensor([0.0282, 0.8304, 0.0847, 0.0056, 0.0042, 0.0337, 0.0117, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  2.59958553314209 ; mask density:  0.02520274929702282 ; pred:  tensor([0.0282, 0.8302, 0.0848, 0.0057, 0.0043, 0.0339, 0.0117, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  2.564049005508423 ; mask density:  0.024945920333266258 ; pred:  tensor([0.0282, 0.8299, 0.0848, 0.0057, 0.0043, 0.0340, 0.0118, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  2.529555559158325 ; mask density:  0.024696895852684975 ; pred:  tensor([0.0282, 0.8297, 0.0849, 0.0057, 0.0043, 0.0341, 0.0118, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  2.4960567951202393 ; mask density:  0.02445530705153942 ; pred:  tensor([0.0282, 0.8295, 0.0850, 0.0057, 0.0043, 0.0342, 0.0118, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  2.463507652282715 ; mask density:  0.02422081120312214 ; pred:  tensor([0.0282, 0.8293, 0.0850, 0.0057, 0.0043, 0.0343, 0.0119, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  2.431854724884033 ; mask density:  0.023992717266082764 ; pred:  tensor([0.0282, 0.8291, 0.0851, 0.0057, 0.0043, 0.0344, 0.0119, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  2.4010510444641113 ; mask density:  0.023770777508616447 ; pred:  tensor([0.0282, 0.8290, 0.0851, 0.0057, 0.0043, 0.0344, 0.0119, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  2.3710694313049316 ; mask density:  0.023554744198918343 ; pred:  tensor([0.0282, 0.8289, 0.0852, 0.0057, 0.0043, 0.0345, 0.0120, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  2.341860294342041 ; mask density:  0.02334314025938511 ; pred:  tensor([0.0282, 0.8288, 0.0852, 0.0057, 0.0043, 0.0345, 0.0120, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  2.3133938312530518 ; mask density:  0.02313585951924324 ; pred:  tensor([0.0282, 0.8287, 0.0852, 0.0057, 0.0043, 0.0345, 0.0120, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  2.2856438159942627 ; mask density:  0.022932805120944977 ; pred:  tensor([0.0282, 0.8287, 0.0852, 0.0057, 0.0043, 0.0345, 0.0120, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  2.2585818767547607 ; mask density:  0.022733870893716812 ; pred:  tensor([0.0282, 0.8287, 0.0852, 0.0057, 0.0043, 0.0345, 0.0120, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  9.524871349334717\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_690graph_idx_-1.npy\n",
            "node label:  1\n",
            "neigh graph idx:  695 93\n",
            "Node predicted label:  1\n",
            "epoch:  0 ; loss:  54.313072204589844 ; mask density:  0.7127357721328735 ; pred:  tensor([0.0641, 0.5269, 0.3094, 0.0566, 0.0027, 0.0116, 0.0242, 0.0044],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "epoch:  1 ; loss:  52.762718200683594 ; mask density:  0.6947243213653564 ; pred:  tensor([0.0527, 0.5928, 0.2753, 0.0393, 0.0024, 0.0127, 0.0215, 0.0032],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  2 ; loss:  51.158992767333984 ; mask density:  0.6757776141166687 ; pred:  tensor([0.0433, 0.6596, 0.2329, 0.0259, 0.0021, 0.0144, 0.0193, 0.0024],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  3 ; loss:  49.52958679199219 ; mask density:  0.6559023261070251 ; pred:  tensor([0.0380, 0.7081, 0.1980, 0.0185, 0.0020, 0.0159, 0.0175, 0.0019],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  4 ; loss:  47.85962677001953 ; mask density:  0.6351881623268127 ; pred:  tensor([0.0343, 0.7467, 0.1687, 0.0137, 0.0020, 0.0173, 0.0158, 0.0016],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  5 ; loss:  46.15611267089844 ; mask density:  0.6137625575065613 ; pred:  tensor([0.0324, 0.7716, 0.1484, 0.0110, 0.0020, 0.0185, 0.0146, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  6 ; loss:  44.416603088378906 ; mask density:  0.5917530655860901 ; pred:  tensor([0.0315, 0.7878, 0.1343, 0.0095, 0.0020, 0.0196, 0.0139, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  7 ; loss:  42.64265441894531 ; mask density:  0.5692850351333618 ; pred:  tensor([0.0311, 0.7990, 0.1240, 0.0085, 0.0021, 0.0205, 0.0134, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  8 ; loss:  40.83941650390625 ; mask density:  0.5464830994606018 ; pred:  tensor([0.0310, 0.8073, 0.1162, 0.0078, 0.0022, 0.0212, 0.0130, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  9 ; loss:  39.014060974121094 ; mask density:  0.5234718918800354 ; pred:  tensor([0.0311, 0.8136, 0.1100, 0.0074, 0.0022, 0.0218, 0.0127, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  10 ; loss:  37.17536163330078 ; mask density:  0.5003803968429565 ; pred:  tensor([0.0312, 0.8186, 0.1052, 0.0070, 0.0023, 0.0222, 0.0123, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  11 ; loss:  35.33235549926758 ; mask density:  0.4773400127887726 ; pred:  tensor([0.0313, 0.8230, 0.1007, 0.0067, 0.0023, 0.0227, 0.0121, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  12 ; loss:  33.49544906616211 ; mask density:  0.45448362827301025 ; pred:  tensor([0.0315, 0.8267, 0.0969, 0.0064, 0.0024, 0.0230, 0.0118, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  13 ; loss:  31.67508888244629 ; mask density:  0.4319390058517456 ; pred:  tensor([0.0317, 0.8300, 0.0936, 0.0062, 0.0024, 0.0233, 0.0116, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  14 ; loss:  29.88150978088379 ; mask density:  0.409829318523407 ; pred:  tensor([0.0320, 0.8329, 0.0907, 0.0060, 0.0024, 0.0235, 0.0113, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  15 ; loss:  28.12458610534668 ; mask density:  0.3882667124271393 ; pred:  tensor([0.0322, 0.8356, 0.0880, 0.0059, 0.0024, 0.0237, 0.0111, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  16 ; loss:  26.41368865966797 ; mask density:  0.3673537075519562 ; pred:  tensor([0.0324, 0.8380, 0.0854, 0.0057, 0.0025, 0.0240, 0.0109, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  17 ; loss:  24.757417678833008 ; mask density:  0.3471810519695282 ; pred:  tensor([0.0326, 0.8402, 0.0831, 0.0056, 0.0025, 0.0242, 0.0107, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  18 ; loss:  23.163373947143555 ; mask density:  0.32782289385795593 ; pred:  tensor([0.0328, 0.8421, 0.0811, 0.0055, 0.0025, 0.0244, 0.0106, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  19 ; loss:  21.637805938720703 ; mask density:  0.309337854385376 ; pred:  tensor([0.0329, 0.8437, 0.0795, 0.0054, 0.0025, 0.0246, 0.0104, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  20 ; loss:  20.18556785583496 ; mask density:  0.2917707860469818 ; pred:  tensor([0.0330, 0.8450, 0.0781, 0.0053, 0.0025, 0.0247, 0.0103, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  21 ; loss:  18.81024742126465 ; mask density:  0.2751476466655731 ; pred:  tensor([0.0331, 0.8460, 0.0770, 0.0052, 0.0026, 0.0249, 0.0102, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  22 ; loss:  17.513959884643555 ; mask density:  0.2594798803329468 ; pred:  tensor([0.0332, 0.8470, 0.0759, 0.0052, 0.0026, 0.0250, 0.0101, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  23 ; loss:  16.297761917114258 ; mask density:  0.24476423859596252 ; pred:  tensor([0.0332, 0.8478, 0.0751, 0.0051, 0.0026, 0.0252, 0.0100, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  24 ; loss:  15.161430358886719 ; mask density:  0.23098629713058472 ; pred:  tensor([0.0332, 0.8484, 0.0745, 0.0051, 0.0026, 0.0253, 0.0099, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  25 ; loss:  14.103802680969238 ; mask density:  0.21812161803245544 ; pred:  tensor([0.0332, 0.8488, 0.0741, 0.0050, 0.0026, 0.0255, 0.0098, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_25.pdf\n",
            "epoch:  26 ; loss:  13.122830390930176 ; mask density:  0.20613932609558105 ; pred:  tensor([0.0331, 0.8490, 0.0739, 0.0050, 0.0027, 0.0256, 0.0097, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  27 ; loss:  12.215744972229004 ; mask density:  0.19499459862709045 ; pred:  tensor([0.0330, 0.8489, 0.0739, 0.0050, 0.0027, 0.0257, 0.0097, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  28 ; loss:  11.379532814025879 ; mask density:  0.18464291095733643 ; pred:  tensor([0.0328, 0.8483, 0.0746, 0.0051, 0.0027, 0.0257, 0.0097, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  29 ; loss:  10.609977722167969 ; mask density:  0.17503783106803894 ; pred:  tensor([0.0326, 0.8476, 0.0755, 0.0051, 0.0027, 0.0257, 0.0097, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  30 ; loss:  9.903169631958008 ; mask density:  0.16613440215587616 ; pred:  tensor([0.0323, 0.8467, 0.0765, 0.0052, 0.0027, 0.0257, 0.0098, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  31 ; loss:  9.25501537322998 ; mask density:  0.15788474678993225 ; pred:  tensor([0.0320, 0.8456, 0.0778, 0.0053, 0.0028, 0.0257, 0.0099, 0.0010],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  32 ; loss:  8.66122055053711 ; mask density:  0.15023960173130035 ; pred:  tensor([0.0318, 0.8444, 0.0791, 0.0054, 0.0028, 0.0256, 0.0099, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  33 ; loss:  8.117664337158203 ; mask density:  0.14315076172351837 ; pred:  tensor([0.0315, 0.8431, 0.0804, 0.0054, 0.0028, 0.0257, 0.0100, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  34 ; loss:  7.620584487915039 ; mask density:  0.13657499849796295 ; pred:  tensor([0.0314, 0.8416, 0.0816, 0.0055, 0.0028, 0.0258, 0.0101, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  35 ; loss:  7.165940761566162 ; mask density:  0.13046768307685852 ; pred:  tensor([0.0312, 0.8401, 0.0829, 0.0056, 0.0028, 0.0260, 0.0103, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  36 ; loss:  6.75004243850708 ; mask density:  0.12479143589735031 ; pred:  tensor([0.0311, 0.8386, 0.0843, 0.0057, 0.0028, 0.0261, 0.0104, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  37 ; loss:  6.369649410247803 ; mask density:  0.11950720846652985 ; pred:  tensor([0.0309, 0.8370, 0.0857, 0.0058, 0.0028, 0.0262, 0.0105, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  38 ; loss:  6.021242618560791 ; mask density:  0.11456488072872162 ; pred:  tensor([0.0307, 0.8356, 0.0869, 0.0059, 0.0029, 0.0263, 0.0106, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  39 ; loss:  5.701726913452148 ; mask density:  0.10993508994579315 ; pred:  tensor([0.0306, 0.8345, 0.0879, 0.0060, 0.0029, 0.0264, 0.0106, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  40 ; loss:  5.408451080322266 ; mask density:  0.10558389127254486 ; pred:  tensor([0.0304, 0.8337, 0.0888, 0.0060, 0.0029, 0.0263, 0.0106, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  41 ; loss:  5.139121055603027 ; mask density:  0.10148933529853821 ; pred:  tensor([0.0303, 0.8331, 0.0897, 0.0061, 0.0030, 0.0262, 0.0105, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  42 ; loss:  4.891570091247559 ; mask density:  0.09761533886194229 ; pred:  tensor([0.0301, 0.8325, 0.0906, 0.0062, 0.0030, 0.0261, 0.0104, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  43 ; loss:  4.663341999053955 ; mask density:  0.09394583106040955 ; pred:  tensor([0.0301, 0.8323, 0.0910, 0.0062, 0.0031, 0.0260, 0.0103, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  44 ; loss:  4.453213214874268 ; mask density:  0.09048061072826385 ; pred:  tensor([0.0299, 0.8320, 0.0915, 0.0062, 0.0031, 0.0260, 0.0102, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  45 ; loss:  4.259680271148682 ; mask density:  0.087206169962883 ; pred:  tensor([0.0297, 0.8315, 0.0923, 0.0063, 0.0032, 0.0258, 0.0101, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  46 ; loss:  4.0803704261779785 ; mask density:  0.08409181237220764 ; pred:  tensor([0.0296, 0.8316, 0.0924, 0.0062, 0.0032, 0.0258, 0.0100, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  47 ; loss:  3.9139554500579834 ; mask density:  0.08113035559654236 ; pred:  tensor([0.0295, 0.8321, 0.0920, 0.0062, 0.0032, 0.0260, 0.0099, 0.0011],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  48 ; loss:  3.7595455646514893 ; mask density:  0.07831548899412155 ; pred:  tensor([0.0294, 0.8329, 0.0911, 0.0061, 0.0033, 0.0262, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  49 ; loss:  3.616088628768921 ; mask density:  0.07564256340265274 ; pred:  tensor([0.0294, 0.8339, 0.0900, 0.0060, 0.0033, 0.0264, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  50 ; loss:  3.482865333557129 ; mask density:  0.07310930639505386 ; pred:  tensor([0.0294, 0.8348, 0.0888, 0.0058, 0.0034, 0.0268, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_50.pdf\n",
            "epoch:  51 ; loss:  3.35892915725708 ; mask density:  0.07071232795715332 ; pred:  tensor([0.0295, 0.8356, 0.0874, 0.0057, 0.0034, 0.0272, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  52 ; loss:  3.243600368499756 ; mask density:  0.06844766438007355 ; pred:  tensor([0.0297, 0.8364, 0.0861, 0.0056, 0.0034, 0.0277, 0.0099, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  53 ; loss:  3.136148452758789 ; mask density:  0.06631044298410416 ; pred:  tensor([0.0298, 0.8370, 0.0850, 0.0055, 0.0034, 0.0281, 0.0100, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  54 ; loss:  3.0358667373657227 ; mask density:  0.06429418921470642 ; pred:  tensor([0.0300, 0.8374, 0.0839, 0.0054, 0.0034, 0.0285, 0.0101, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  55 ; loss:  2.9422075748443604 ; mask density:  0.06239710748195648 ; pred:  tensor([0.0303, 0.8378, 0.0826, 0.0053, 0.0034, 0.0291, 0.0103, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  56 ; loss:  2.854743242263794 ; mask density:  0.06061198189854622 ; pred:  tensor([0.0305, 0.8378, 0.0817, 0.0053, 0.0034, 0.0296, 0.0105, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  57 ; loss:  2.7727580070495605 ; mask density:  0.058931831270456314 ; pred:  tensor([0.0307, 0.8377, 0.0810, 0.0052, 0.0034, 0.0301, 0.0106, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  58 ; loss:  2.695802688598633 ; mask density:  0.057349685579538345 ; pred:  tensor([0.0310, 0.8376, 0.0802, 0.0052, 0.0034, 0.0306, 0.0108, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  59 ; loss:  2.6234428882598877 ; mask density:  0.055858779698610306 ; pred:  tensor([0.0312, 0.8374, 0.0797, 0.0051, 0.0034, 0.0310, 0.0110, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  60 ; loss:  2.5552804470062256 ; mask density:  0.05445156991481781 ; pred:  tensor([0.0314, 0.8371, 0.0792, 0.0051, 0.0034, 0.0314, 0.0111, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  61 ; loss:  2.4909369945526123 ; mask density:  0.05312272161245346 ; pred:  tensor([0.0316, 0.8369, 0.0786, 0.0051, 0.0034, 0.0319, 0.0113, 0.0012],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  62 ; loss:  2.4302282333374023 ; mask density:  0.05186724290251732 ; pred:  tensor([0.0318, 0.8366, 0.0782, 0.0050, 0.0035, 0.0323, 0.0114, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  63 ; loss:  2.3728561401367188 ; mask density:  0.05068038031458855 ; pred:  tensor([0.0320, 0.8362, 0.0779, 0.0050, 0.0035, 0.0327, 0.0116, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  64 ; loss:  2.3185336589813232 ; mask density:  0.04955733194947243 ; pred:  tensor([0.0321, 0.8358, 0.0776, 0.0050, 0.0035, 0.0330, 0.0117, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  65 ; loss:  2.267031669616699 ; mask density:  0.04849408194422722 ; pred:  tensor([0.0323, 0.8354, 0.0774, 0.0050, 0.0035, 0.0333, 0.0119, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  66 ; loss:  2.2181506156921387 ; mask density:  0.04748687148094177 ; pred:  tensor([0.0324, 0.8350, 0.0772, 0.0050, 0.0035, 0.0336, 0.0120, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  67 ; loss:  2.171701431274414 ; mask density:  0.04653220251202583 ; pred:  tensor([0.0325, 0.8346, 0.0771, 0.0050, 0.0035, 0.0339, 0.0121, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  68 ; loss:  2.1275112628936768 ; mask density:  0.04562682285904884 ; pred:  tensor([0.0326, 0.8342, 0.0770, 0.0049, 0.0035, 0.0342, 0.0122, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  69 ; loss:  2.0854380130767822 ; mask density:  0.04476741701364517 ; pred:  tensor([0.0327, 0.8338, 0.0769, 0.0049, 0.0035, 0.0345, 0.0123, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  70 ; loss:  2.0453033447265625 ; mask density:  0.043951161205768585 ; pred:  tensor([0.0328, 0.8334, 0.0768, 0.0049, 0.0035, 0.0347, 0.0124, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  71 ; loss:  2.0069711208343506 ; mask density:  0.043175384402275085 ; pred:  tensor([0.0329, 0.8332, 0.0767, 0.0049, 0.0035, 0.0349, 0.0125, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  72 ; loss:  1.9703410863876343 ; mask density:  0.04243781045079231 ; pred:  tensor([0.0330, 0.8329, 0.0766, 0.0049, 0.0035, 0.0350, 0.0126, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  73 ; loss:  1.9353036880493164 ; mask density:  0.04173615947365761 ; pred:  tensor([0.0331, 0.8327, 0.0766, 0.0049, 0.0035, 0.0352, 0.0127, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  74 ; loss:  1.9017510414123535 ; mask density:  0.0410681813955307 ; pred:  tensor([0.0332, 0.8325, 0.0765, 0.0049, 0.0035, 0.0353, 0.0128, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  75 ; loss:  1.869585633277893 ; mask density:  0.04043183848261833 ; pred:  tensor([0.0333, 0.8322, 0.0764, 0.0049, 0.0035, 0.0355, 0.0129, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "log/mask/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "log/grad/graphsyn2_base_h20_o20_explainsyn2_base_h20_o20_explain_75.pdf\n",
            "epoch:  76 ; loss:  1.8387212753295898 ; mask density:  0.03982515633106232 ; pred:  tensor([0.0333, 0.8320, 0.0764, 0.0049, 0.0035, 0.0356, 0.0130, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  77 ; loss:  1.8090766668319702 ; mask density:  0.03924628719687462 ; pred:  tensor([0.0334, 0.8317, 0.0764, 0.0049, 0.0035, 0.0357, 0.0130, 0.0013],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  78 ; loss:  1.780576229095459 ; mask density:  0.03869350999593735 ; pred:  tensor([0.0335, 0.8315, 0.0763, 0.0049, 0.0035, 0.0359, 0.0131, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  79 ; loss:  1.7531532049179077 ; mask density:  0.03816528990864754 ; pred:  tensor([0.0335, 0.8312, 0.0763, 0.0049, 0.0035, 0.0360, 0.0132, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  80 ; loss:  1.7267429828643799 ; mask density:  0.03766009211540222 ; pred:  tensor([0.0336, 0.8310, 0.0763, 0.0049, 0.0035, 0.0361, 0.0133, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  81 ; loss:  1.7012852430343628 ; mask density:  0.03717648610472679 ; pred:  tensor([0.0336, 0.8307, 0.0763, 0.0049, 0.0035, 0.0362, 0.0133, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  82 ; loss:  1.676726222038269 ; mask density:  0.036713164299726486 ; pred:  tensor([0.0337, 0.8305, 0.0763, 0.0049, 0.0035, 0.0363, 0.0134, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  83 ; loss:  1.6530165672302246 ; mask density:  0.0362689234316349 ; pred:  tensor([0.0337, 0.8303, 0.0764, 0.0049, 0.0035, 0.0365, 0.0135, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  84 ; loss:  1.6301078796386719 ; mask density:  0.03584258630871773 ; pred:  tensor([0.0338, 0.8300, 0.0764, 0.0049, 0.0035, 0.0366, 0.0135, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  85 ; loss:  1.607956886291504 ; mask density:  0.03543310984969139 ; pred:  tensor([0.0338, 0.8298, 0.0764, 0.0049, 0.0035, 0.0367, 0.0136, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  86 ; loss:  1.5865250825881958 ; mask density:  0.0350395143032074 ; pred:  tensor([0.0338, 0.8296, 0.0764, 0.0049, 0.0035, 0.0368, 0.0137, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  87 ; loss:  1.56577467918396 ; mask density:  0.03466090187430382 ; pred:  tensor([0.0339, 0.8294, 0.0764, 0.0049, 0.0035, 0.0369, 0.0137, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  88 ; loss:  1.545670747756958 ; mask density:  0.0342964231967926 ; pred:  tensor([0.0339, 0.8291, 0.0764, 0.0049, 0.0035, 0.0369, 0.0138, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  89 ; loss:  1.5261808633804321 ; mask density:  0.03394529968500137 ; pred:  tensor([0.0340, 0.8289, 0.0765, 0.0049, 0.0035, 0.0370, 0.0138, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  90 ; loss:  1.5072745084762573 ; mask density:  0.033606771379709244 ; pred:  tensor([0.0340, 0.8287, 0.0765, 0.0049, 0.0035, 0.0371, 0.0139, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  91 ; loss:  1.4889233112335205 ; mask density:  0.03328017145395279 ; pred:  tensor([0.0340, 0.8286, 0.0765, 0.0049, 0.0035, 0.0372, 0.0139, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  92 ; loss:  1.4711015224456787 ; mask density:  0.03296484053134918 ; pred:  tensor([0.0341, 0.8284, 0.0765, 0.0048, 0.0035, 0.0373, 0.0140, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  93 ; loss:  1.4537832736968994 ; mask density:  0.03266018629074097 ; pred:  tensor([0.0341, 0.8282, 0.0765, 0.0048, 0.0035, 0.0374, 0.0140, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  94 ; loss:  1.4369468688964844 ; mask density:  0.03236564248800278 ; pred:  tensor([0.0341, 0.8280, 0.0766, 0.0048, 0.0035, 0.0374, 0.0141, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  95 ; loss:  1.4205772876739502 ; mask density:  0.03208291903138161 ; pred:  tensor([0.0341, 0.8279, 0.0766, 0.0048, 0.0035, 0.0375, 0.0141, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  96 ; loss:  1.4046664237976074 ; mask density:  0.031811270862817764 ; pred:  tensor([0.0342, 0.8278, 0.0765, 0.0048, 0.0035, 0.0375, 0.0142, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  97 ; loss:  1.3891524076461792 ; mask density:  0.03155005723237991 ; pred:  tensor([0.0342, 0.8278, 0.0765, 0.0048, 0.0035, 0.0375, 0.0142, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  98 ; loss:  1.3740241527557373 ; mask density:  0.031298667192459106 ; pred:  tensor([0.0343, 0.8280, 0.0764, 0.0048, 0.0035, 0.0375, 0.0142, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "epoch:  99 ; loss:  1.3592993021011353 ; mask density:  0.03105434775352478 ; pred:  tensor([0.0343, 0.8281, 0.0762, 0.0048, 0.0035, 0.0374, 0.0142, 0.0014],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "finished training in  9.653904676437378\n",
            "Saved adjacency matrix to  masked_adj_syn2_base_h20_o20_explainnode_idx_695graph_idx_-1.npy\n",
            "log/graph/syn2_exp_0syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_1syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_2syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_3syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_4syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_5syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_6syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_7syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_8syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_9syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_10syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_11syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_12syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_13syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_14syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_15syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_16syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_17syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_18syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_19syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_20syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_21syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_22syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_23syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_24syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_25syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_26syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_27syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_28syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_29syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_30syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_31syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_32syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_33syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_34syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_35syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_36syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_37syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_38syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_39syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_40syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_41syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_42syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_43syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_44syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_45syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_46syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_47syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_48syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_49syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_50syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_51syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_52syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_53syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_54syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_55syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_56syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_57syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_58syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "log/graph/syn2_exp_59syn2_base_h20_o20_explainsyn2_base_h20_o20_explain_0.pdf\n",
            "Traceback (most recent call last):\n",
            "  File \"explainer_main.py\", line 316, in <module>\n",
            "    main()\n",
            "  File \"explainer_main.py\", line 312, in main\n",
            "    range(400, 700, 5), prog_args\n",
            "  File \"/content/gnn-model-explainer/explainer/explain.py\", line 333, in explain_nodes_gnn_stats\n",
            "    plt.savefig(\"log/pr/pr_\" + self.args.dataset + \"_\" + model + \".png\")\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\", line 723, in savefig\n",
            "    res = fig.savefig(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/figure.py\", line 2203, in savefig\n",
            "    self.canvas.print_figure(fname, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\", line 2126, in print_figure\n",
            "    **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py\", line 535, in print_png\n",
            "    with cbook.open_file_cm(filename_or_obj, \"wb\") as fh:\n",
            "  File \"/usr/lib/python3.7/contextlib.py\", line 112, in __enter__\n",
            "    return next(self.gen)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\", line 418, in open_file_cm\n",
            "    fh, opened = to_filehandle(path_or_file, mode, True, encoding)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\", line 403, in to_filehandle\n",
            "    fh = open(fname, flag, encoding=encoding)\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'log/pr/pr_syn2_exp.png'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visulaize_prediction(idx, threshold = .4, exp=5):\n",
        "    # get saved mask\n",
        "    os.chdir(os.path.join('/content/gnn-model-explainer', 'log'))\n",
        "    file_name = f\"masked_adj_syn{exp}_base_h20_o20_explainnode_idx_{idx}graph_idx_-1.npy\"\n",
        "    with open(file_name, 'rb') as f:\n",
        "        mask = np.load(f)\n",
        "\n",
        "    # visualize mask\n",
        "    mask_thr = mask > threshold\n",
        "    G = nx.from_numpy_matrix(mask_thr)\n",
        "    st = set()\n",
        "    for tup in G.edges:\n",
        "      st.add(tup[0])\n",
        "      st.add(tup[1])\n",
        "    len_G = len(G.nodes)\n",
        "    nodes_remove = [node for node in range(len_G) if node not in st]\n",
        "    G.remove_nodes_from(nodes_remove)\n",
        "    colors = []\n",
        "    for tup in list(G.edges):\n",
        "        colors.append(mask[tup[0], tup[1]])\n",
        "    print(G.edges, colors)\n",
        "  \n",
        "    f = nx.draw_networkx(G, edge_color=colors)\n",
        "    try:\n",
        "        os.mkdir('figs')\n",
        "    except: \n",
        "        pass\n",
        "    plt.savefig(f'figs/prediction_{idx}.jpg')                     \n",
        "\n",
        "i = 14\n",
        "visulaize_prediction(400 + 5*i, .01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "ajE24-ziYjba",
        "outputId": "7e1ab0a8-94d0-405e-d2ac-dc960218a923"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 1), (1, 2), (1, 3), (3, 5), (3, 6), (3, 7), (4, 7), (7, 8), (7, 9)] [0.9987682104110718, 0.9983554482460022, 0.9975160360336304, 0.9970535039901733, 0.9958109259605408, 0.9978829622268677, 0.9983197450637817, 0.9982632398605347, 0.9982450008392334]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explain the network"
      ],
      "metadata": {
        "id": "AtA7Vuc_FP9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.edge_index"
      ],
      "metadata": {
        "id": "1Z3EHlDlXYrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_idx = 8\n",
        "x, edge_index = data.x, data.edge_index\n",
        "explainer = GNNExplainer(model, epochs=200,)\n",
        "node_feat_mask, edge_mask = explainer.explain_node(node_idx, x, edge_index)"
      ],
      "metadata": {
        "id": "_GPgJKj4JgcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax, G = explainer.visualize_subgraph(node_idx, edge_index, edge_mask, y=data.y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ur6zm0oMJlvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0].node_stores"
      ],
      "metadata": {
        "id": "mF1KgmeDJpPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qKPDFnpdK4o7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}